<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>期望最大算法（Expectation Maximization Algorithm）</title>
    <url>/2024/11/20/EM/</url>
    <content><![CDATA[<h2 id="介绍">介绍</h2>
<p>期望最大化算法（Expectation-Maximization algorithm,
EM）是一种在统计学中估计概率模型参数的方法，特别适用于包含隐变量（latent
variables）的概率模型。</p>
<p>如果一个概率模型只有观测变量，那么我们可以<strong>基于观测得到的数据，用最大似然估计求得概率模型的参数</strong>。但是如果概率模型还包含了无法观测的变量（<font color=blue>隐变量</font>），则无法用上述方法估计，所以需要考虑隐变量，引入新的方法对参数进行估计。</p>
<h2 id="应用举例">应用举例</h2>
<p>假设我们有一组一维数据<span class="math inline">\(X=\{x_1,
x_2,...,x_n\}\)</span>，我们认为这些数据是由2个正态分布混合而成的。我们的目标是估计这2个正态分布的参数（均值和方差）以及它们各自的权重。参数如下：</p>
<ul>
<li>第1个分布 <span class="math inline">\(N(\mu_1,
\sigma_1)\)</span>，其中<span class="math inline">\(\mu_1=5\)</span>,
<span class="math inline">\(\sigma_1=9\)</span></li>
<li>第2个分布 <span class="math inline">\(N(\mu_2,
\sigma_2)\)</span>，其中<span class="math inline">\(\mu_2=15\)</span>,
<span class="math inline">\(\sigma_2=0.5\)</span></li>
</ul>
<p>两个分布的权重满足：<span
class="math inline">\(\sum_{k=1}^2\pi_k=1\)</span></p>
<p>我们目前手中只有这一组一维观测数据<span
class="math inline">\(X=\{x_1,
x_2,...,x_n\}\)</span>，已知观测数据由2个正态分布组成，目标是求出这2个正态分布的参数以及各自的权重。</p>
<p>注意，我们不能直接使用观测数据去拟合2个分布，因为观测数据的分布实际上是2个正态分布混合而成，其中包含了一个隐变量：</p>
<p><span class="math display">\[
    z_i= \begin{cases}
        0&amp; x_i \in N(\mu_1, \sigma_1^2)\\
        1&amp; x_i \in N(\mu_2, \sigma_2^2)
        \end{cases}
\]</span></p>
<p>隐变量<span class="math inline">\(z_i\)</span>表示数据点<span
class="math inline">\(x_i\)</span>由哪个分布生成。而隐变量<span
class="math inline">\(z_i\)</span>的值无法被观测，所以当我们用最大似然估计去做时，需要考虑所有可能的隐变量情况（不同取值的权重）：</p>
<p><span class="math display">\[
\begin{aligned}
L(\theta)&amp;=\prod_{i=1}^{n}f(x_i;\theta) \\
    ln L(\theta) &amp;= \sum_{i=1}^{n}ln f(x_i:\theta) \\
    &amp;=  \sum_{i=1}^{n}ln \sum_{k=1}^{2} \pi_k f(x_i:\theta_k)
    \end{aligned}
\]</span> 但由于 <span
class="math inline">\(\pi_k\)</span>未知，所以难以进行最大似然估计。</p>
<h2 id="em算法步骤">EM算法步骤</h2>
<p>我们首先用两个正态分布混合生成观测数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">8</span>)</span><br><span class="line">n_samples = ⅓</span><br><span class="line">data = np.concatenate((np.random.normal(<span class="number">5</span>, <span class="number">3</span>, n_samples),</span><br><span class="line">                       np.random.normal(<span class="number">15</span>, <span class="number">0.5</span>, n_samples)))</span><br></pre></td></tr></table></figure>
<p>并随机给予两个分布初始参数以及权重，代码如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mu1, sigma1 = <span class="number">10</span>, <span class="number">1</span></span><br><span class="line">mu2, sigma2 = <span class="number">20</span>, <span class="number">1</span></span><br><span class="line">pi1, pi2 = <span class="number">0.5</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">tolerance = <span class="number">1e-6</span></span><br><span class="line">max_iterations = <span class="number">1000</span></span><br></pre></td></tr></table></figure></p>
<p>EM算法分为两个步骤： ### E步（期望步, Expectation step）：
我们需要计算每个数据点 <span class="math inline">\(x_i\)</span>
属于不同分布的后验概率（<font color=blue>本质上是隐变量z的条件期望值</font>），此处可以使用贝叶斯公式计算得到：</p>
<p><span class="math display">\[
\begin{aligned}
P(z_i=1|x_i,
\theta)&amp;=\frac{P(x_i|z_i=1)P(z_i=1)}{P(x_i)}=\frac{P(x_i|z_i=1)P(z_i=1)}{P(x_i|z_i=1)P(z_i=1)+P(x_i|z_i=2)P(z_i=2)}\\
&amp;=\frac{\pi_1N(x_i|\mu_1, \sigma_1^2)}{\pi_1N(x_i|\mu_1, \sigma_1^2)
+ \pi_2N(x_i|\mu_2, \sigma_2^2)}
\end{aligned}
\]</span></p>
<p>同理可以求得<span class="math inline">\(P(z_i=2|x_i,
\theta)\)</span>。代码如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_pdf</span><span class="params">(x, mu, sigma)</span>:</span></span><br><span class="line">    <span class="comment"># 计算x_i的概率密度</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> / (np.sqrt(<span class="number">2</span> * np.pi) * sigma)) * np.exp(<span class="number">-0.5</span> * ((x - mu) / sigma) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(max_iterations):</span><br><span class="line">    <span class="comment"># E步：计算后验概率</span></span><br><span class="line">    likelihood1 = normal_pdf(data, mu1, sigma1)</span><br><span class="line">    likelihood2 = normal_pdf(data, mu2, sigma2)</span><br><span class="line">    total_likelihood = pi1 * likelihood1 + pi2 * likelihood2</span><br><span class="line">    posterior1 = (pi1 * likelihood1) / total_likelihood</span><br><span class="line">    posterior2 = (pi2 * likelihood2) / total_likelihood</span><br></pre></td></tr></table></figure></p>
<h3 id="m步最大化步-maximization-step">M步（最大化步, Maximization
step）：</h3>
<p>在E步得到 <span class="math inline">\(x_i\)</span>
属于不同分布的后验概率后，也是隐变量<span
class="math inline">\(z_i\)</span>的条件期望后，我们利用这个期望来更新参数估计值，以最大化观测数据的似然函数。</p>
<blockquote>
<p>因为现在已经知道了关于隐变量z_i的条件期望，所以我们可以用最大似然估计去估计各分布的参数了：</p>
</blockquote>
<p><span class="math display">\[
    \mu_k=\mathop{\arg\max}\limits_{\mu_k}{\sum_{i=1}^{n}P(z_i=k|x_i,
\theta)}lnf(x_i;\mu_k,\sigma_k^2)
\]</span></p>
<p><span class="math display">\[
    \sigma_k^2=\mathop{\arg\max}\limits_{\sigma_k^2}{\sum_{i=1}^{n}P(z_i=k|x_i,
\theta)}lnf(x_i;\mu_k,\sigma_k^2)
\]</span></p>
<p>对上述两式进行求解可得各部分更新公式如下（<span
class="math inline">\(\pi_k\)</span>更新同理）： <span
class="math display">\[
    \mu_k =
\frac{\sum_{i=1}^{n}P(z_i=k|x_i,\theta)x_i}{\sum_{i=1}^{n}P(z_i=k|x_i,\theta)}
\]</span></p>
<p><span class="math display">\[
    \sigma_k^2 = \frac{\sum_{i=1}^{n}P(z_i=k|x_i,
\theta)(x_i-\mu_k)^2}{\sum_{i=1}^{n}P(z_i=k|x_i,\theta)}
\]</span></p>
<p><span class="math display">\[
    \pi_k=\frac{1}{n}\sum_{i=1}^{n}P(z_i=k|x_i,\theta)
\]</span></p>
<p>代码如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># M步: 更新参数</span></span><br><span class="line">pi1_new = np.mean(posterior1)</span><br><span class="line">pi2_new = np.mean(posterior2)</span><br><span class="line">mu1_new = np.sum(posterior1 * data) / np.sum(posterior1)</span><br><span class="line">mu2_new = np.sum(posterior2 * data) / np.sum(posterior2)</span><br><span class="line">sigma1_new = np.sqrt(np.sum(posterior1 * (data - mu1_new) ** <span class="number">2</span>) / np.sum(posterior1))</span><br><span class="line">sigma2_new = np.sqrt(np.sum(posterior2 * (data - mu2_new) ** <span class="number">2</span>) / np.sum(posterior2))</span><br></pre></td></tr></table></figure></p>
<p>完整代码如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">np.random.seed(<span class="number">8</span>)</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">data = np.concatenate((np.random.normal(<span class="number">5</span>, <span class="number">3</span>, n_samples // <span class="number">2</span>),</span><br><span class="line">                       np.random.normal(<span class="number">15</span>, <span class="number">0.5</span>, n_samples // <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">mu1, sigma1 = <span class="number">10</span>, <span class="number">1</span></span><br><span class="line">mu2, sigma2 = <span class="number">20</span>, <span class="number">1</span></span><br><span class="line">pi1, pi2 = <span class="number">0.5</span>, <span class="number">0.5</span></span><br><span class="line">tolerance = <span class="number">1e-6</span></span><br><span class="line">max_iterations = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_pdf</span><span class="params">(x, mu, sigma)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> / (np.sqrt(<span class="number">2</span> * np.pi) * sigma)) * np.exp(<span class="number">-0.5</span> * ((x - mu) / sigma) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(max_iterations):</span><br><span class="line">    <span class="comment"># E步: 计算后验概率</span></span><br><span class="line">    likelihood1 = normal_pdf(data, mu1, sigma1)</span><br><span class="line">    likelihood2 = normal_pdf(data, mu2, sigma2)</span><br><span class="line">    total_likelihood = pi1 * likelihood1 + pi2 * likelihood2</span><br><span class="line">    posterior1 = (pi1 * likelihood1) / total_likelihood</span><br><span class="line">    posterior2 = (pi2 * likelihood2) / total_likelihood</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># M步: 更新参数</span></span><br><span class="line">    pi1_new = np.mean(posterior1)</span><br><span class="line">    pi2_new = np.mean(posterior2)</span><br><span class="line">    mu1_new = np.sum(posterior1 * data) / np.sum(posterior1)</span><br><span class="line">    mu2_new = np.sum(posterior2 * data) / np.sum(posterior2)</span><br><span class="line">    sigma1_new = np.sqrt(np.sum(posterior1 * (data - mu1_new) ** <span class="number">2</span>) / np.sum(posterior1))</span><br><span class="line">    sigma2_new = np.sqrt(np.sum(posterior2 * (data - mu2_new) ** <span class="number">2</span>) / np.sum(posterior2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查是否收敛</span></span><br><span class="line">    <span class="keyword">if</span> (abs(mu1_new - mu1) &lt; tolerance <span class="keyword">and</span></span><br><span class="line">        abs(mu2_new - mu2) &lt; tolerance <span class="keyword">and</span></span><br><span class="line">        abs(sigma1_new - sigma1) &lt; tolerance <span class="keyword">and</span></span><br><span class="line">        abs(sigma2_new - sigma2) &lt; tolerance):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    mu1, mu2, sigma1, sigma2, pi1, pi2 = mu1_new, mu2_new, sigma1_new, sigma2_new, pi1_new, pi2_new</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印最终结果</span></span><br><span class="line">print(<span class="string">f"迭代次数: <span class="subst">&#123;iteration&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"μ1: <span class="subst">&#123;mu1&#125;</span>, σ1: <span class="subst">&#123;sigma1&#125;</span>, π1: <span class="subst">&#123;pi1&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"μ2: <span class="subst">&#123;mu2&#125;</span>, σ2: <span class="subst">&#123;sigma2&#125;</span>, π2: <span class="subst">&#123;pi2&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制结果</span></span><br><span class="line">plt.hist(data, bins=<span class="number">30</span>, density=<span class="literal">True</span>, alpha=<span class="number">0.6</span>, color=<span class="string">'g'</span>)</span><br><span class="line">x = np.linspace(min(data), max(data), <span class="number">100</span>)</span><br><span class="line">plt.plot(x, pi1 * normal_pdf(x, mu1, sigma1), <span class="string">'r-'</span>, lw=<span class="number">2</span>, label=<span class="string">f'N(<span class="subst">&#123;mu1:<span class="number">.2</span>f&#125;</span>, <span class="subst">&#123;sigma1:<span class="number">.2</span>f&#125;</span>)'</span>)</span><br><span class="line">plt.plot(x, pi2 * normal_pdf(x, mu2, sigma2), <span class="string">'b-'</span>, lw=<span class="number">2</span>, label=<span class="string">f'N(<span class="subst">&#123;mu2:<span class="number">.2</span>f&#125;</span>, <span class="subst">&#123;sigma2:<span class="number">.2</span>f&#125;</span>)'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
结果输出如下：
<center>
<img src="/imgs/EM/EM.png"/>
</center>
]]></content>
      <categories>
        <category>通用算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>概率论</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>[ISCA 2024]  ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models</title>
    <url>/2024/11/27/ElasticRec/</url>
    <content><![CDATA[<blockquote>
<p>题目：ElasticRec: A Microservice-based Model Serving Architecture
Enabling Elastic Resource Scaling for Recommendation Models</p>
<p>来源：ISCA 2024</p>
<p>作者：韩国科学技术院</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>推荐系统（<code>RecSys</code>）广泛应用在许多线上服务中，为增加<code>RecSys</code>的推理时的吞吐量，数据中心通常对<code>RecSys</code>进行模型级别（model-wise）的资源管理。然而，<code>RecSys</code>中不同模块有着异构的资源需求，比如：</p>
<ul>
<li><code>RecSys</code>中<code>MLP</code>模块 对于计算资源的需求高</li>
<li><code>RecSys</code>中<code>Embedding Table</code>模块对于内存资源的需求高</li>
</ul>
<p>如果将<code>RecSys</code>模型看作一个整体进行服务部署、资源分配等操作，势必会造成大量的资源浪费；但对<code>RecSys</code>模型中的每一层进行资源管理又是非常具有挑战性的（这里类似于单体应用和微服务应用的关系）。因此，作者提出了<code>ElasticRec</code>，一种基于微服务架构的推荐系统细粒度资源分配方法，目标是减少<strong>部署时的内存消耗</strong>，提升<code>RecSys</code>的吞吐量。</p>
<h2 id="背景">背景</h2>
<p>文章背景主要介绍了深度推荐模型的结构，以及深度推荐模型如何集成到Kubernetes集群中，为用户提供线上推理服务。</p>
<h3 id="深度推荐模型dlrm">深度推荐模型（DLRM）</h3>
<center>
<img src="/imgs/ElasticRec/DLRM.jpg" width='500'/>
</center>
<p>如图所示，深度推荐模型（<code>DLRM</code>）包含3个主要组件：</p>
<ul>
<li><code>Bottom MLP</code>
<ul>
<li>输入：dense input（比如 用户年龄）</li>
<li>输出：dense output（高维特征）</li>
<li>类型：<strong>计算敏感型</strong></li>
</ul></li>
<li><code>Embedding Table</code>
<ul>
<li>输入：多个 sparse input（比如 商品ID）</li>
<li>输出：dense output（高维特征）</li>
<li>类型：<strong>内存敏感型</strong></li>
<li>作用：根据稀疏输入得到<code>Embedding Table</code>中的高维特征。一般来说，一次查询中所有的sparse
input得到的dense
output会执行<code>pool</code>操作进行池化，得到单个dense output</li>
</ul></li>
<li><code>Top MLP</code>
<ul>
<li>输入：<code>Bottom MLP</code>的输出 拼接
<code>Embedding Table</code>的输出</li>
<li>输出：给商品打分</li>
</ul></li>
</ul>
<blockquote>
<p>在生产环境中，由于商品（item）的种类非常多，比如Amazon有数亿的商品种类。<code>Embedding Table</code>为每个商品种类都维护了一个特征，导致<code>Embedding Table</code>的大小可以达到<strong>几十GB</strong>，相比于<code>Bottom MLP</code>，有几点需要关注：</p>
<ol type="1">
<li><code>Embedding Table</code>对于计算不敏感，即<code>pool</code>操作并不需要太多计算资源；相反，对于内存带宽限制极为敏感，特别是有非常多的dense
output需要<code>pool</code>时</li>
<li>在<code>DLRM</code>中，<code>Embedding Table</code>通常有<u>多个</u>，对内存的压力是极大的</li>
</ol>
</blockquote>
<h3 id="模型服务架构model-serving-architectures">模型服务架构（Model
Serving Architectures）</h3>
<h4 id="模型容器化">模型容器化</h4>
<p>这篇文章关注的是<code>DLRM</code>的推理。在线上应用中，<code>DLRM</code>被打包成镜像，镜像中包含了模型参数以及常用的机器学习库，以容器的方式运行在Kubernetes集群中，如Fig.
2（a）所示。</p>
<center>
<img src="/imgs/ElasticRec/DLRM_inference.png" width='500'/>
</center>
<h4 id="模型的自动伸缩">模型的自动伸缩</h4>
<p><strong>Kubernetes</strong>是一个容器编排工具，它能管理容器的生命周期，对容器进行自动化调度、资源分配。</p>
<p><strong>吞吐量</strong>是一个衡量在线服务性能的指标，单位是QPS（query
per second），吞吐量越高，代表在线服务单位时间内处理请求的数量</p>
<p>对于<code>DLRM</code>而言，处理单个请求的时间基本可以看作变化很小的，那么为提高吞吐量，可以采用Kubernetes的水平pod伸缩（Horizontal
Pod Autoscaling，HPA）机制对<code>DLRM</code>进行副本复制，如Fig. 2 (b)
所示，增加<code>DLRM</code>的副本数可以提高系统的并行处理能力，从而增大吞吐量</p>
<p>然而，HPA 是一种 model-wise
的分配方案，它将整个<code>DLRM</code>模型进行复制，包括内存占用非常大的<code>Embedding Table</code>模块，<u>但实际上<code>Embedding Table</code>并不涉及复杂的计算，所以一般不是（不是绝对）吞吐量的瓶颈所在。</u>无脑进行HPA势必会造成大量内存浪费。</p>
<h4 id="模型的硬件约束">模型的硬件约束</h4>
<p>因为 大型 <code>DLRM</code> 的 <code>Embedding Table</code>
通常有几十GB，将 <code>Embedding Table</code>
全部放在高内存带宽的GPU中通常不太可行，所以会退而求其次的使用如下两种方式：①
CPU-only ② CPU-GPU。</p>
<ul>
<li><strong>CPU-only</strong>：<code>Bottom MLP</code> 和
<code>Embedding Table</code> 均运行在CPU</li>
<li><strong>CPU-GPU</strong>：<code>Bottom MLP</code> 运行在GPU，
<code>Embedding Table</code> 均运行在CPU</li>
</ul>
<p>可以看到， <code>Embedding Table</code>
都运行在CPU关联的内存上，如果能优化这一部分的内存使用，就可以提升<code>DLRM</code>的最大副本数量，从而提高系统的吞吐量。</p>
<h2 id="动机">动机</h2>
<p>文章的动机从两点出发，阐述为什么现有的资源分配方案会导致次优性能（sub-optimal
performance）：</p>
<ol type="1">
<li><code>RecSys</code>的不同模块具有异构资源需求</li>
<li><code>Embedding Table</code>不同部分的访问频率相差极大</li>
</ol>
<h3 id="异构资源需求">异构资源需求</h3>
<p>Fig. 3 (a)
展示了三个推荐模型（RM1，RM2，RM3）的不同模块<code>Bottom MLP</code> 和
<code>Embedding Table</code> 在 ①计算复杂度（FLOPS）和 ②内存大小
上的差别。可以看出：<u><code>Bottom MLP</code>在计算复杂度上远高于
<code>Embedding Table</code>，但是内存占用远远小于
<code>Embedding Table</code></u></p>
<p>Fig.3 (b)
展示了三个推荐模型的不同模块在两种硬件约束下的延时占比。原文虽然没有讨论，但可以推测，<u>在CPU-only架构下，推理的延时开销主要集中在<code>Bottom MLP</code>的计算；在CPU-GPU架构下，延时的开销在于将 <code>Embedding Table</code>
的数据从CPU传输到GPU</u></p>
<center>
<img src='/imgs/ElasticRec/异构资源需求.png' width='500'/>
</center>
<p>此外，文章还讨论了吞吐量的瓶颈问题，Fig.
4展示了作者的想法，实际上<code>Bottom MLP</code>
计算开销大，内存占用小，适合扩充副本来提升吞吐量；而
<code>Embedding Table</code>
本身吞吐量就很大，但是内存占用大，所以对于副本扩充应该谨慎。</p>
<center>
<img src='/imgs/ElasticRec/吞吐量.png' width='500'/>
</center>
<p>当然，作者还通过实验，验证了不同硬件约束下不同模块的吞吐量存在差异，来支撑上述想法：</p>
<center>
<img src='/imgs/ElasticRec/图5.png' width='500'/>
</center>
<blockquote>
<p><font color='blue'>综上所述，文章说明<code>RecSys</code>中不同模块的<strong>异构资源需求</strong>，以及<strong>吞吐量的差异</strong>。为后续对不同模块分别进行切分提供了实验依据</font></p>
</blockquote>
<h3 id="embedding-table-的倾斜访问模式">Embedding Table
的倾斜访问模式</h3>
<p>这一个实证分析较为简单，主要验证<code>Embedding Table</code>不同索引的访问频率的差异，如Fig.
6所示，在三个数据集中，大部分的访问集中在少数的索引（热点嵌入，hot
embeddings）</p>
<center>
<img src='/imgs/ElasticRec/倾斜访问.png' width='500'/>
</center>
<blockquote>
<p><font color='blue'>换句话说，将资源选择性地分配给 hot
embeddings，可以在提升吞吐量的同时，达到节省资源的目的</font></p>
</blockquote>
<h2 id="方法设计">方法设计</h2>
<p>Fig. 7展示了ElasticRec的系统架构，整体思路分为三个模块：</p>
<ol type="1">
<li>部署开销估计</li>
<li>基于动态规划（DP）的<code>Embedding Table</code>划分</li>
<li>推理时重索引</li>
</ol>
<center>
<img src='/imgs/ElasticRec/架构.png' width='500'/>
</center>
<h3 id="部署开销估计">部署开销估计</h3>
<p><strong>前置处理</strong>：将<code>Embedding Table</code>的index按照访问频率从大到小排序，hot
embeddings集中在最左侧</p>
<center>
<img src='/imgs/ElasticRec/sort.png' width='500'/>
</center>
<p>根据动机中提到的“<strong>将资源选择性地分配给 hot
embeddings，可以在提升吞吐量的同时，达到节省资源的目的</strong>”，文章将<code>Embedding Table</code>切分为shards，每个shard包含了<code>Embedding Table</code>的一部分index。<u>那么如何切分<code>Embedding Table</code>，以及如何评估切分策略的优劣呢？</u></p>
<p>文章首先定义了如何评估切分策略的优劣，切分策略的优劣由<font color='red'>固定吞吐量的前提下，所有shard的内存开销决定</font>。用最少的内存达到目标吞吐量，评估算法如下：</p>
<center>
<img src='/imgs/ElasticRec/cost_estimate.png' width='500'/>
</center>
<p>算法入口为<code>COST(k, j)</code>，表示范围为[k,j]的shard的内存消耗，这个内存消耗由两部分组成：</p>
<ul>
<li><code>REPLICAS(k,j)</code>：
计算特定吞吐量下，shard应该被分配的副本数量
<ol type="1">
<li>计算shard被访问的概率<code>probability</code>和被访问的向量数<code>ns</code>，<code>probability = CDF(j)- CDF(k)</code>，<code>ns = probability × nt</code></li>
<li>估计单个shard的副本在给定的访问向量数<code>ns</code>下能达到的<code>QPS</code>，<code>estimated QPS = QPS(ns)</code>，这里的<code>QPS()</code>是一个回归模型，可以线下测试得到</li>
<li>估计达到目标吞吐量<code>target_traffic</code>需要的副本数，<code>num_replicas = target_traffic/estimated_QPS</code></li>
</ol></li>
<li><code>CAPACITY(k,j)</code>：对于shard的每一个副本，计算存储embedding的内存开销
<ul>
<li>直接计算shard的副本大小：<code>(j − k +1)×(size_of_a_single_embedding_vector)</code></li>
</ul></li>
</ul>
<blockquote>
<p>这里需要特别注意回归模型<code>QPS()</code>，输入的参数除了需要访问的向量数<code>ns</code>外，还需要考虑向量本身的大小，如下图所示，QPS既与向量数有关，也与向量本身维度相关</p>
</blockquote>
<center>
<img src='/imgs/ElasticRec/QPS_predict.png' width='500'/>
</center>
<h3
id="基于dp的embedding-table分区算法">基于DP的<code>Embedding Table</code>分区算法</h3>
<p>在上一节中，当给定一个shard划分策略，我们可以评估每个shard在目标QPS下的内存开销，进而可以尝试找到给定QPS下最小内存开销的分区策略</p>
<p><u>这里文章有一个前提，即无论怎么划分，所有shard的目标QPS都是一样的，这样可以保证不存在多余的资源浪费</u></p>
<p>这个分区问题有两个操作：① 分多少shard
②每个shard的范围。假设我们用<span
class="math inline">\(Mem[num_{shards}][x]\)</span>表示<code>Embedding Table</code>在<span
class="math inline">\([0:x]\)</span>范围下，分区数量为<span
class="math inline">\(num_{shards}\)</span>的最小内存开销。那么这个问题具有两个明显的特性：<strong>最优子结构</strong>和<strong>重叠子问题</strong></p>
<ul>
<li><p><strong>最优子结构</strong>：<span
class="math inline">\(Mem[num_{shards}][x]\)</span>可以由子问题的最优解构造而来，假设最后一个shard的大小为<span
class="math inline">\(m\)</span>，那么可以简化表示为<span
class="math inline">\(Mem[num_{shards}][x] =
min(Mem[num_{shards}-1][x-m] + COST(m))\)</span>，比如下图中，<span
class="math inline">\(Mem[3][5]=min(Mem[2][5-m]+COST(m))=Mem[2][3]+COST(4,5)=4\)</span></p>
<center>
<p><img src='/imgs/ElasticRec/DP.png' width='500'/></p>
</center></li>
<li><p><strong>重叠子问题</strong>：求解过程中会反复遇到相同的子问题，需要将结果存储到表中，避免重复计算</p></li>
</ul>
<p>因此，自然而然可以想到用动态规划（DP）求解，文章给出的算法如下：</p>
<center>
<img src='/imgs/ElasticRec/DP_algo.png' width='500'/>
</center>
<p>最后根据最小内存开销回溯DP表可以得到分区策略</p>
<h3 id="推理时重索引">推理时重索引</h3>
<p>这个部分的重点在于分区后，如何根据原始<code>Embedding Table</code>的index
ID找到对应的shard中的某个embedding，以及确认index分别属于哪个input（为提高吞吐量，一个query包含了多个input）。</p>
<center>
<img src='/imgs/ElasticRec/remap.png' width='500'/>
</center>
<p>文章提出了两种索引：</p>
<ul>
<li><code>indices</code>：存储一次query要从<code>Embedding Table</code>中查找的具体ID。</li>
<li><code>offset</code>：指示每个input对应的的<code>indices</code>中的起始位置。</li>
</ul>
<p><strong>对于Fig.
11（a）未分区前</strong>，一个query（<code>indices</code>）包含了两个input，分别为红色的[1,
7]，灰色的[3,4,8]，<code>offset</code>表示第一个input要从<code>indices</code>第0个元素算起，第二个input从<code>indices</code>第1个元素算起，即input1为[1,7]，input2为[3,4,8]</p>
<p><strong>对于Fig.
11（b）分区后</strong>，首先计算中间的<code>indices</code>，具体为根据<code>indices</code>中的index计算应该被分到哪个分区（减去之前分区的大小），可以很容易把<code>indices</code>划分为shard
A 的[1,3,4]和shard B
的[7,8]，同时把<code>offset</code>进行划分（基于indices）</p>
<p><strong>对于Fig.
11（b）分区后，由于各shard索引重置了</strong>，所以需要在中间的<code>indices</code>和<code>offset</code>的基础上，进行重索引，具体为减去之前分区的大小，比如Fig.
11(b) 中 shard B的[7,8]减去shard
A的大小后，变成[1,2]。这样便可以直接从各shard中查找embedding了</p>
<h3 id="最终部署">最终部署</h3>
<p>因为Kubernetes的 horizontal pod autoscaling
提供了弹性伸缩时参考指标的接口，对于：</p>
<ul>
<li><code>Embedding Table</code>的shard，文章直接将每个shard可以承受的最大吞吐量作为参考指标，到达最大吞吐量则扩容</li>
<li><code>Bottom MLP</code>，则采用SLA的65%作为扩容的阈值</li>
</ul>
<p>（这里不是很明白为什么要采用不同的阈值指标，为什么不都用吞吐量？）</p>
<h2 id="实验部分">实验部分</h2>
<p>文章分别验证了 <code>ElasticRec</code> 在 <strong>CPU-only</strong>
以及 <strong>CPU-GPU</strong> 环境下的性能表现：</p>
<ul>
<li><strong>CPU-only</strong>：本地集群（1 master + 11 worknode）</li>
<li><strong>CPU-GPU</strong>：云集群（20 CPU-GPU nodes，GPU为 NVIDIA
Tesla T4，节点间有31Gbps的带宽）</li>
</ul>
<p><code>DLRM</code>模型的开发是基于facebook的dlrm<a
href="#dlrm"><sup>1</sup></a>。Kubernetes自动伸缩器采用的custom
metric来自prometheus。SLA设置为400ms。而对于验证的<code>DLRM</code>模型，作者选择了三个先进的推荐模型（RM1,
RM2,
RM3），并在RM1的基础上进行参数改造，设置了很多个microbenchmarks，参数变动和三个RM模型如下：</p>
<center>
<img src='/imgs/ElasticRec/config.png'/>
</center>
<p>其中 Locality 指标 <span class="math inline">\(P\)</span>
代表多少请求分布在前10%的热点向量，<span
class="math inline">\(P\)</span>越大，代表请求分布越集中在热点。</p>
<h3 id="microbenchmarks实验">Microbenchmarks实验</h3>
<p>文章一开始探讨了不同RM配置下的内存消耗：</p>
<center>
<img src='/imgs/ElasticRec/Microbenchmarks.png' width='500'/>
</center>
<ol type="1">
<li><strong>MLP size</strong>：随着MLP
size的扩大，计算复杂度提高，延时会逐渐违背SLA。<code>Model-wise</code>的方法会扩容整个模型，而<code>ElasticRec</code>只需要扩容内存开销极小的<code>Bottom MLP</code>，所以内存消耗差距很大</li>
<li><strong>Locality</strong>：访问越集中在热点，<code>ELasticRec</code>效果越好，因为只需要扩容热点那一部分</li>
<li><strong>Number of
tables</strong>：系统中可能不止一个<code>Embedding Table</code>，比如用户ID表和商品ID表，随着表数量的增多，<code>ELasticRec</code>对每个表都进行分片，降低扩容时的内存开销</li>
<li><strong>Number of
shards</strong>：分片数量并不是越多越好，因为每个shard会有最小内存消耗（程序运行必须的消耗），即算法1中的min_mem_alloc，所以当分片数量大于4后，效果没有那么明显了</li>
</ol>
<h3 id="不同-rm-在-cpu-only-环境下的性能">不同 RM 在 CPU-only
环境下的性能</h3>
<p>文章接下来在<strong>CPU-only</strong>环境下，比较了<code>ElasticRec</code>和<code>Model-wise</code>方法在三个推荐模型（RM1，RM2，RM3）的性能表现。以下实验都是在吞吐量为100QPS的下进行的</p>
<ol type="1">
<li>Fig. 13 展示了<strong>内存消耗</strong>的对比</li>
<li>Fig. 14
展示了<strong>内存利用率</strong>的对比，这里的<strong>内存利用率</strong>是作者自己定义的，表示<u>当前shard在前1000个请求中被访问的embedding的比例</u>，可以看出<code>Model-wise</code>只有一个shard（S1），并且<strong>内存利用率</strong>很低，对整个shard扩容显得很不值；<code>ElasticRec</code>有4个shard，前3个<strong>内存利用率</strong>很高（高频shard），最后一个非常低。</li>
<li>Fig. 15
展示了两种方法在吞吐量为100QPS所需要消耗的CPU服务器数量。这里我不太清楚是如何算出需要消耗的CPU服务器数量的（一般算的是虚拟CPU使用量？）</li>
</ol>
<center>
<img src='/imgs/ElasticRec/cpu-only-exp.png'/>
</center>
<h3 id="不同-rm-在-cpu-gpu-环境下的性能">不同 RM 在 CPU-GPU
环境下的性能</h3>
<p>在<strong>CPU-GPU</strong>环境下，<code>ElasticRec</code>将 MLP
模块设计为 GPU-centric 容器，将 Embedding Table 模块设计为只用 CPU
的容器；<code>Model-wise</code>
则将CPU和GPU都分配给一个容器。以下实验都是在吞吐量为100QPS的下进行的，实验效果如下</p>
<center>
<img src='/imgs/ElasticRec/CPU-GPU-exp.png'/>
</center>
<h3 id="动态输入流量实验">动态输入流量实验</h3>
<p>前几个实验都是在固定吞吐量（QPS=100）下进行的，这个实验动态调整流量大小，然后观察<code>ElasticRec</code>和<code>Model-wise</code>的吞吐量表现、资源消耗以及尾部延时。</p>
<p>流量大小先逐步增大，然后降到一个固定值</p>
<center>
<img src='/imgs/ElasticRec/dynamic-workload.png' width="500"/>
</center>
<p>可以发现，<code>ElasticRec</code>的吞吐量、内存消耗以及SLA违背都低于对比方法</p>
<h3 id="与-gpu-embedding-caches-的对比">与 GPU Embedding Caches
的对比</h3>
<p>GPU Embedding Caches
方法是之前的一个工作，原理是把<code>Embedding Table</code>的hot
embeddings存到GPU缓存中，能缓解CPU内存带宽压力（减少CPU与GPU的交互）</p>
<center>
<img src='/imgs/ElasticRec/compare-GPU-cache.png'/>
</center>
<p>文章对比了 <code>Model-wise</code>、<code>Model-wise (cache)</code>
和 <code>ElasticRec</code> 在200 QPS
的内存消耗，<code>ElasticRec</code>的内存消耗仍然是最低的</p>
<blockquote>
<p>这里我很好奇为什么不比较延时？<code>ElasticRec</code>延时应该比不过<code>Model-wise (cache)</code>，毕竟CPU和GPU交互需要时间。</p>
</blockquote>
<div>
<p><a name="dlrm"></a> [1]
<a>https://github.com/facebookresearch/dlrm</a></p>
</div>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>推荐系统</tag>
        <tag>资源管理</tag>
        <tag>面向AI的微服务</tag>
        <tag>ISCA</tag>
        <tag>2024</tag>
      </tags>
  </entry>
  <entry>
    <title>最长公共子序列（Longest Common Subsequence）</title>
    <url>/2024/11/20/LCS/</url>
    <content><![CDATA[<p>最长公共子序列（LCS）是一个在一个序列集合中用来查找所有序列中最长子序列的问题。这与查找最长公共子串的问题不同的地方是：<strong>子序列不需要在原序列中占用连续的位置。而最长公共子串（要求连续）和最长公共子序列是不同的。</strong></p>
<p>比如：字符串 A<font color=red>BC</font>BD<font color=red>AB</font> 和
字符串 <font color=red>B</font>D<font color=red>CAB</font> 的LCS为
<font color=red>BCAB</font>。</p>
<blockquote>
<p>LCS在计算机领域有诸多应用，比如可以：</p>
<ul>
<li>比较 DNA 序列或蛋白质序列。</li>
<li>比较不同版本的文件，找出更改的部分</li>
<li>文本（代码）相似性检</li>
<li>...</li>
</ul>
<p>假设有两个版本的文件：</p>
<ul>
<li>文件 V1: The quick brown fox jumps over the lazy dog.</li>
<li>文件 V2: A quick brown dog jumps over the lazy cat.</li>
</ul>
<p>通过 LCS 算法，可以找到它们的最长公共子序列为 quick brown jumps over
the lazy，剩余的部分为更改，这有助于生成补丁文件和合并冲突。</p>
</blockquote>
<h2 id="题目">题目</h2>
<p>leetcode 1143 是最长公共子序列的经典问题： &gt; 给定两个字符串 text1
和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在
公共子序列 ，返回 0 。 &gt; &gt;一个字符串的 子序列
是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。
&gt; &gt;- 例如，"ace" 是 "abcde" 的子序列，但 "aec" 不是 "abcde"
的子序列。 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。
&gt; &gt; 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。</p>
<p>示例 1：</p>
<p>输入：text1 = "abcde", text2 = "ace"<br />
输出：3<br />
解释：最长公共子序列是 "ace" ，它的长度为 3 。</p>
<p>示例 2：</p>
<p>输入：text1 = "abc", text2 = "abc"<br />
输出：3<br />
解释：最长公共子序列是 "abc" ，它的长度为 3 。</p>
<p>示例 3：</p>
<p>输入：text1 = "abc", text2 = "def"<br />
输出：0<br />
解释：两个字符串没有公共子序列，返回 0 。</p>
<p>提示：</p>
<ul>
<li>1 &lt;= text1.length, text2.length &lt;= 1000</li>
<li>text1 和 text2 仅由小写英文字符组成。</li>
</ul>
<h2 id="分析">分析</h2>
<p>最长公共子序列（Longest Common Subsequence,
LCS）问题非常适合使用动态规划来解决，原因在于它具备了动态规划的两个关键特性：<font color=red>最优子结构</font>和<font color=red>重叠子问题</font>。</p>
<ol type="1">
<li><p><strong>最优子结构</strong>：LCS问题的最优解可以由其子问题的最优解构建而来。具体来说：</p>
<ul>
<li>如果两个序列的最后一个字符相同，那么这个字符必定是LCS的一部分，接下来的问题就转化为了求这两个序列去掉最后一个字符之后的LCS。即
<span class="math inline">\(LCS(i, j) = LCS(i-1, j-1) + 1\)</span></li>
<li>如果两个序列的最后一个字符不同，则LCS不会同时包含这两个字符，问题转化为求一个序列去掉最后一个字符之后与另一个序列的LCS。即
<span class="math inline">\(LCS(i, j) = max(LCS(i-1, j), LCS(i,
j-1))\)</span></li>
</ul>
<p>这种性质允许我们将大问题分解为更小的子问题，通过解决这些子问题来构建原始问题的解。</p></li>
<li><p><strong>重叠子问题</strong>：在求解 LCS
的过程中，我们会反复遇到相同的子问题。例如，在计算两个序列 <span
class="math inline">\(X\)</span> 和 <span
class="math inline">\(Y\)</span> 的LCS时，可能会多次计算 <span
class="math inline">\(X\)</span> 的前 <span
class="math inline">\(i\)</span> 个字符和 <span
class="math inline">\(Y\)</span> 的前 <span
class="math inline">\(j\)</span>
个字符的LCS。由于这些子问题会被多次求解，我们可以将它们的结果存储起来，避免重复计算，这就是动态规划中所谓的“记忆化”。</p></li>
</ol>
<h2 id="求解">求解</h2>
<p>动态规划的求解步骤如下：</p>
<ol type="1">
<li><p>定义状态</p>
<ul>
<li>在最长公共子序列（LCS）问题中，状态可以用一个二维数组 <span
class="math inline">\(dp\)</span> 表示，其中 <span
class="math inline">\(dp[i][j]\)</span> 表示序列 <span
class="math inline">\(X\)</span> 的前 <span
class="math inline">\(i\)</span> 个字符和序列 <span
class="math inline">\(Y\)</span> 的前 <span
class="math inline">\(j\)</span> 个字符的最长公共子序列的长度。</li>
</ul></li>
<li><p>状态转移方程</p>
<ul>
<li><p>在上节最优子结构的判断中，已经定义出了状态转移方程：</p>
<ul>
<li>if <span class="math inline">\(X[i]==Y[j]\)</span>， <span
class="math inline">\(dp[i][j] = dp[i-1][j-1] + 1\)</span></li>
<li>if <span class="math inline">\(X[i]!=Y[j]\)</span>， <span
class="math inline">\(dp[i][j] = max(dp[i-1][j],
dp[i][j-1])\)</span></li>
</ul></li>
</ul></li>
</ol>
<p>python代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonSubsequence</span><span class="params">(self, text1, text2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type text1: str</span></span><br><span class="line"><span class="string">        :type text2: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n, m = len(text1), len(text2)</span><br><span class="line">        <span class="comment"># 多声明一行一列，方便计算dp[1][1]</span></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(m+<span class="number">1</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, m+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> text1[i<span class="number">-1</span>] == text2[j<span class="number">-1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i][j<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> dp[n][m]</span><br></pre></td></tr></table></figure>
<p>最终的<span class="math inline">\(dp\)</span>表为：</p>
<p>[0, 0, 0, 0]</p>
<p>[0, 1, 1, 1]</p>
<p>[0, 1, 1, 1]</p>
<p>[0, 1, 2, 2]</p>
<p>[0, 1, 2, 2]</p>
<p>[0, 1, 2, 3]</p>
<h2 id="扩充">扩充</h2>
<p>如果题目要求我们求出具体的最长公共子序列呢？我们可以根据<span
class="math inline">\(dp\)</span>表进行回溯，思路为</p>
<ol type="1">
<li>从<span class="math inline">\(dp[n][m]\)</span>开始向前回溯</li>
<li>如果当前<span
class="math inline">\(X[i]==Y[j]\)</span>，那么说明此时<span
class="math inline">\(X[i]\)</span>（<span
class="math inline">\(Y[j]\)</span>）属于LCS的一部分，则加入LCS；同时<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>都向前推进一位</li>
<li>如果当前<span
class="math inline">\(X[i]!=Y[j]\)</span>，那么我们需要找出LCS是在<span
class="math inline">\(X[i-1]\)</span>和<span
class="math inline">\(Y[j]\)</span>中产生，还是在<span
class="math inline">\(X[i]\)</span>和<span
class="math inline">\(Y[j-1]\)</span>中产生，则只需要对比<span
class="math inline">\(dp[i-1][j]\)</span> 和 <span
class="math inline">\(dp[i][j-1]\)</span></li>
</ol>
<p>所以代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonSubsequence</span><span class="params">(self, text1, text2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type text1: str</span></span><br><span class="line"><span class="string">        :type text2: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n, m = len(text1), len(text2)</span><br><span class="line">        <span class="comment"># 多声明一行一列，方便计算dp[1][1]</span></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(m+<span class="number">1</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> range(n+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, m+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> text1[i<span class="number">-1</span>] == text2[j<span class="number">-1</span>]:</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i][j<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># 回溯</span></span><br><span class="line">        i, j = n, m</span><br><span class="line">        lcs = []</span><br><span class="line">        <span class="keyword">while</span> (i &gt; <span class="number">0</span> <span class="keyword">and</span> j &gt; <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">if</span> text1[i<span class="number">-1</span>]==text2[j<span class="number">-1</span>]:</span><br><span class="line">                lcs.append(text1[i<span class="number">-1</span>])</span><br><span class="line">                i-=<span class="number">1</span></span><br><span class="line">                j-=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> dp[i<span class="number">-1</span>][j] &gt; dp[i][j<span class="number">-1</span>]:</span><br><span class="line">                    i-=<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    j-=<span class="number">1</span></span><br><span class="line">        lcs_str = <span class="string">''</span>.join(lcs)[::<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[n][m], lcs_str</span><br><span class="line"></span><br><span class="line">solution = Solution()</span><br><span class="line">lcs_len, lcs = solution.longestCommonSubsequence(<span class="string">'abcde'</span>, <span class="string">'ace'</span>)</span><br><span class="line">print(lcs_len, lcs)</span><br></pre></td></tr></table></figure>
<p>回溯路径为：</p>
<p>[0, 0, 0, 0]</p>
<p>[0, <font color=blue>1</font>, 1, 1]</p>
<p>[0, <font color=blue>1</font>, 1, 1]</p>
<p>[0, 1, <font color=blue>2</font>, 2]</p>
<p>[0, 1, <font color=blue>2</font>, 2]</p>
<p>[0, 1, 2, <font color=blue>3</font>]</p>
]]></content>
      <categories>
        <category>leetcode基础算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>[ICPP 2023] On Optimizing Traffic Scheduling for Multi-replica Containerized Microservices</title>
    <url>/2024/12/21/OptTraffic/</url>
    <content><![CDATA[<blockquote>
<p>题目：On Optimizing Traffic Scheduling for Multi-replica
Containerized Microservices</p>
<p>ICPP 2023</p>
<p>作者：中国科学技术大学，华为</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>基于容器的微服务系统部署已经变得越来越重要，为了应对高并发（high
concurrency）和提升错误容忍度（fault
tolerance），微服务系统通常会引入多个副本（replicas）。目前有两种类别的服务部署方法：</p>
<ul>
<li><code>resource-friendly</code>：将 pod
尽可能分散部署到不同机器上，目标是<u>平均各机器的资源使用率</u>。（e.g.,
Kubernetes，Docker Swarm，OpenShift）</li>
<li><code>traffic localization</code>：将流量（traffic）交互频繁的
pod放到同一台主机上，目标是<u>减少跨主机流量导致的性能降级</u>。（e.g.,
CA-WFD，Blender，NetMARKS）</li>
</ul>
<p>这两种方法各有利弊：①
<code>resource-friendly</code>存在大量跨主机流量（cross-machine
traffic），可能会引发性能降级；②
<code>traffic localization</code>在多replicas情况下，由于机器资源不足，也会存在大量cross-machine
traffic。</p>
<p>文章提出了一种网络感知的流量调度（traffic scheduling）方法
<code>OptTraffic</code>，首先通过轻量级监控方法估计容器之间的流量大小，然后通过算法分配容器之间的流量比例（默认应该是负载均衡），目标是减少
cross-machine traffic，并尽可能平均各主机的资源利用率。</p>
<h2 id="背景">背景</h2>
<h3 id="基于容器的微服务">基于容器的微服务</h3>
<p>文章首先介绍了微服务是什么，这里就不进行赘述了。然后介绍了基于容器的微服务部署方法：</p>
<p>首先是传统的容器部署，代表为K8s，文章说是基于<code>resource-friendly</code>的，简而言之，就是只考虑容器的CPU和内存需求。这里我先查阅了K8s调度器
kube-scheduler 的<a
href="https://kuboard.cn/">说明</a>，以下是文档的内容：</p>
<blockquote>
调度器执行步骤如下：
<center>
<img src="/imgs/OptTraffic/k8s-schedule.png" width='700'/>
</center>
<ol type="1">
<li>找出该 Pod 的所有
<strong>可选节点</strong>，这个过程称为过滤（filtering）</li>
<li>按照某种方式对每一个 <strong>可选节点</strong>
评分，这个过程称为打分（scoring）</li>
<li>选择评分最高的 <strong>可选节点</strong></li>
<li>将最终选择结果通知 API Server，这个过程称为绑定（binding）</li>
</ol>
<p>文章的重点在打分，假设全部节点皆可部署，则有如下打分策略：</p>
<ul>
<li><code>SelectorSpreadPriority</code>：将 Pod
分散到不同的节点，主要考虑同属于一个
Service、StatefulSet、Deployment的情况</li>
<li><code>InterPodAffinityPriority</code>：遍历 weightedPodAffinityTerm
并求和，找出结果最高的节点</li>
<li><code>LeastRequestedPriority</code>：已被消耗的资源最少的节点得分最高。如果节点上的
Pod 越多，被消耗的资源越多，则评分约低</li>
<li><code>MostRequestedPriority</code>：已被消耗的资源最多的节点得分最高。此策略会把
Pod 尽量集中到集群中的少数节点上</li>
<li><code>RequestedToCapacityRatioPriority</code>：按 requested /
capacity 的百分比评分</li>
<li><code>BalancedResourceAllocation</code>：资源使用均衡的节点评分高</li>
<li><code>NodePreferAvoidPodsPriority</code>：根据节点的 annotation
scheduler.alpha.kubernetes.io/preferAvoidPods 评分。可使用此 annotation
（容忍度 Toleration 和 污点 Taint）标识哪些 Pod
不能够运行在同一个节点上</li>
<li><code>NodeAffinityPriority</code>：基于
PreferredDuringSchedulingIgnoredDuringExecution 指定的 node affinity
偏好评分。参考 将容器组调度到指定的节点</li>
<li><code>TaintTolerationPriority</code>：
根据节点上不可容忍的污点数评分</li>
<li><code>ImageLocalityPriority</code>：有限选择已经有该 Pod
所需容器镜像的节点</li>
<li><code>ServiceSpreadingPriority</code>：确保 Service 的所有 Pod
尽量分布在不同的节点上。</li>
<li><code>CalculateAntiAffinityPriorityMap</code>：anti-affinty，参考将容器组调度到指定的节点</li>
<li><code>EqualPriorityMap</code>：为每个节点指定相同的权重</li>
</ul>
<p>这些打分策略都有一定的权重，最终的分数计算如下： <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">finalScoreNode &#x3D; (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + ...</span><br></pre></td></tr></table></figure></p>
<p>经过查阅<a
href="https://www.qikqiak.com/k8strain2/scheduler/overview/">资料</a>，默认开启的调度代码如下，可以看出，<u>在所有节点都有对应镜像，没有亲和性和污点干扰的情况下，k8s其实偏向于<code>resource-friendly</code>，即分布更加均匀，相关策略有BalancedAllocationName、LeastAllocatedName</u>：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Score: &amp;schedulerapi.PluginSet&#123;</span><br><span class="line">           Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">               &#123;Name: noderesources.BalancedAllocationName, Weight: 1&#125;,</span><br><span class="line">               &#123;Name: imagelocality.Name, Weight: 1&#125;,</span><br><span class="line">               &#123;Name: interpodaffinity.Name, Weight: 1&#125;,</span><br><span class="line">               &#123;Name: noderesources.LeastAllocatedName, Weight: 1&#125;,</span><br><span class="line">               &#123;Name: nodeaffinity.Name, Weight: 1&#125;,</span><br><span class="line">               &#123;Name: nodepreferavoidpods.Name, Weight: 10000&#125;,</span><br><span class="line">               &#x2F;&#x2F; Weight is doubled because:</span><br><span class="line">               &#x2F;&#x2F; - This is a score coming from user preference.</span><br><span class="line">               &#x2F;&#x2F; - It makes its signal comparable to NodeResourcesLeastAllocated.</span><br><span class="line">              &#123;Name: podtopologyspread.Name, Weight: 2&#125;,</span><br><span class="line">               &#123;Name: tainttoleration.Name, Weight: 1&#125;,</span><br><span class="line">           &#125;,</span><br><span class="line">       &#125;,</span><br></pre></td></tr></table></figure>
总的来说，K8s默认是<code>resource-friendly</code>的说法倒也没啥问题~~~</p>
</blockquote>
<p>然后文章介绍了集中式部署，即以减少cross-machine
traffic为目标的<code>traffic localization</code>方法，这里进行了一个实证分析来论证cross-machine
traffic的影响，具体实验方法为：</p>
<blockquote>
将经典应用 socialNetwork 部署到2台主机，并用 <a
href="https://github.com/Mellanox/sockperf">sockperf</a>
模拟两个主机之间的流量干扰，测量不同QPS下不同带宽的延时的P99分布
<center>
<img src="/imgs/OptTraffic/fig 2.png" width='500'/>
</center>
<p><font color='blue'>图中显示QPS增大时，同样带宽下的P99延时明显增大，所以cross-machine
traffic受带宽限制，会影响延时。</font>但这有没有可能是 POD
资源不够导致的（这里应该给 POD 设置足够充足的资源来消除影响）？</p>
</blockquote>
<h3 id="traffic-localization-及其局限"><code>Traffic localization</code>
及其局限</h3>
<p>为了解决cross-machine traffic带来的性能降级问题，许多工作提出了
<code>traffic localization</code> 的部署方法，有两种实现方式：</p>
<ul>
<li>将属于同一个application的所有容器部署到相同或邻近的主机</li>
<li>将流量交互密切的容器对（container pairs）调度到相同或邻近的主机</li>
</ul>
<p><code>traffic localization</code> 不仅可以减少 cross-machine traffic
带来的<strong>网络延迟</strong>（由传输路径和带宽大小决定），也可以减少对网络数据包操作（包的封装和解析）带来的<strong>计算开销</strong>。</p>
<blockquote>
<p>为了验证 <code>traffic localization</code>
对于节省网络流量和降低响应延时的作用，文章在两种部署方式上进行实验：</p>
<ol type="1">
<li>部署 socialNetwork 到 5 台主机上</li>
<li>部署 socialNetwork 到 1
台主机上（<code>traffic localization</code>）</li>
</ol>
<center>
<img src="/imgs/OptTraffic/fig 3.png" width='500'/>
</center>
<p><font color='blue'>图（a）显示 <code>traffic localization</code>
可以减少大量的cross-machine traffic。图（b）显示
<code>traffic localization</code> 可以大幅度减少响应延时</font></p>
</blockquote>
<p><strong>Limitation</strong>：<code>traffic localization</code>虽然能显著减少cross-machine
traffic来提升性能，但是通常很难完全将一个 application
的所有微服务都部署到一台主机上，原因在于微服务还会产生多个replicas，除此之外，监控容器之间的流量也会引入较大的CPU负载，<code>traffic localization</code>也会造成机器资源使用不均。具体如下：</p>
<p>（1）<strong>当 POD
分布在不同主机时</strong>。因为负载均衡，流量会均匀流向不同主机的POD，造成大量的cross-machine
traffic</p>
<p>（2）<strong>高CPU负载</strong>。对容器间流量的监控一般需要抓取数据包，对数据包的封装和解析过程会造成高昂的CPU开销。比如socialNetwork有27个容器，消耗13个CPU核，而加上监控装置（e.g.,
iftop）会多消耗 1.4-2 个
CPU核【istio也存在这样的问题，甚至sidecar消耗更多】</p>
<p>（3）<strong>不均衡的资源使用</strong>。许多pod有着相同的资源需求，比如都是CPU密集型，将他们放置在一台主机上会造成资源使用不均衡，即CPU过载，但是内存利用率低。除此之外，将所有pod放在一台主机上会使得fault
tolerance降低。</p>
<h2 id="方法设计">方法设计</h2>
<center>
<img src="/imgs/OptTraffic/structure.png" width='500'/>
</center>
<p><u><code>OptTraffic</code> 的目标是最小化 cross-machine
traffic</u>。整体架构如上图所示。包含三个组件：</p>
<ol type="1">
<li><code>Traffic Monitor</code>。这个组件用轻量级的方式估计每个
container pair 之间的流量大小。做法为收集每个容器的 incoming 和 outgoing
traffic，然后构建一个 traffic graph，最后通过简单的数学计算估计每个
container pair 之间的流量大小</li>
<li><code>Traffic Allocator</code>。流量调度模块，决定 upstream
container 流向 每个 downstream container
的流量比例。具体策略为本地优先（local-first），以减少跨主机流量</li>
<li><code>Container Scheduler</code>。异步调整 container pairs
的部署，以实现机器之间的负载均衡。</li>
</ol>
<h3 id="traffic-monitor">Traffic Monitor</h3>
<p><code>Traffic Monitor</code> 有两个目的：①
识别频繁交互（heavy-traffic）的链路 ② 为链路调度进行优先级排序</p>
<p>现有两种方式进行容器间流量的监控：</p>
<ol type="1">
<li>在容器所在POD加一个sidecar，接管流量的转发和监控（e.g.,
istio）。</li>
<li>使用网络包嗅探工具在userspace进行解析（e.g., tcpdump and
iftop）。</li>
</ol>
<p>这两种方式都会造成高昂的CPU负载。文章还特别提到了在kernel space
工作的
eBPF，可以减少监控负载，但仍需要分析网络包，并且实现复杂。所以作者想用数学方法来减少监控带来的负载。</p>
<blockquote>
<p>eBPF
避免用户态与内核态切换：传统上，像tcpdump这样的工具是在用户空间工作的，它们需要将网络数据包从内核空间复制到用户空间进行分析。这个过程涉及上下文切换和内存拷贝，带来了额外的CPU开销。而eBPF程序则可以在内核空间内部处理数据包，无需这种昂贵的数据传输。</p>
</blockquote>
<p><strong>Step 1: 监控容器流量</strong>。</p>
<center>
<img src="/imgs/OptTraffic/fig 6.png" width='500'/>
</center>
<p>如 fig 6 所示，一个upstream container 可以与多个 downstream
container进行交互，<code>OptTraffic</code>首先使用 Prometheus
监控每个container接收和发送的traffic</p>
<blockquote>
<p>Prometheus 只是一个收集工具，数据源来自于
操作系统记录的每个容器接收和发送的包的大小（这些数据已经保存在主机上，所以只需要周期性地获取就行，几乎没有监控成本），数据保存在
proc 文件系统中，经过查资料，应该用的是这两个指标：</p>
<ul>
<li>container_network_receive_bytes_total</li>
<li>container_network_transmit_bytes_total</li>
</ul>
</blockquote>
<p><strong>Step 2: 计算链路流量</strong>。链路流量（Link
Traffic）指的是特定的两个容器之间的流量大小，<code>OptTraffic</code>
直接使用数学方法计算得到，减少监控的负载（注意：上一步只能监控到每个容器的接收和发送的traffic大小，不能知道与特定容器之间的traffic大小）。计算步骤如下，从入度为
1 的节点 <span class="math inline">\(p\)</span> （根节点）开始，先将与
<span class="math inline">\(p\)</span> 相关的两条边赋值（即&lt;<span
class="math inline">\(p\)</span>,<span
class="math inline">\(q\)</span>&gt;和&lt;<span
class="math inline">\(q\)</span>,<span
class="math inline">\(p\)</span>&gt;），这两条边的值完全等于 <span
class="math inline">\(p\)</span> 的 send 和 receive 的 traffic 。然后将
<span class="math inline">\(p\)</span> 相关的流量从 <span
class="math inline">\(q\)</span> 中减去，从图中删去 <span
class="math inline">\(p\)</span>，重复如上过程，直到所有节点计算完毕。</p>
<blockquote>
<ol type="1">
<li>构造微服务系统的
DAG。（可以通过通过配置文件、官方文档或者其他监控手段得到）</li>
<li>基于 DAG 构造 traffic graph。traffic
graph有着和DAG相同的构造，唯一的不同是，traffic graph是双向边，代表
traffic 也可以从 downstream container 发往 upstream container
（response）</li>
<li>基于 traffic graph 计算每个 link 的traffic大小，算法如下：</li>
</ol>
<center>
<img src="/imgs/OptTraffic/algo 1.png" width='500'/>
</center>
<p><code>GraphMerge</code> 和
<code>BreakCycle</code>两个函数用于解决图中存在环的情况。如下图所示，环有两种情况：</p>
<ol type="1">
<li>upstream containers 来自不同微服务 （Fig. 7(b)）</li>
<li>upstream containers 来自同一个微服务的不同replicas （Fig.
7(c)）</li>
</ol>
<center>
<img src="/imgs/OptTraffic/fig 7.png" width='500'/>
</center>
<p>对于 case1，<code>BreakCycle</code> 函数首先选择环中 incoming 和
outgoing traffic 最少的 container
进行讨论，然后通过网络包嗅探工具监控这个 container 每条边 send 和
receive 的traffic。然后在相关 containers 中移除这个 container
的traffic，最后删除这个container，这样就可以破坏环。</p>
<p>对于case2，<code>GraphMerge</code> 函数将属于同一个微服务的 replicas
进行合并，至于每个 replica 分得的
traffic，可以通过流量分配策略进行估算</p>
<p><strong>负载分析</strong>：从设计上看，<code>OptTrace</code>
确实轻量级，因为不需要监控每条 link 的 traffic，而每个 vertex 的 traffic
本身就被os记录下来了。即使有环，也只需要对一部分容器进行网络包嗅探分析，破环即可。算法复杂度为O(N)，N为节点数量。</p>
</blockquote>
<h3 id="traffic-allocation">Traffic Allocation</h3>
<p>这个模块是核心，决定了 upstream 微服务向某个 downstream 微服务的不同
replicas 发送 traffic 的比例。downstream
微服务的replicas可能部署在不同主机上，所以策略不再是简单的负载均衡</p>
<center>
<img src="/imgs/OptTraffic/fig 8.png" width='500'/>
</center>
<p>如上图所示，有2个 upstream containers 和 3个downstream
containers，并部署在2台主机上。如果是按照负载均衡的原则，假设每个upstream
container发送 1 traffic，则发往每个downstream container <span
class="math inline">\(\frac{1}{3}\)</span> traffic，每个downstream
containers总共需要接收 <span class="math inline">\(\frac{2}{3}\)</span>
的traffic。从Fig 8 (a)中看出，有 <span
class="math inline">\(1=\frac{1}{3}+\frac{1}{3}+\frac{1}{3}\)</span>
（一半）的traffic是cross-machine的。</p>
<blockquote>
<p>文章介绍了 <code>Two-step allocation</code> 方法来进行 traffic
分配：</p>
<ol type="1">
<li>第一步，采用 <strong>local-first</strong>
原则，即traffic尽可能分配到同台主机的containers。假设机器 <span
class="math inline">\(i\)</span> 有 <span
class="math inline">\(m_i\)</span> 个upstream containers 和 <span
class="math inline">\(n_i\)</span> 个 downstream
containers，则每个upstream container需要发送给每个downstream
container比例为 <span class="math inline">\(min(\frac{m/n}{m_i},
\frac{1}{n_i})\)</span>的traffic。<font color='blue'>这里的比例很简单也很有趣，每个downstream
container接收到的traffic是 <span class="math inline">\(m_i\times
min(\frac{m/n}{m_i}, \frac{1}{n_i})=min(m/n,
m_i/n_i)\)</span>，第一个是全局负载均衡的traffic大小，第二个是单机上负载均衡的traffic大小。这里的原理是：使得低于负载均衡的traffic尽可能发往本机的container，但这个traffic尽量不高于全局负载均衡的值</font>。所以
Fig 8 (b) 中 DC3 的 traffic 大小为 <span
class="math inline">\(min(\frac{2}{3}, 1)=\frac{2}{3}\)</span></li>
<li>第二步，这一步叫<code>padding</code>，即将第一步中剩余未分配的traffic导向其余downstream
containers，这些traffic就是cross-machine traffic了。如 Fig 8
(c)所示。</li>
</ol>
<p>看了一下<a
href="https://github.com/sirius8050/OptTraffic">源代码</a>，流量调度用的是iptable实现的，具体指令类似如下这样
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo iptables -t nat -I KUBE-SVC-KVWZRLJ3RRU55EYE 1 -s 0.0.0.0&#x2F;0 -j KUBE-SEP-5EMJYGBVFF6DIW4U -m statistic --mode random  --probability 0.17</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>以上是论文的核心，至于动态调度那一块，首先是没看懂，可能以后有时间再读读</p>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>流量调度</tag>
        <tag>ICPP</tag>
        <tag>2023</tag>
      </tags>
  </entry>
  <entry>
    <title>[ICSE 2021] MicroHECL: High-Efficient Root Cause Localization in Large-Scale Microservice Systems</title>
    <url>/2024/05/13/MicoHECL/</url>
    <content><![CDATA[<blockquote>
<p>题目：MicroHECL: High-Efficient Root Cause Localization in
Large-Scale Microservice Systems</p>
<p>来源：ICSE 2021</p>
<p>作者：复旦大学CodeWisdom团队，阿里云</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>微服务系统的可用性问题直接影响了业务的运行，这些问题通常由各种各样的故障类型以及服务间故障的传播造成。如何设计精准且高效定位故障根因的方法成为了一个重大挑战。然而，现有基于服务调用图的方法在<font color=red>异常检测的准确率</font>和<font color=red>图游走的效率</font>上存在不足。本文提出了一种高效的根因定位方法MicroHECL，通过如下步骤定位故障根因：</p>
<ul>
<li>动态构建一段时间窗口内的服务调用图</li>
<li>对不同异常类型进行个性化异常检测</li>
<li>对不同异常类型分析异常传播链路，通过剪枝提高效率</li>
</ul>
<h2 id="背景">背景</h2>
<p>工业微服务系统包含大量的微服务（e.g.,
alibaba有3000个微服务，300个子系统）。一个服务都可能运行在成百上千个容器中，并时常动态创建和销毁。服务之间也存在复杂的调用关系（同步、异步）。</p>
<p>微服务系统可用性问题可能由不同类型的异常引起，每种异常都由一组<strong>指标</strong>表示。异常可能源自服务并沿服务调用传播，最终导致可用性问题。文章具体关注三种故障类型（就是谷歌提到的几种黄金指标）：</p>
<ul>
<li>性能异常（Performance Anomaly）</li>
<li>可靠性异常（Reliability Anomaly）</li>
<li>流量异常（Traffic Anomaly）</li>
</ul>
<h2 id="microhecl-概述">MicroHECL 概述</h2>
<center>
<img src="/imgs/MicroHECL/MicroHECL.png"/>
</center>
<p>MicroHECL支持三种故障类型的检测和诊断：性能异常、可靠性异常和流量异常。最终输出候选的故障根因服务排名。</p>
<h3 id="服务调用图构建">服务调用图构建</h3>
<p>当MicroHECL检测到异常后（<code>3-sigma</code>），会启动根因分析流程，首先就是构建过去<strong><em>30min</em></strong>内的服务依赖图（service
call
graph）。图上的节点就是每一个微服务；图上的边代表服务之间的调用关系，比如<span
class="math inline">\(S_i \to S_j\)</span>代表微服务<span
class="math inline">\(S_i\)</span>调用了微服务<span
class="math inline">\(S_j\)</span>；节点上具有一些属性：响应时间（RT），错误数量（EC）以及每秒请求量（QPS）。</p>
<h3 id="异常传播链分析">异常传播链分析</h3>
<p>检测到异常的微服务并不一定是故障根因，但是故障根因一般在这个微服务的上游或者下游。异常传播链分析的目的是<strong>筛选初始异常服务中可能的异常传播链来识别一组候选根本原因服务</strong>。整个过程由以下几步组成：</p>
<ul>
<li>分析入口服务（即最初汇报异常的微服务，后面会混用）</li>
<li>异常传播链扩展</li>
<li>根因排序</li>
</ul>
<h4 id="分析入口服务">分析入口服务</h4>
文章首先根据经验定义了三种故障类型的传播方向：
<center>
<img src="/imgs/MicroHECL/direction.png" width="500"/>
</center>
<p>性能异常和可靠性异常的传播方向很好理解，因为上游服务的响应时间和状态码受下游服务影响。流量异常的传播方向是从上游到下游，原因是【笔者自己的理解】上游服务发生了故障（比如网络拥塞），那么发送到下游的流量会大幅减少，所以下游服务会出现QPS急剧减少的异常。这个结论也可以在ImpactTracer<a
href="#ImpactTracer"><sup>1</sup></a>中找到。</p>
<center>
<img src="/imgs/MicroHECL/chain.png"/>
</center>
<p>有了故障的传播方向，文章从<strong>入口服务开始，向邻居节点不断扩展分析</strong>。如图Fig.
2所示，整个过程描述如下： &gt; 1. 将入口服务<span
class="math inline">\(S_5\)</span>纳入异常传播链 &gt; 2.
异常检测。检测<span class="math inline">\(S_5\)</span>的邻居节点<span
class="math inline">\(S_4\)</span>和<span
class="math inline">\(S_7\)</span>的异常类型 &gt; 3. 确认<span
class="math inline">\(S_4\)</span>的异常类型为<strong>Traffic
Anomaly</strong>，<span
class="math inline">\(S_7\)</span>的异常类型为<strong>Performance
Anomaly</strong> &gt; 4. 检测是否符合传播方向（<span
class="math inline">\(S_4\)</span>是否是<span
class="math inline">\(S_5\)</span>的上游，<span
class="math inline">\(S_7\)</span>是否是<span
class="math inline">\(S_5\)</span>的下游） &gt; 5. 符合，将<span
class="math inline">\(S_4\)</span>和<span
class="math inline">\(S_7\)</span>添加到异常传播链 &gt; 6. 从<span
class="math inline">\(S_4\)</span>和<span
class="math inline">\(S_7\)</span>出发，对邻居节点重复上述步骤</p>
<p>以上过程其实就是故障的溯源，图中的箭头可以看作故障的传播路径。过程中涉及的异常检测在<a
href="#33-服务异常检测">3.3节</a>会提到。</p>
<h4 id="异常传播链扩展">异常传播链扩展</h4>
<p>过程与3.2.1中描述的扩展过程一致。对于每个检测到的上游/下游异常节点，将其添加到异常传播链中。当无法向链中添加更多节点时，异常传播链的扩展结束。比如Fig.
2对于<span class="math inline">\(S_4\)</span>方向的传播分析，以<span
class="math inline">\(S_1\)</span>结束；对于<span
class="math inline">\(S_7\)</span>方向的传播分析，以<span
class="math inline">\(S_9\)</span>和<span
class="math inline">\(S_{10}\)</span>结束。</p>
<h4 id="候选根因定位">候选根因定位</h4>
<p>本文选择异常传播链的末端服务作为候选根因，比如Fig.
2中的候选根因服务为<span class="math inline">\(S_1\)</span>，<span
class="math inline">\(S_9\)</span>和<span
class="math inline">\(S_{10}\)</span>。那么如何排名呢？</p>
<ul>
<li>选取入口服务过去60min的业务指标 <span
class="math inline">\(X\)</span></li>
<li>选取候选根因服务过去60分钟的质量指标（<code>RT</code>,
<code>EC</code> or <code>QPS</code>）<span
class="math inline">\(Y\)</span></li>
<li>计算两者之间的皮尔逊相关系数：</li>
</ul>
<p><span class="math display">\[
   P(X,
Y)=\frac{\sum_{i=1}^n{(X_i-\overline{X})(Y_i-\overline{Y})}}{\sqrt{\sum_{i=1}^n{(X_i-\overline{X})^2}\sum_{i=1}^n{(Y_i-\overline{Y})^2}}}
\]</span></p>
<p>皮尔逊相关系数范围为[-1,1]，绝对值越接近1则表明相关性越大。所以，根因则根据皮尔逊相关系数的绝对值来排序。</p>
<h3 id="服务异常检测">服务异常检测</h3>
<p>这篇文章的重点应该是放在了如何设计精准的异常检测上。不同于以往的方法只使用一种异常检测手段，本文对三种故障类型（Performance
Anomaly，Reliability Anomaly，Traffic
Anomaly）分别设计了异常检测方法。</p>
<p>这三种方法分别对应三种指标：响应时间（<code>RT</code>），错误数量（<code>EC</code>）以及每秒请求量（<code>QPS</code>），以下是阿里巴巴监控系统中获取的异常案例：</p>
<center>
<img src="/imgs/MicroHECL/metrics.png"/>
</center>
<h4 id="性能异常检测">性能异常检测</h4>
<p>在<code>RT</code>的异常检测中，需要考虑<code>RT</code>可能存在的周期性（如Fig.
3
(d)）所示，简单的使用3-sigma方法会将这种正常周期波动视为异常。所以不仅需要考虑当前期间的质量指标，还需要考虑<strong>前一天</strong>和<strong>前一周同一天</strong>的质量指标。</p>
<p>本文使用OC-SVM训练异常检测模型，OC-SVM是一种常用的无监督机器学习模型，常用于异常检测和分类。文章为<code>RT</code>构建了以下4种特征：</p>
<blockquote>
<ul>
<li>当前检测窗口中<code>RT</code>的值大于给定比较时间窗口内<code>RT</code>的最大值的数量。</li>
<li>当前检测窗口中<code>RT</code>的最大值与给定比较时间窗口内的<code>RT</code>最大值的差值。</li>
<li>当前检测时间窗口中超过给定比较时间窗口中<code>RT</code>滑动平均值最大值的数量。</li>
<li>当前检测窗口中<code>RT</code>的平均值与给定时间窗口内<code>RT</code>的滑动平均值的最大值的比值。</li>
</ul>
</blockquote>
<p>其中，当前检测窗口大小为<strong><em>10min</em></strong>，给定比较时间窗口有3种：①过去一小时、②前一天同一小时、③前一周同一天的同一小时。（如果数据保存没那么完善和严格的话，笔者认为定义一段正常时间为比较窗口应该也是可以接受的）。所以一共有3*4=12种特征。</p>
<p>对于模型训练和验证，文章拿10000样本作为训练集，600个正负比例1:1的样本作为测试集。</p>
<h4 id="可靠性异常检测">可靠性异常检测</h4>
<p>这里提到<code>EC</code>大多时候都是0（Fig.2
(b,c)），偶尔会出现少许波动，但很快会恢复（比如断路器打开时<code>EC</code>升高，关闭后<code>EC</code>降低），也没有周期性，如果用性能异常的模型，则会出现大量误报（少许波动都会算进去）。</p>
<p>所以，文章采用随机森林（Random
Forest，RF）来分类，文章为<code>EC</code>构建了以下5种特征：</p>
<blockquote>
<ul>
<li>计算最近一小时的<code>EC</code>和前一天同一时间段的<code>EC</code>的增量；使用3-Sigma规则识别当前检测窗口中可能存在的增量异常值；如果存在，则返回异常值的平均值作为特征值，否则返回0。</li>
<li>计算最近一小时内<code>EC</code>值和每一个值的前一分钟<code>EC</code>值的增量；使用3-Sigma规则识别当前检测窗口中可能存在的增量异常值；如果存在，则返回异常值的平均值作为特征值，否则返回0。</li>
<li>检测窗口内的平均<code>RT</code>是否大于设定的阈值（例如，在本文的线上系统中，阈值设置为50ms）</li>
<li>检测窗口内最大错误率（<code>EC</code>/sum(<code>QPS</code>)）。</li>
<li>检测窗口内<code>RT</code>和<code>EC</code>的皮尔逊相关系数</li>
</ul>
</blockquote>
<p>其中，当前检测窗口大小为<strong><em>10min</em></strong>，对于模型训练和验证，文章拿1000样本作为训练集（有标签，正负比1:3），400个正负比例5:3的样本作为测试集。</p>
<h4 id="流量异常检测">流量异常检测</h4>
<p><code>QPS</code>大多满足正态分布（Fig.2
(c,f)），所以直接采用3-sigma进行检测。 &gt;
这里笔者有小小的疑问，QPS真的满足正态分布吗？在系统那边的文章，许多流量都是以泊松分布注入的</p>
<p>其中，当前检测窗口大小为<strong><em>10min</em></strong>，选择过去<strong><em>1h</em></strong>计算均值和标准差。3-sigma的均值和方差选择。为了进一步消除误报，还需要检测初始异常服务的<code>QPS</code>和业务指标（就是入口服务被异常检测的指标）的皮尔逊系数，如果大于0.9，则报告流量异常。</p>
<h4 id="剪枝">剪枝</h4>
<p>为了提高MicroHECL的异常回溯效率，需要控制指数增长的异常传播链分支数量，因为不断地进行异常检测也是非常耗时的。</p>
<p>核心思想：<font color='red'>异常传播链中的两个连续服务调用的相应质量指标应该具有相似变化趋势</font></p>
<blockquote>
<p>例子：Fig. 2中的 <span class="math inline">\(S_1 \to S_4\)</span> 和
<span class="math inline">\(S_4 \to S_5\)</span> 都是<strong>Traffic
Anomaly</strong> 的传播路径，如果<span class="math inline">\(S_1 \to
S_4\)</span> 的 <code>QPS</code> 和 <span class="math inline">\(S_4 \to
S_5\)</span> 的 <code>QPS</code>
没有相似的趋势（即皮尔逊相关系数&lt;0.7），则需要剪掉<span
class="math inline">\(S_1 \to S_4\)</span>，那么<span
class="math inline">\(S_4\)</span>就取代<span
class="math inline">\(S_1\)</span>变成了候选根因。</p>
</blockquote>
<p>这里的检测窗口选取的过去<strong><em>60min</em></strong>。剪枝操作执行在异常节点加入异常调用链之前。</p>
<h2 id="总结">总结</h2>
<p>文章思路挺好的，有理有据，方法朴实有效。写作的顺序不是传统的总分形式，首先就把整体流程讲完了，然后拿出异常检测和剪枝单独讲，初看有点不适应。</p>
<h2 id="参考文献">参考文献</h2>
<div>
<p><a name="ImpactTracer"></a> [1] Xie R, Yang J, Li J, et al.
ImpactTracer: Root Cause Localization in Microservices Based on Fault
Propagation Modeling, (DATE), 2023.
<a>https://ieeexplore.ieee.org/abstract/document/10137078/</a></p>
</div>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>trace</tag>
        <tag>根因定位</tag>
        <tag>ICSE</tag>
        <tag>2021</tag>
      </tags>
  </entry>
  <entry>
    <title>[TSC 2024] Diagnosing Performance Issues for Large-Scale  Microservice Systems with Heterogeneous Graph</title>
    <url>/2024/06/22/MicroDig/</url>
    <content><![CDATA[<blockquote>
<p>题目：Diagnosing Performance Issues for Large-Scale Microservice
Systems with Heterogeneous Graph</p>
<p>来源：TSC 2024</p>
<p>作者：南开大学AIOps@NKU团队，清华大学Netman实验室</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>微服务系统的可用性对于业务运营和企业声誉至关重要。然而，微服务系统的动态性和复杂性给大规模微服务系统的性能问题诊断带来了重大挑战。文章分析了腾讯性能故障的真实案例后，发现故障传播的<font color=red>因果关系</font>与服务之间的<font color=red>调用关系</font>不一致，所以之前基于调用关系的根因定位方法准确率不高。文章提出适用于大规模微服务系统的性能问题诊断方法，MicroDig，步骤如下：</p>
<ul>
<li>基于调用和微服务之间的因果关系构建异构传播图</li>
<li>采样面向异构的随机游走算法进行根因服务定位</li>
</ul>
<p>MicroDig在腾讯、Train-Ticket、银行三个数据集上能实现至少85%的top-3
accuracy。</p>
<h2 id="背景">背景</h2>
<p>随着微服务系统的快速演变和规模扩张，微服务自身固有的动态性和复杂性给系统的可靠性维护带来了挑战。当微服务系统发生性能异常时，需要及时定位到根因服务，并把问题工单发给对应微服务的团队。然而，由于微服务数量太过庞大（Alibaba有超过30000服务），并且服务之间交互复杂，性能异常在服务之间进行传播，导致大量服务同时异常，进而使得人工诊断变得耗时耗力。</p>
<p>有许多现有工作基于trace来进行根因分析。trace记录了每次请求的调用路径以及相关性能表现，然而，<font color=red>海量的traces会带来极大的存储开销</font>（eBay每天产生150
billion的traces）。所以，越来越多的公司只保留两个服务之间的端到端聚合调用（end-to-end
aggregated calls）。</p>
<p>有些工作已经使用了aggregated call（这里指的是Codewisdom团队的GMTA<a
href="#GMTA"><sup>1</sup></a>），采取模式匹配的方式进行根因定位，但需要非常充足的历史故障数据，这在现实场景中很难实现；也有一些工作基于因果图进行根因定位，他们的因果挖掘算法有极高的计算成本，并且准确率较低。</p>
注：aggregated call在GMTA中提到过，应该就是一段时间（比如1
min）内trace的聚合：
<center>
<img src="/imgs/MicroDig/GMTA-path.png"/>
</center>
<p>文章提出的MicroDig的核心思想是：调用关系不等于因果关系（在动机中有具体说明），于是在故障诊断前先构造因果图（节点是微服务和调用）。
【从相关工作的分析到方法的提出有点衔接生硬，可能是因为因果方面的分析放到了动机的原因】</p>
<h2 id="动机">动机</h2>
<h3 id="调用关系异常传播的因果关系">调用关系≠异常传播的因果关系</h3>
文章举了一个例子来说明这个观点：
<center>
<img src="/imgs/MicroDig/call-casual.png"/>
</center>
<p><span class="math inline">\(A \to B \to C\)</span>
的异常次数急剧增加，如果仅仅根据调用关系去分析异常传播，那么根因是<span
class="math inline">\(C\)</span>，然而，操作员却没有在<span
class="math inline">\(C\)</span>中发现有意义的故障报告。因为 <span
class="math inline">\(B\)</span> 已经用尽了文件描述符，无法建立与 <span
class="math inline">\(C\)</span> 的新连接，所以<span
class="math inline">\(B \to C\)</span>有大量的异常出现。所以根因是<span
class="math inline">\(B\)</span>不是<span
class="math inline">\(C\)</span>，这与调用关系的回溯是违背的。文章提到腾讯有35%的性能问题不能仅仅依靠调用关系回溯解决。</p>
<blockquote>
<p>所以异常的被调用服务不一定是根因，调用方和被调用方都有可能是根因</p>
</blockquote>
<h3 id="异构传播图">异构传播图</h3>
<p>由3.1可知，仅仅从调用关系分析异常传播是不够的，所以本文提出了一种异构传播图（heterogeneous
propagation graph）来描述故障传播的因果关系：</p>
<center>
<img src="/imgs/MicroDig/hpg.png"/>
</center>
<ul>
<li><p>如上图所示，<span class="math inline">\(R(A,B)\)</span> 代表<span
class="math inline">\(A \to B\)</span>的异常率（anomaly rate），<span
class="math inline">\(R(A)\)</span> 代表服务<span
class="math inline">\(A\)</span>
本身的异常率。注意，服务本身的异常率，如<span
class="math inline">\(R(A)\)</span>，在这个工作中是不可观测的；边的异常率，如<span
class="math inline">\(R(A,B)\)</span>，是可以被观测的。</p></li>
<li><p>因为3.1中展示了调用方和被调用方均可能贡献异常，所以每个服务都应该有一条指向调用边的因果线（比如<span
class="math inline">\(R(A) \to R(A,B)\)</span>）。</p></li>
<li><p>文章添加了一些假设：①服务之间是独立的，比如<span
class="math inline">\(R(A)\)</span>和<span
class="math inline">\(R(B)\)</span>是独立的【这个假设有点不太符合现实】，②没有交集的两条调用边是独立的，比如<span
class="math inline">\(R(A,B)\)</span>和<span
class="math inline">\(R(C,D)\)</span></p></li>
</ul>
<p>根据作者的设计，这里应该就能看到<span
class="math inline">\(R(B,C)\)</span>是受<span
class="math inline">\(R(B)\)</span>和<span
class="math inline">\(R(C)\)</span>影响的了，从某种意义上给3.1的问题提供了思路。</p>
<h2 id="microdig-架构">MicroDig 架构</h2>
<center>
<img src="/imgs/MicroDig/structure.png"/>
</center>
<p>可以看到MicroDig分为几个部分： 1. 性能监控 (Monitoring) 2.
相关调用的识别 (Association Call Identification) 3. 异构传播图的构建
(Heterogeneous Propagation Graph Construction) 4. 根因定位 (Root Cause
Localization)</p>
<h3 id="association-call-identification">Association Call
Identification</h3>
<p>对于大规模微服务系统，如果直接构造调用图，那么图中会包含大量与故障不相关的调用边。所以需要对边进行筛选。</p>
<h4 id="构造port-level-异常子图">构造<code>port-level</code>
异常子图</h4>
<p>文章首先构造 <code>port-level</code>
异常子图，<code>port-level</code>即接口级别，图中的节点都是接口，
具体步骤如下：</p>
<ol type="1">
<li>构造 <code>port-level</code>
调用图（为什么选用<code>port-level</code>）</li>
<li>在调用图上进行
<em>宽度优先搜索</em>，对于被遍历的边，采用<em>3-sigma 异常检测</em>
对边的异常率或者超时率进行检测，将异常边保留下来，就得到<code>port-level</code>异常子图</li>
</ol>
<blockquote>
<p>为什么先构造<code>port-level</code>异常子图，而不是直接构造<code>service-level</code>异常子图？因为一个service包含太多port，聚合后一些异常port的表现可能被其他正常port掩盖。</p>
</blockquote>
<h4 id="构造service-level-异常子图">构造<code>service-level</code>
异常子图</h4>
<p>聚合构造好的<code>port-level</code>异常子图，即把同一个服务的port节点合并为一个service节点，就得到了<code>service-level</code>异常子图。对于服务<span
class="math inline">\(S\)</span>和<span
class="math inline">\(S&#39;\)</span>，定义<span
class="math inline">\(F(p,p&#39;)\)</span>和<span
class="math inline">\(N(p,p&#39;)\)</span>分别为其中<code>port-level</code>边<span
class="math inline">\(p \to
p&#39;\)</span>的异常调用数和总调用数，那么时间点<span
class="math inline">\(t\)</span>的<span class="math inline">\(S \to
S&#39;\)</span>的异常率<span class="math inline">\(R_t(S,
S&#39;)\)</span>为：</p>
<p><span class="math display">\[
    R_t(S, S&#39;)=\frac{\sum_{p\in S, p&#39; \in
S&#39;}F_t(p,p&#39;)}{\sum_{p\in S, p&#39; \in S&#39;}N_t(p,p&#39;)}
\]</span></p>
<p>整个过程如图(a) (b)所示 ：</p>
<center>
<img src="/imgs/MicroDig/hpg-build.png"/>
</center>
<h4 id="构造-heterogeneous-propagation-graph">构造 Heterogeneous
Propagation Graph</h4>
<p>3.1
中提到调用关系≠故障传播的因果关系，所以<code>service-level</code>异常子图也不能直接用于根因定位，需要进一步构建Heterogeneous
Propagation Graph （HPG）：</p>
<center>
<img src="/imgs/MicroDig/hpg-algo.png"  width="500"/>
</center>
<p>原理很简单： 1.
<strong>设置service节点</strong>：把<code>service-level</code>异常子图的所有服务加入到
HPG 2. <strong>设置call节点</strong>：对于每个服务<span
class="math inline">\(S\)</span>，将<span
class="math inline">\(S\)</span>的出边和入边作为节点加入HPG 3.
<strong>call节点和service节点的关系</strong>：对于每个call节点（<span
class="math inline">\(S \to S&#39;\)</span>），设置 <span
class="math inline">\(S \to (S \to S&#39;)\)</span>，<span
class="math inline">\((S \to S&#39;) \to S&#39;\)</span> 4.
<strong>call节点和call节点的关系</strong>：对于每个服务<span
class="math inline">\(S\)</span>，设置：出边 <span
class="math inline">\(\to\)</span> 入边</p>
<h3 id="根因服务定位">根因服务定位</h3>
<p>异构传播图（HPG）有两种节点（service，call）和两种边（service <span
class="math inline">\(\to\)</span> call, call <span
class="math inline">\(\to\)</span>
call）。本文采取针对异构图的随机游走算法来定位根因：</p>
<center>
<img src="/imgs/MicroDig/HPG-example.png" width="300"/>
</center>
<h4 id="转移权重">转移权重</h4>
<p>随机游走的核心是<strong>定义不同边的游走权重</strong>：</p>
<ol type="1">
<li>对于call <span class="math inline">\(\to\)</span> call：比如<span
class="math inline">\(C_{23} \to
C_{12}\)</span>，通过计算这两个调用的异常率数组之间的相关系数来决定游走权重。</li>
<li>对于service <span class="math inline">\(\to\)</span> call：比如<span
class="math inline">\(S_1 \to C_{12}\)</span>
<ul>
<li><p>首先计算service的异常程度，定义<span
class="math inline">\(\mathbb{S}_U\)</span>和<span
class="math inline">\(\mathbb{S}_D\)</span>分别表示服务<span
class="math inline">\(S\)</span>的上游服务集合和下游服务集合，服务<span
class="math inline">\(S\)</span>的异常程度<span
class="math inline">\(\alpha_S\)</span>可以表示为： <span
class="math display">\[
\alpha_S = \frac{|\{S&#39;|S&#39;\in \mathbb{S}_U \cup \mathbb{S}_D,
\theta(S&#39;)=1 \}|}{|\mathbb{S}_U \cup \mathbb{S}_D|}
\]</span> 当<span
class="math inline">\(S&#39;\)</span>有任意一条<code>port-level</code>的边是异常时，<span
class="math inline">\(\theta(S&#39;)=1\)</span>。</p></li>
<li><p>然后计算service <span class="math inline">\(\to\)</span>
call的权重。对于任意一个call节点<span class="math inline">\(C=S_{caller}
\to S_{callee}\)</span>，有两条service <span
class="math inline">\(\to\)</span> call类型的边：<span
class="math inline">\(S_{caller} \to C\)</span> 和 <span
class="math inline">\(C \to
S_{callee}\)</span>。这两条边的权重分别为：<span
class="math inline">\(\omega_{caller}\)</span> 和 <span
class="math inline">\(\omega_{callee}\)</span>，分别计算如下： <span
class="math display">\[
\omega_{caller}=max(0, \Delta \eta)*[0.5+\beta sgn(\Delta \alpha)]
\]</span></p></li>
</ul>
<span class="math display">\[
   \omega_{callee}=max(0, \Delta \eta)*[0.5-\beta sgn(\Delta \alpha)]
\]</span> 其中，<span class="math inline">\(\Delta \alpha =
\alpha(S_{caller})-\alpha(S_{callee})\)</span>，<span
class="math inline">\(\Delta
\eta\)</span>即当前服务的所有入边的权重-所有出边的权重。</li>
</ol>
<h4 id="异构随机游走">异构随机游走</h4>
<p>与<code>Personal pageRank</code>不同的是，作者没有用个性化向量来跳出陷阱，而是在图上加了以下几种边来防止掉入陷阱：</p>
<ul>
<li><strong>backward
edge</strong>：如果有节点只有一条有向边连接，那么则加一个与有向边方向相反的backward
edge，权重是有向边的<span class="math inline">\(\rho\)</span>倍。</li>
<li><strong>self-loop edge</strong>：给每个节点加上自环边</li>
</ul>
<p>游走算法如下图所示，与普通的随机游走没有太大差别：</p>
<center>
<img src="/imgs/MicroDig/random-walk.png" width="500"/>
</center>
<h2 id="总结">总结</h2>
<ul>
<li><strong>创新点</strong>：这篇文章的创新点不是很突出，随机游走感觉已经玩烂了（如果随机游走上能再精进一下，可能会好一些）。。。但是异构图的构建还是让人耳目一新的</li>
<li><strong>动机</strong>：动机比较简单，没有实证分析（对腾讯数据的实证分析应该加上的）。</li>
<li><strong>代码复现</strong>：公布的代码里应该是没有完整数据的，其实除公司以外的测试数据集应该要公开的。</li>
</ul>
<h2 id="参考文献">参考文献</h2>
<div>
<p><a name="GMTA"></a> [1] Guo X, Peng X, Wang H, et al. Graph-based
trace analysis for microservice architecture understanding and problem
diagnosis, ESEC/FSE. 2020: 1387-1397.
<a>https://taoxiease.github.io/publications/esecfse20in-trace.pdf</a></p>
</div>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>trace</tag>
        <tag>根因定位</tag>
        <tag>TSC</tag>
        <tag>2024</tag>
      </tags>
  </entry>
  <entry>
    <title>[FSE 2024] TraStrainer: Adaptive Sampling for Distributed Traces with System Runtime State</title>
    <url>/2024/06/23/TraStrainer/</url>
    <content><![CDATA[<blockquote>
<p>题目：TraStrainer: Adaptive Sampling for Distributed Traces with
System Runtime State</p>
<p>来源：FSE 2024</p>
<p>作者：中山大学DDS实验室</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>微服务系统每天都会产生大量的trace数据，带来了极大的计算和存储成本。trace
sampling 技术被用来缓解这种压力。trace sampling 分为两种：</p>
<ul>
<li><code>random sampling</code>：又称 head
sampling，即以固定概率决定每条trace是否采样</li>
<li><code>biased sampling</code>：又称 tail
sampling，即根据trace的状态决定是否采样</li>
</ul>
<p>很明显，<code>random sampling</code>
实现起来简单，但无法保证得到高质量的采样数据；<code>biased sampling</code>
能够根据用户偏好进行采样（比如高延时、异常状态码）。</p>
<p>先前的 <code>biased sampling</code>
工作大多基于密度（diversity），即偏好采样那些少见（edge-case）的traces，常见（common-case）的traces则少采样一些。然而，作者认为仅根据trace的状态进行采样是不充分的，应该再考虑<font color=blue>当前系统运行状态（system
runtime
state）</font>，特别是系统处在故障状态时。（<em>作者很有想法，在trace采样中玩了多模态，引入了metric，我觉得陈鹏飞老师实验室的工作还是很扎实且新颖的</em>）</p>
<center>
<img src="/imgs/TraStrainer/trace-metric.png"/>
</center>
<p>本文提出了TraStrainer，从以下角度进行在线采样： -
考虑密度：采用一种可解释的编码方式将trace转化为向量，方便后续密度采样 -
考虑系统状态：结合当前系统各种运行指标生成偏好向量，方便后续系统采样 -
密度采样+系统采样 <span class="math inline">\(\to\)</span>
最终采样决策</p>
<h2 id="动机">动机</h2>
<p>陈鹏飞老师实验室有大量关于<strong>微服务系统的故障诊断</strong>的工作，其中有许多是基于trace进行分析的，比如<code>MicroRank</code>，<code>TraceRank</code>和<code>MicroSketch</code>。</p>
<p>trace采样是这些工作的上游任务，先前与<code>biased sampling</code>相关的工作都是基于密度的，目标是采样edge-case
traces，没有考虑过采样的traces对下游故障诊断工作的影响。作者从以下两点进行了分析：</p>
<ol type="1">
<li><p><strong>仅考虑edge-case traces是不够的</strong>。作者在此举例说明
common-case traces也有很大的用处:</p>
<ul>
<li><strong>common-case traces
可能与根因有关</strong>。比如线程池因为太多请求的到来而用尽，而这些与根因相关的请求的traces并不一定是异常的，也就被认定为common-case
traces。而我们分析这些common-case
traces，可以发现这个时刻有高峰流量（这个是我根据自己理解加的）。</li>
<li><strong>common-case traces
有利于下游的分析任务</strong>。很多工作比如TraceRCA，T-Rank，都需要common-case
traces来获得系统的正常模式，从而与故障时刻进行比对。</li>
</ul></li>
<li><p><strong>结合 system runtime state
有利于判断有价值的trace</strong>。作者拿了华为的一个真实场景进行分析，如Fig.
3所示，[a,b]时间段 Node A 的 MySQL服务进行全表查询，导致 Node A
的CPU被打满，到达 Node A
的请求变得异常。SREs通常先检查系统状态，发现CPU升高，然后分析经过 Node A
的traces。<font color=red>然而，如果只根据密度进行trace采样，那么[a,b]的traces将被采集的很少，因为还没有发生异常</font>。<font color=green>如果结合系统状态进行采样，那么[a,b]的traces将给予更高的采样权重（[a,b]存在CPU攀升）。</font></p></li>
</ol>
<center>
<img src="/imgs/TraStrainer/metric-importance.png"/>
</center>
<p>综上，作者认为应该在trace采样时不仅仅考虑traces之间的密度，也要引入对当前系统状态的考虑。</p>
<h2 id="问题定义">问题定义</h2>
<p>给定一段时间收集的traces <span
class="math inline">\(\mathcal{T}\)</span>、对应的系统状态指标 <span
class="math inline">\(\mathcal{M}\)</span>、采样率 <span
class="math inline">\(\beta\)</span>，需要对<span
class="math inline">\(\mathcal{T}\)</span>中每个trace <span
class="math inline">\(t\)</span> 计算采样概率 <span
class="math inline">\(\rho\)</span>。整个过程定义为：</p>
<p><span class="math display">\[
S_p(\beta, \mathcal{T}, \mathcal{M}, t) \to \rho, \mathcal{T&#39;}
\]</span></p>
<p>其中，<span class="math inline">\(\mathcal{T&#39;}\)</span>是<span
class="math inline">\(\mathcal{T}\)</span>的采样子集。</p>
<h2 id="trastrainer-概要">TraStrainer 概要</h2>
<center>
<img src="/imgs/TraStrainer/TraStrainer.png"/>
</center>
<p>TraStrainer的架构和其他在线采样器相似，包含以下模块：</p>
<ul>
<li><p><strong>Runtime Data Preprocessing</strong>：</p>
<ul>
<li>Trace Encoder：对trace进行结构和状态编码</li>
<li>System Bias Extractor：将当前系统状态指标进行编码</li>
</ul></li>
<li><p><strong>Comprehensive Sampling</strong>：</p>
<ul>
<li>System-Biased Sampler：优先采样与当前系统波动最相似的trace</li>
<li>Diversity-Biased Sampler：优先采样edge cases traces</li>
<li>Composite Sampler：结合上述两种采样器进行最终决策</li>
</ul></li>
</ul>
<h3 id="trace-encoder">Trace Encoder</h3>
<p>如Fig.5所示，trace的编码包含<strong>结构编码</strong>和<strong>状态编码</strong>两部分：</p>
<center>
<img src="/imgs/TraStrainer/encoder-example.png"/>
</center>
<p><strong>状态编码</strong>：结合 Fig. 5 的例子进行说明，Fig. 5 的Trace
Vector的上半部分展示了由指标（node，metric_name）构成的向量，比如指标<span
class="math inline">\(m_1\)</span>就是（<span
class="math inline">\(C\)</span>, <span
class="math inline">\(SQLConnectionTime\)</span>）。一条trace由各种span构成，文章的span携带了一些tag（比如Node和annotation）。为了计算<span
class="math inline">\(m_1\)</span>的值<span
class="math inline">\(f_{m_1}\)</span>，作者将所有与<span
class="math inline">\(m_1\)</span>相关的span的duration结合起来，具体计算如下：</p>
<p><span class="math display">\[
   f_{m_1}=(|S_a|+1)*\sum_{i=1}^{n}s_{m_1i}.duration
\]</span></p>
<p><span class="math inline">\(s_{m_1i}\)</span>即与指标<span
class="math inline">\(m_1\)</span>相关的span，而<span
class="math inline">\(|S_a|\)</span>即相关span中异常span的个数（状态码为error，Fig.5中为1）
&gt;
注：最开始不太理解这种设计，后来发现是作者将指标与对应的trace的状态信息（延时+状态码）联系起来，相当于量化了指标对trace状态的影响，非常巧妙。</p>
<p><strong>结构编码</strong>：这一块比较简单，即将trace看做一棵树，每层可能有多个span，这些spans由<code>parentSpan</code>、<code>method</code>以及<code>params</code>组成，每一层的spans都被编码为一个特征。这些特征共同组成一个vector。</p>
<h3 id="system-bias-extractor">System Bias Extractor</h3>
<p>这一部分的本质是衡量当前系统哪个指标比较重要，这个重要程度由指标的<strong>异常程度</strong>决定。每个指标的异常程度组合成一个一维的<code>preference vector</code>数组，</p>
<center>
<img src="/imgs/TraStrainer/metric-anomaly.png"/>
</center>
<p>作者认为基于统计模型的异常检测不准确，无法识别周期性；而基于LSTM和Transformer的深度学习模型在响应太慢，无法适应线上采样。所以最终采用<code>DLinear algorithm</code><a
href="#DLinear"><sup>1</sup></a>，如Fig.6所示，这个算法通过指标的历史时序数据预测当前值<span
class="math inline">\(v_k&#39;\)</span>，并通过以下公式计算指标异常程度：
<span class="math display">\[
   \alpha=\frac{v_k&#39;-v_k}{max(v_k&#39;, v_k)}
\]</span></p>
<p>这个公式通过预测值与真实值的差距计算异常度。所有指标<span
class="math inline">\(\mathcal{M}\)</span>的异常度拼在一起就是<code>preference vector</code>
<span class="math inline">\(\mathcal{P}\)</span>。</p>
<h3 id="system-biased-sampler">System-Biased Sampler</h3>
<p>System-Biased
Sampler的核心是优先考虑与当前系统指标波动最相似的traces（与motivation中的故障诊断对上）。那么需要对新到来的trace进行注意力评估和采样概率计算。</p>
<p>本文定义了一个固定长度look-back window，由<span
class="math inline">\(k\)</span>条最近收集的历史traces组成：<span
class="math inline">\([t_1,...,t_k]\)</span>。System-Biased
Sampler只需用到trace的状态编码部分，每条trace的状态向量由n个指标组成，表示为<span
class="math inline">\(t_i=[f_{1i},...,f_{ni}]\)</span>。对历史traces每一维指标计算均值<span
class="math inline">\(\mu_i\)</span>和标准差<span
class="math inline">\(\sigma_i\)</span>，则对新到来的trace <span
class="math inline">\(t_{k+1}\)</span> 的第<span
class="math inline">\(i\)</span>个指标注意力分数计算如下： <span
class="math display">\[
   a_i = \frac{|f_{ik+1}-\mu_i|}{\sigma_i}
\]</span></p>
<p><span
class="math inline">\(t_{k+1}\)</span>的所有指标的注意力分数记为 <span
class="math inline">\(\mathcal{A}=[a_1,...,a_n]\)</span>，TraStrainer通过将注意力分数<span
class="math inline">\(\mathcal{A}\)</span>和<code>preference vector</code>
<span class="math inline">\(\mathcal{P}\)</span>
进行点积得到面向系统的采样概率<span class="math inline">\(p_s\)</span>：
<span class="math display">\[
   p_s(t_{k+1})= \frac{2}{1+e^{-2\mathcal{P·\mathcal{A}(t_{k+1})}}}-1
\]</span></p>
<p>以上操作是将点积<span
class="math inline">\(P·\mathcal{A}(t_{k+1})\)</span>通过tanh函数映射到[0,1]范围，点积越大，代表当前trace与当前系统状态越相似，面向系统的采样概率越大。</p>
<h3 id="diversity-biased-sampler">Diversity-Biased Sampler</h3>
<p>Diversity-Biased Sampler的目标是考虑edge-case
traces（即少见的traces），这篇文章与先前工作一样基于聚类来筛选edge-case
traces。</p>
<p>论文将look-back
window的历史traces进行聚类（基于trace的特征），并计算每个类的质量（traces数量），并把新trace
<span class="math inline">\(t_{k+1}\)</span> 归于最近的类 <span
class="math inline">\(c_{k+1}&#39;\)</span>。<span
class="math inline">\(c_{k+1}&#39;\)</span>的质量为<span
class="math inline">\(ma_{k+1}&#39;\)</span>，计算 trace <span
class="math inline">\(t_{k+1}\)</span> 和 所属类<span
class="math inline">\(c_{k+1}&#39;\)</span> 之间的Jaccard相似度<span
class="math inline">\(si(t_{k+1})\)</span>。</p>
<p><strong>一般来说，所属类<span
class="math inline">\(c_{k+1}&#39;\)</span>的质量和<span
class="math inline">\(si(t_{k+1})\)</span>越小，代表所属类越稀有、新trace越独特，应该给予更高的采样概率</strong>。所以面向密度的采样概率<span
class="math inline">\(p_d(t_{k+1})\)</span>计算如下： <span
class="math display">\[
   p_d(t_{k+1})=\frac{\frac{1}{ma_{k+1}&#39;*si(t_{k+1})}}{\sum_{i=1}^{k+1}\frac{1}{ma_{i}&#39;*si(t_{i})}}
\]</span></p>
<h3 id="composite-sampler">Composite Sampler</h3>
<p>对于新到trace <span
class="math inline">\(t\)</span>，综合两个采样概率 <span
class="math inline">\(p_s(t)\)</span> 和 <span
class="math inline">\(p_d(t)\)</span> 后，考虑采样额度 <span
class="math inline">\(\beta\)</span>，基于动态投票机制（dynamic voting
mechanism）最终决策。</p>
<center>
<img src="/imgs/TraStrainer/vote.png"/>
</center>
<p>首先统计过去look-back window里采样概率 <span
class="math inline">\(\theta\)</span>，如果： - <span
class="math inline">\(\theta \geq
\beta\)</span>，必须两个采样决策都为True，才采样 - <span
class="math inline">\(\theta \leq
\beta\)</span>，只需要有一个采样决策为True，即可采样</p>
<div>
<p><a name="DLinear"></a> [1] Ailing Zeng, Muxi Chen, Lei Zhang, and
Qiang Xu. 2023. Are transformers effective for time series forecasting?.
In Proceedings of the AAAI conference on artificial intelligence, Vol.
37. 11121–11128.</p>
</div>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>trace</tag>
        <tag>trace采样</tag>
        <tag>FSE</tag>
        <tag>2024</tag>
      </tags>
  </entry>
  <entry>
    <title>[SoCC 2021] Characterizing Microservice Dependency and Performance: Alibaba Trace Analysis</title>
    <url>/2024/05/08/alibaba-traces-socc-2021/</url>
    <content><![CDATA[<blockquote>
<p>题目：Characterizing Microservice Dependency and Performance: Alibaba
Trace Analysis</p>
<p>来源：SoCC 2021</p>
<p>作者：中国科学院深圳先进技术研究院, 阿里巴巴</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>现在有大量针对微服务架构的研究，比如资源管理、弹性伸缩以及故障诊断等。但是目前仍缺乏针对生产环境中微服务特性的实证研究。这篇文章对阿里巴巴公布的trace数据集<a
href="#alibabaTrace"><sup>1</sup></a>进行了详细的实证分析，从以下角度揭示了<strong>生产环境</strong>下微服务系统的特点：</p>
<ul>
<li>微服务调用图的特点，与传统作业DAG的不同</li>
<li>无状态微服务之间的依赖关系</li>
<li>微服务系统的运行时性能受哪些因素的影响</li>
</ul>
<p>此外，现有的微服务benchmark也存在一些问题，如：</p>
<ul>
<li><b>规模太小</b>。经典的benchmark（如DeathStarBench<a
href="#DeathStarBench"><sup>2</sup></a>），只包含数个微服务（不超过40）。在这些小规模的微服务benchmark上得到的结论不一定能推广到生产环境中；</li>
<li><b>静态依赖</b>。这些benchmark的依赖关系都是静态的，无法模拟生产环境中常见的动态性。</li>
</ul>
<p>所以这篇文章还基于阿里巴巴的trace数据构建了一个仿真的数学模型，模拟大规模动态微服务系统。</p>
<h2 id="背景">背景</h2>
<h3 id="微服务架构">微服务架构</h3>
<p>这里首先介绍了微服务架构的调用图，以及图中常见的组件：</p>
<center>
<img src="/imgs/alibaba-traces-socc-2021/microservice.jpg"/>
</center>
<p>这里引入了几个关键术语：</p>
<ul>
<li><strong><em>Entering
Microservice</em></strong>：入口微服务，即请求进入微服务系统的入口。通常是前端微服务。</li>
<li><strong><em>UM,
DM</em></strong>：分别指代一条调用链路的上游微服务（upstream
microservice）和下游微服务（downstream microservice）。</li>
</ul>
<p>对于微服务种类，文章基于服务提供的功能将微服务划分为有状态微服务（stateful）和无状态微服务（stateless）。</p>
<ul>
<li><strong><em>stateful微服务</em></strong>：通常存储有一些状态数据，常见的有数据库（database）和缓存（memCached），它们大多的接口大多分为两类：reading
和 writing。</li>
<li><strong><em>stateless微服务</em></strong>：不存储状态数据，所以可以轻松的伸缩，它们通常提供成百上千个不同接口，用于完成不同的业务功能。</li>
</ul>
<p>对于微服务交互种类，文章基于交互协议划分了三种类别：</p>
<ul>
<li><strong><em>IP</em></strong>：进程间通信（Inter Process
communication），常发生在stateless微服务和stateful微服务之间。</li>
<li><strong><em>RPC</em></strong>：远程过程调用（Remote Procedure
Call），一种双向通信，DM必须返回给UM结果。</li>
<li><strong><em>MQ</em></strong>：消息队列（Message
Queue），一种单向通信，UM发送消息到第三方中间件（消息队列），消息队列储存这个消息，直到DM主动取出这个消息。</li>
</ul>
<p>一般来说，RPC效率高，MQ更加灵活。</p>
<p>此外，还介绍了两个概念：调用深度（call depth）和响应延迟（RT）。</p>
<ul>
<li><strong><em>call
depth</em></strong>：调用深度指调用图中最长的路径长度，比如Figure
1中的调用图长度为5。</li>
<li><strong><em>RT</em></strong>：从UM发出请求到UM收到回复的时长。即使同一种接口的请求也会因为参数、状态的不同产生差距较大的延时。</li>
</ul>
<h3 id="alibaba-trace">Alibaba Trace</h3>
<p>alibaba的trace与常见的trace数据模型不同<a
href="#OpenTracing"><sup>3</sup></a>，因为它更像一种多模态监控数据，包含了<strong>节点信息</strong>、<strong>指标</strong>以及<strong>调用链</strong>等。具体信息如下：</p>
<center>
<img src="/imgs/alibaba-traces-socc-2021/alibaba-trace.jpg" width = "500"/>
</center>
<ol type="1">
<li><p><strong><em>物理运行环境</em></strong>：阿里巴巴的集群采用K8s进行管理，整个集群运行在裸机云（bare-metal
cloud）上，服务与作业通常混合部署在一起以提高资源利用率。Figure 2 (a)
介绍了云上两种常见的运行方式：</p>
<ol type="1">
<li>Online
Services：比如微服务，运行在容器中，直接由K8s管理，有持续向外界提供服务的能力。（<u>stateful微服务一般部署在特定集群中，不参与混合部署</u>）</li>
<li>Offline
Jobs：这些作业一般都需要执行特定的任务，需要K8s事先为它们分配资源，然后调度到特定的机器上执行。</li>
</ol></li>
<li><p><strong><em>微服务系统指标</em></strong>：这个大概分为三个部分：硬件层（缓存命中率）、操作系统层（CPU利用率）、应用层（JVM垃圾回收），具体内容如Figure
2 （b）。</p></li>
<li><p><strong><em>微服务调用链</em></strong>：如Figure 2
(c)所示，大体上与OpenTracing的数据模型类似，但是摒弃了<code>spanID</code>和<code>parentSpanId</code>，只留下UM和DM的信息，并用<code>rpcId</code>来唯一标识一个trace内的不同调用，<code>Communication Paradigm</code>代表调用类型（又名<code>rpctype</code>，如rpc）。</p>
<center>
<p><img src="/imgs/alibaba-traces-socc-2021/trace-demo.jpg" width = "300"/></p>
</center></li>
<li><p><strong><em>聚合调用</em></strong>：如Figure 2
(d)所示，本质上是对单个微服务的调用信息进行聚合和统计。</p></li>
</ol>
<h2 id="调用图的剖析">调用图的剖析</h2>
<p>这一块内容很多，我只提炼出较为有意义的部分。<font color=red>这里的调用图（call
graph）并不是指整个微服务依赖图，应该指的是单个trace的拓扑图</font>。</p>
<h3 id="微服务调用图特征">微服务调用图特征</h3>
<p>作者在这里总结了三个特征，对下游任务非常有启发：</p>
<ol type="1">
<li><p><em>调用图的微服务数量呈现长尾分布</em></p>
<center>
<p><img src='/imgs/alibaba-traces-socc-2021/service-num-heavy-distribution.png'/></p>
</center></li>
</ol>
<blockquote>
<p><strong>现有的benchmark太小了</strong>：10%的调用图的微服务数量&gt;40，存在微服务数量&gt;100的调用图。
<strong>大量的Memcacheds</strong>：大规模的调用图中有一半的微服务都是Memcacheds，可能是为了减少RT。</p>
</blockquote>
<ol start="2" type="1">
<li><p><em>调用图是一棵树，并且很多图是一条长链路</em></p>
<center>
<p><img src='/imgs/alibaba-traces-socc-2021/depth-svcnum.png'/></p>
</center>
<center>
<p><img src='/imgs/alibaba-traces-socc-2021/heat.png'/></p>
</center></li>
</ol>
<blockquote>
<p><strong>较短的深度</strong>：一半的调用图深度在2~4 （a）
<strong>树有点胖</strong>：，深度随着微服务数量增加没有明显变化
（b），说明调用图是宽且浅的？很多下游微服务只是简单的查询数据（stateful微服务一般是叶子节点）
<strong>较深的图一般都是长链路</strong>：深度增加，但是后面的的微服务数量大多为1个，说明这棵树的宽度基本集中在第2层，后面的都是一条长链路</p>
<p>有些下游任务（弹性伸缩）会对调用图进行编码，作者特别提到有些图有很长的深度，会让这些任务产生很大的模型以及过拟合。我觉得这没有直接关系，这些数量远远达不到图网络的极限。而且这个实验也可以反过来说，大部分图深度都是很短的。</p>
</blockquote>
<ol start="3" type="1">
<li><p><em>许多stateless微服务是热点</em></p>
<center>
<p><img src='/imgs/alibaba-traces-socc-2021/degree.png' width='500'/></p>
</center></li>
</ol>
<blockquote>
<p><strong>存在高入度微服务</strong>：有5%的stateless微服务入度&gt;16，这些微服务在90%的调用图存在，处理了95%的请求。这些服务很大概率是瓶颈，可以用来指导弹性伸缩。</p>
</blockquote>
<ol start="4" type="1">
<li><em>微服务调用图大多是动态的</em></li>
</ol>
<center>
<img src='/imgs/alibaba-traces-socc-2021/clusters.png' width='500'/>
</center>
<blockquote>
<p>这个动态和其他文章提到的动态不一样，文中的动态性指的是请求同一个服务的接口，如果参数不一样，会产生不同的拓扑链路（Figure
6）；其他文章提到的是微服务系统始终在动态变化。</p>
</blockquote>
<h3 id="微服务调用关系特征">微服务调用关系特征</h3>
<ol type="1">
<li><p><em>不同层之间调用类型差别大</em></p>
<center>
<p><img src='/imgs/alibaba-traces-socc-2021/dist-invo-type.png' width='500'/></p>
</center></li>
</ol>
<p>首先考虑微服务是否DM，大致分为以下几类：①
<code>black holes</code>（没有DM），②<code>relay</code>（必须有DM），③<code>normal</code>（一定概率有DM）</p>
<p>IP(S2D)，IP(S2M)，IP(S2) 表示IP通信的双方分别为：stateless
微服务与database，stateless
微服务与Memcacheds，stateless微服务与stateless微服务</p>
<blockquote>
<p>深度增加，black
holes比例增加，relay比例减少，normal中对应部分也是如此。</p>
<p>深度增加，IP(S2M)
比例先增后减，IP(S2D)在升高，表明缓存命中率在下降，转而去查询数据库。MQ比例增加，说明业务链路较深时（业务复杂），倾向于使用MQ来减少RT</p>
</blockquote>
<h2 id="微服务之间的依赖">微服务之间的依赖</h2>
<p>这一章节对如何设计和优化微服务架构有启发，不是我研究的范畴，暂时略过</p>
<h3 id="并行依赖">并行依赖</h3>
<center>
<img src='/imgs/alibaba-traces-socc-2021/parallel.png' width='500'/>
</center>
<blockquote>
<p><strong>并行依赖很少</strong>：数据集中大部分的微服务都很少被并行调用，这个并行给我的感觉就是异步调用</p>
</blockquote>
<h2 id="微服务的运行时性能">微服务的运行时性能</h2>
<p>这个章节很重要，对资源管理有很大的指导作用。首先介绍一个定义：MCR代表微服务调用速率，我的理解是服务承受的负载</p>
<h3 id="mcr对资源的影响">MCR对资源的影响</h3>
<ol type="1">
<li><em>MCR与CPU利用率和Young GC强相关，但与Memory利用率相关性弱</em>
<center>
<img src='/imgs/alibaba-traces-socc-2021/resource-mcr.png' width='500'/>
</center></li>
</ol>
<blockquote>
<p><strong>与CPU利用率，Young GC强相关</strong>：Young
GC指的是对JVM堆内存中的新生代区域进行垃圾回收<a
href="#YoungGC"><sup>4</sup></a>，Young
GC频繁会造成性能下降或者应用stop，可能是因为内存泄漏等原因。
<strong>与内存，Old GC相关性弱</strong>：alibaba
trace中容器的内存一般都很稳定，Old
GC频率可能也是如此（老年代本身垃圾回收就不频繁），所以在实验中不是很明显（受限于数据集特征）。</p>
</blockquote>
<ol start="2" type="1">
<li><em>资源对响应时间的影响</em>
<div>
<p><img src='/imgs/alibaba-traces-socc-2021/resource-RT.png' width='300'/>
<img src='/imgs/alibaba-traces-socc-2021/RT-mcr.png' width='300'/></p>
</div></li>
</ol>
<p>图中的延时选的是P75延时 &gt;
<strong>与CPU利用率强相关</strong>：随着CPU利用率升高，RT明显升高，但RT对内存反应不是很明显（可能是因为缺乏高内存数据）
&gt; <strong>与容器的MCR不太相关</strong>：Alibaba
trace中即使MCR很高了，CPU利用率可能还低于10%，所以RT变化不大，<em>说明资源浪费很严重</em></p>
<h2 id="随机图模型">随机图模型</h2>
<center>
<img src='/imgs/alibaba-traces-socc-2021/graph-generator.png' width='300'/>
</center>
<p>这里简单讲一下，代码实现应该不难： 1.
准备一个存储stateless服务的队列<span
class="math inline">\(Q\)</span>，并放入Entering Microservice 2.
执行循环，直到<span class="math inline">\(Q\)</span>为空 1. <span
class="math inline">\(Q\)</span>弹出一个服务作为UM 2.
如果UM的类型是<code>Relay</code>或者<code>normal (Relay)</code>，则根据数据集中DM服务类型的分布，生成对应类型的服务数量
3. 为生成的DMs中不同服务类型确定<code>communication paradigm</code> 4.
将生成的DMs中stateless的微服务放入<span class="math inline">\(Q\)</span>
3. 图优化 1. 遍历生成的图的每一层 1.
随机选择两个父项，如果他们共享相同的标签，则合并他们的两个孩子。 2.
合并的节点将连接到两个父节点</p>
<blockquote>
<p>暂时还没有看到随机模型被其他论文使用，可能是因为大家都可以自己搭建环境生产数据吧，也可能是因为alibaba
trace够用了</p>
</blockquote>
<h2 id="参考文献">参考文献</h2>
<div>
<p><a name="alibabaTrace"></a> [1]
<a>https://github.com/alibaba/clusterdata</a></p>
</div>
<div>
<p><a name="DeathStarBench"></a> [2] Yu Gan. An Open-Source Benchmark
Suite for Microservices and Their Hardware-Software Implications for
Cloud &amp; Edge Systems. ASPLOS, 2019.
<a>https://github.com/delimitrou/DeathStarBench</a></p>
</div>
<div>
<p><a name="OpenTracing"></a> [3] OpenTracing, “Opentracing,”
<a>https://opentracing.io/specification</a></p>
</div>
<div>
<p><a name="YoungGC"></a> [4] java 六 Young GC 和 Full GC
<a>https://www.cnblogs.com/klvchen/articles/11758324.html</a></p>
</div>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>trace</tag>
        <tag>实证分析</tag>
        <tag>SoCC</tag>
        <tag>2021</tag>
      </tags>
  </entry>
  <entry>
    <title>[arxiv 2024] StatuScale: Status-aware and Elastic Scaling Strategy for Microservice Applications</title>
    <url>/2024/12/12/StatusScaler/</url>
    <content><![CDATA[<blockquote>
<p>题目：StatuScale: Status-aware and Elastic Scaling Strategy for
Microservice Applications</p>
<p>来源：arxiv 2024</p>
<p>作者：中国科学院深圳先进技术研究院</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>相比于单体架构，微服务架构具有更好的弹性，可以进行微服务级别的弹性伸缩。<u>然而，现有的弹性伸缩无法检测<strong>突发流量</strong></u>，突发流量可能由很多原因引起：</p>
<ul>
<li>商店促销</li>
<li>特殊活动</li>
<li>软件故障</li>
<li>...</li>
</ul>
<p>这些突发流量通常是短暂（short-lived）且超出预期的（unexpected），会造成瞬间的性能降级。微服务必须要快速分配足够数量的资源以保证性能。</p>
<p>本文推出了一种状态感知的弹性伸缩控制器
<code>StatusScale</code>，能够感知负载的趋势，预测流量峰值，并进行<strong>水平伸缩</strong>和<strong>垂直伸缩</strong>。此外，本文提出了一种新的指标：</p>
<ul>
<li><code>correlation factor</code>：评估资源使用的效率</li>
</ul>
<p>文章在 Sock-Shop 和 Hotel-Reservation
应用上进行评估，将响应延时降低了接近10%，资源利用效率得到提高。</p>
<h2 id="方法">方法</h2>
<p><code>StatuScale</code>
根据当前负载状态选择不同的资源分配方案（垂直和水平），架构如下：</p>
<center>
<img src="/imgs/statuscale/structure.png" width='500'/>
</center>
<ol type="1">
<li><code>Load Preprocessor</code>是产生流量的模块</li>
<li><code>Auto Scaler</code>是全文核心，分为：
<ol type="1">
<li><strong>Vertical
Scaling</strong>：对负载状态进行预测，制定垂直伸缩方案</li>
<li><strong>Horizontal
Scaling</strong>：负载状态不稳定时，进行水平伸缩</li>
</ol></li>
<li><code>Performance Evaluator</code>是评估模块，主要评估响应延时、SLO违背率、资源利用效率、资源消耗等</li>
</ol>
<h3 id="系统建模">系统建模</h3>
<p>弹性伸缩的优化目标和约束如下：</p>
<p><span class="math display">\[
    \mathop{\sum_{m\in M}{P_m/A_M \times \sum_{p\in
P}{R_p/A_p}+\omega^t\sum_{m\in M}RT_m/A_M}}
\]</span></p>
<p><span class="math display">\[
    s.t. \quad P_m \geq 1, \quad R_p, RT_m \geq 0,\quad A_p\geq A_M\geq
0
\]</span></p>
<p><span class="math inline">\(M\)</span> 代表微服务集合，<span
class="math inline">\(A_M\)</span> 代表应用中微服务的数量（即 <span
class="math inline">\(|M|\)</span>），<span
class="math inline">\(P\)</span> 代表应用中pod集合，<span
class="math inline">\(A_p\)</span> 代表应用中pod的数量（即 <span
class="math inline">\(|P|\)</span>），<span
class="math inline">\(P_m\)</span> 代表微服务 <span
class="math inline">\(m\)</span> 的pod数，<span
class="math inline">\(R_p\)</span> 代表为pod <span
class="math inline">\(P\)</span> 垂直分配的资源配额，<span
class="math inline">\(RT_m\)</span> 代表微服务 <span
class="math inline">\(m\)</span> 的响应延时。</p>
<p>优化目标的前半部分是资源消耗（服务的平均pod数<span
class="math inline">\(\times\)</span>pod平均资源额度），后半部分是平均延时，权重设置为
<span
class="math inline">\(\omega^t\)</span>。目标是同时优化资源消耗和响应延时。</p>
<h3 id="垂直伸缩">垂直伸缩</h3>
<p>微服务的负载可能由于特殊的用户活动（e.g.,
商品促销）而陡然升高，进而导致服务资源不足而性能下降。所以监控、分析和理解负载的趋势是非常重要的。</p>
<h4 id="基于-lightgbm-的负载预测器">基于 LightGBM 的负载预测器</h4>
<p>为了能高效地进行负载预测，作者没有采用较重的深度学习或者机器学习模型，而是准备选用基于集成模型的
LightGBM 来进行预测。</p>
<p>遗憾的是，LightGBM 面临准确性问题，在Fig. 2中，作者试图用 LightGBM
预测 Alibaba
数据集的负载，但是出现了大量负载低估的情况。这些低估会导致资源分配较少，进而导致性能下降。</p>
<center>
<img src="/imgs/statuscale/alibaba-forecast.png" width='500'/>
</center>
<h4 id="load-status-detector">Load Status Detector</h4>
<p>文章另辟蹊径，不再直接训练和预测负载的准确值，而是判断负载是否处于
<strong>“stable”</strong> 状态。文章引入了金融分析中常用的
<code>resistance line</code> 和 <code>support line</code>
两个概念用来辅助判断。</p>
<p>在 Fig. 3 (a)中，作者展示了如何根据 <code>resistance line</code> 和
<code>support line</code>
进行负载状态感知。图中橙色的点是预测时的低估点。首先将x轴分为6个时间窗口，每个时间窗口有5个数据点。</p>
<ol type="1">
<li>首先，<code>StatuScale</code>使用第1个窗口的数据去生成
<code>resistance line</code> 和 <code>support line</code></li>
<li>然后，去判断第2个窗口是否违背了第1个窗口的<code>resistance line</code>
和 <code>support line</code>。如果违背了，则判断状态为
<strong>“unstable”</strong>，采取特定的伸缩策略；如果没有违背，则判断状态为
<strong>“stable”</strong>，则能够继续进行负载预测。Fig. 3 (a) 中状态为
<strong>“stable”</strong></li>
<li>合并第1个和第2个窗口的数据来更新<code>resistance line</code> 和
<code>support line</code>，进而判断第3个窗口超过了<code>resistance line</code>，所以标记第3个窗口状态为
<strong>“unstable”</strong></li>
<li>第4个窗口重新生成<code>resistance line</code> 和
<code>support line</code>，重复上述过程</li>
</ol>
总的来说，<code>resistance line</code> 和 <code>support line</code>
相当于负载波动的上下边界，在边界内的负载一般都是
<strong>“stable”</strong> 的。
<center>
<img src="/imgs/statuscale/resistance-line.png"/>
</center>
<p>对于<code>resistance line</code> 和
<code>support line</code>的建模，作者摒弃了复杂的非线性函数，因为担心会导致过拟合。此外，作者又担心线性函数过于简单，所以决定采用分段线性函数（因为分段线性函数可以解决周期负载的判断问题），所以就有了
<strong>“unstable”</strong> 后进行<code>resistance line</code> 和
<code>support line</code>重置的环节。<code>resistance line</code>的定义如下：</p>
<p><span class="math display">\[
    f(t) = kt+b+\lambda c_v
\]</span></p>
<p><span class="math inline">\(k\)</span>, <span
class="math inline">\(b\)</span>
分别代表斜率和截距，可以通过多项式拟合数据点得到。<span
class="math inline">\(t\)</span> 代表时间。<span
class="math inline">\(c_v\)</span> 代表变异系数（<span
class="math inline">\(\frac{\mu}{\sigma}\)</span>），用来表示样本的分散程度，也为<code>resistance line</code>留有了一定的容错空间。<span
class="math inline">\(\lambda\)</span> 是权重超参数。</p>
<h4 id="自适应pid控制器">自适应PID控制器</h4>
<p>在上节中，<code>StatuScale</code>
已经能判断出服务的负载状态，当负载状态为 <strong>“unstable”</strong>
时，采用 PID
控制器来维持状态的稳定，目标是使得<font color='blue'>CPU利用率稳定，以及满足SLO。</font></p>
<p>PID 控制器是广泛使用的控制器，由 1）比例 proportional、 2）积分
integral、 3）导数 derivative 组成。PID
控制器旨在根据feedback更新参数，调整输出，使得状态稳定在目标值附加，输出的分数公式如下：</p>
<p><span class="math display">\[
    y(t) = k_Pe(t)+k_I\int_{t-w}^{t}{e(\tau)d\tau+k_D\frac{d}{dt}e(t)}
\]</span></p>
<p>其中，<span
class="math inline">\(e(t)\)</span>代表时刻t的误差（给定值-测量值）。<span
class="math inline">\(k_P\)</span>，<span
class="math inline">\(k_I\)</span>，<span
class="math inline">\(k_D\)</span>
分别是比例增益（proportional）、积分系数（integral）以及导数系数（derivative），分别代表当前误差，过去一段时间的误差以及预测未来的误差的权重。</p>
<center>
<img src="/imgs/statuscale/PID.png" width='500'/>
</center>
<p>PID 控制器的各项权重 <span class="math inline">\(k_P\)</span>，<span
class="math inline">\(k_I\)</span>，<span
class="math inline">\(k_D\)</span>
对系统的稳定性影响很大，<code>StatuScale</code>
引入了一种自适应调节各项权重的 A-PID 控制器。如 Fig.6 所示，A-PID 引入了
BP 网络来调节 PID 的参数（i.e., <span
class="math inline">\(k_P\)</span>，<span
class="math inline">\(k_I\)</span>，<span
class="math inline">\(k_D\)</span>）。BP 网络的配置如下：</p>
<ul>
<li>输入：<strong>输出值</strong>、<strong>目标值</strong>、<strong>误差</strong>、<strong>bias</strong></li>
<li>中间层：hidden size = 5，激活函数为 tanh</li>
<li>输出：<span class="math inline">\(k_P\)</span>，<span
class="math inline">\(k_I\)</span>，<span
class="math inline">\(k_D\)</span>，激活函数为 sigmoid</li>
</ul>
<blockquote>
<p>这里文章漏掉了最关键的 loss，我只能猜测 loss
的原理是，<u>如果当前误差较大，我们希望新的参数能够更大幅度地改变，以便更快地纠正误差；反之，如果误差较小，则应谨慎调整参数，避免过度校正</u>，所以猜测
loss 为 误差相关的函数。</p>
</blockquote>
<blockquote>
<p>此外，A-PID的 output 是如何转化为 CPU 的分配？CPU
target是多少？垂直伸缩这一块并不是讲的很清楚</p>
</blockquote>
<h3 id="水平伸缩">水平伸缩</h3>
<p>文章认为水平伸缩比垂直伸缩更难，因为水平伸缩需要时间去创建和移除POD，并需要时间去进行负载均衡，同时不必要的水平伸缩操作会导致资源浪费。此处引用了ATOM<a
href="#ATOM"><sup>1</sup></a>的结论：</p>
<ul>
<li>低负载：垂直伸缩更有优势，因为资源分配快</li>
<li>高负载：水平伸缩更有优势，因为多个pod分布在多个机器上，将负载均衡了，大大降低了单个pod的压力</li>
</ul>
<p>所以 <code>StatuScale</code>
会优先考虑垂直伸缩（在低负载下更有优势）；如果垂直伸缩无法满足需求，才会考虑用水平伸缩进行粗粒度调整。再用垂直伸缩进行细粒度调整。</p>
<p>首先，<code>StatuScale</code>
将会判断是否需要进行水平伸缩操作，计算当前CPU利用率 <span
class="math inline">\(C_t\)</span> 与目标CPU利用率 <span
class="math inline">\(CPU_{tar}\)</span>
之间的差距，并根据这个差距生成一个转换后的结果 <span
class="math inline">\(S_t\)</span>：</p>
<p><span class="math display">\[
    S_t = \begin{cases}
        1-K^{(CPU_{tar}-C_t)}&amp; C_t ＜ CPU_{tar}\\
        K^{C_t-(CPU_{tar})}-1&amp; C_t \geq CPU_{tar}
        \end{cases}
\]</span></p>
<p>当前CPU利用率 <span class="math inline">\(C_t\)</span> 接近目标值
<span class="math inline">\(CPU_{tar}\)</span>，<span
class="math inline">\(|S_t|\)</span>接近0；否则，<span
class="math inline">\(|S_t|\)</span>值将以指数倍数增长。<code>StatuScale</code>
统计一段滑动窗口内的不同时间点的 <span
class="math inline">\(S_t\)</span>
的和（减少突发流量的影响），并与上下阈值进行比较，以决定是否进行弹性伸缩。</p>
<blockquote>
<p>但是文章并没有给出上下阈值的计算方式？</p>
</blockquote>
<p>当决定采用弹性伸缩时，给定当前副本数（<span
class="math inline">\(R_c\)</span>），伸缩比例（<span
class="math inline">\(\delta\)</span>），伸缩的副本数定义如下：</p>
<p><span class="math display">\[
    R_n = max(\delta R_c, 1)
\]</span></p>
<p>因为水平伸缩的pod需要一段时间才能生效，所以这段时间可能会频繁触发弹性伸缩，所以
<code>StatuScale</code> 引入了 <strong>cooling-off</strong>
周期来减少伸缩次数（这段时间不会触发第二次伸缩，默认为5min）</p>
<p>接下来文章用垂直伸缩来进行细粒度资源调整，具体来说，就是通过一个衰减率来周期地减少资源配额，资源值设置如下：</p>
<p><span class="math display">\[
    V(t) = Vk^{\beta^t-1}
\]</span></p>
<p><span class="math inline">\(0&lt;\beta&lt;1\)</span> 是衰减率，<span
class="math inline">\(V\)</span> 是资源初始值。 &gt;
这里只有减少垂直资源分配，相当于减少水平伸缩多余的那部分资源</p>
<h3 id="联合伸缩">联合伸缩</h3>
<center>
<img src="/imgs/statuscale/flow.png" width='500'/>
</center>
<p>文章的讲述顺序和方法流程是不一样的，所以最开始让我有点费解，真正的整体流程如上图所示：</p>
<ol type="1">
<li>首先判断是否需要水平伸缩，判断方式为上文中提到的计算一段时间的 <span
class="math inline">\(S_t\)</span>，并与上下阈值比较
<ol type="1">
<li>如果需要水平伸缩，则计算<span
class="math inline">\(R_n\)</span>，然后垂直细粒度资源调整（计算<span
class="math inline">\(V(t)\)</span>）</li>
<li>如果不需要，则需要进行垂直伸缩判断</li>
</ol></li>
<li>垂直伸缩检测需要对负载状态进行判断
<ol type="1">
<li>如果状态为 <strong>“stable”</strong>，则用 LightGBM 预测负载</li>
<li>如果状态为 <strong>unstable</strong>，则使用 A-PID
控制器将资源利用率维持在稳定状态</li>
</ol></li>
</ol>
<blockquote>
<p>值得注意的是，k8s的垂直伸缩应该会让容器重启（假如有10个副本，采用垂直伸缩后，相当于这10个副本都需要滚动更新），这真的会比水平伸缩快吗？</p>
</blockquote>
<p>整体算法如下图所示：</p>
<center>
<img src="/imgs/statuscale/algo.png" width='500'/>
</center>
<h2 id="实验评估">实验评估</h2>
<h3 id="实验配置">实验配置</h3>
<ul>
<li><strong>集群配置</strong>：1 master + 2 worker，每个节点都是 4GB
内存 和 4 CPU cores。这个配置算比较小的了</li>
<li><strong>负载</strong>：文章的负载数据来自于 alibaba 的
cluster-trace-v2018<a
href="#statuscale-alibaba-trace"><sup>2</sup></a>，这个数据集里记录了8天内集群里机器和容器的资源使用情况，并调研了CPU负载和QPS的对应关系，如Fig.8所示，将CPU负载转化为QPS，文章使用这个作业负载作为实验的输入流量。负载的注入工具选择
Locust</li>
<li><strong>benchmark</strong>：选用 Sock-Shop 和 Hotel-Reservation</li>
<li><strong>对比方法</strong>：选用了 GBMScaler，Showar，Hyscale
<ul>
<li><code>GBMScaler</code>：选用 LightGBM
进行负载预测，但原文并没有提到如何用预测的结果进行 resource scaling</li>
<li><code>Showar</code>：经典的混合伸缩方法，基于 3-<span
class="math inline">\(\sigma\)</span>
准则进行垂直伸缩，每T秒预估当前CPU分配为过去一段窗口的<span
class="math inline">\(\mu+3\sigma\)</span>；基于 PID
控制器进行水平伸缩（target设置为CPU利用率，<span
class="math inline">\(k_P\)</span>，<span
class="math inline">\(k_I\)</span>，<span
class="math inline">\(k_D\)</span>的更新与
<code>StatusScale</code>一致）</li>
<li><code>Hyscale</code>：与kubernetes
默认弹性伸缩器很像，只需要指定CPU阈值，然后通过水平和垂直伸缩来达到目标</li>
</ul></li>
</ul>
<center>
<img src="/imgs/statuscale/CPU-qps.png"/>
</center>
<h3 id="评估指标">评估指标</h3>
<p>文章主要考虑<strong>系统性能</strong>和<strong>资源消耗</strong>，使用的指标如下：</p>
<ol type="1">
<li>response time （相同资源配额下，<span
class="math inline">\(\int{R_t}dt\)</span>，<span
class="math inline">\(R_t\)</span>是t时刻分配的资源）</li>
<li>SLO violation（相同资源配额下，<span
class="math inline">\(\int{R_t}dt\)</span>）</li>
<li>Accuracy of supply-demand relationships. 这个是看 resource supply
是否准确。在一段时间 <span
class="math inline">\(T\)</span>内，总共可分配资源为 <span
class="math inline">\(R\)</span>，t 时刻的资源需求为 <span
class="math inline">\(d_t\)</span> （<span
class="math inline">\(d_t\)</span> 是通过 Fig.8
拟合出来的），有点类似于误差
<ol type="1">
<li><span class="math inline">\(a_U=\frac{1}{T\cdot
R}\sum_{t=1}^{T}{(d_t-s_t)^+\Delta t}\)</span></li>
<li><span class="math inline">\(a_O=\frac{1}{T\cdot
R}\sum_{t=1}^{T}{(s_t-d_t)^+\Delta t}\)</span></li>
</ol></li>
<li>Correlation factor of supply-demand relationships. 这个指标用于衡量
supply curve 与 demand curve
的相似度（与上一个指标差别不大），本来应该用 <strong>Locust 收集的 QPS
转化为 demand 的 CPU 利用率</strong>，以及用 <strong>Prometheus 收集的
supply 的 CPU 利用率</strong>，然后计算两个曲线的 <code>R-square</code>
（评估回归模型的性能指标）。但文章认为 Locust 与 Prometheus
是两套监控系统，收集周期和统计方式有所区别，所以改用
<code>Dynamic Time Warping</code> 算法来衡量两个时间序列的相似度:
<ul>
<li>首先，将两个curve进行量纲对齐。比如将第一个 curve （<span
class="math inline">\(X=\{x_1,x_2,\dots,x_m\}\)</span>）转到第二个 curve
（<span
class="math inline">\(Y=\{y_1,y_2,\dots,y_m\}\)</span>）的量纲下：<span
class="math inline">\(x^\prime_i=(x_i-\mu_x)\times\frac{\sigma_Y}{\sigma_X}+\mu_Y\)</span>，<span
class="math inline">\(x^\prime_i\)</span> 是转化后的值</li>
<li>定义距离矩阵 <span class="math inline">\(D\)</span>，<span
class="math inline">\(D_{i,j}\)</span>代表 <span
class="math inline">\(X\)</span> 的时间点 <span
class="math inline">\(i\)</span> 和 <span
class="math inline">\(Y\)</span> 的时间点 <span
class="math inline">\(j\)</span>的距离，<span
class="math inline">\(d(x_i,y_j)\)</span> 代表 <span
class="math inline">\(x_i\)</span> 和 <span
class="math inline">\(y_j\)</span> 的欧氏距离（也可以用其他距离），<span
class="math inline">\(D_{i,j}\)</span>计算方式如下：</li>
<li><span class="math display">\[
  D_{i,j}=min\begin{cases}
  D_{i-1,j}+d(x_i,y_j)\\
  D_{i,j-1}+d(x_i,y_j)\\
  D_{i-1,j-1}+d(x_i,y_j)
  \end{cases}
  \]</span></li>
<li><span class="math inline">\(D_{m-1,n-1}\)</span> 代表 curve <span
class="math inline">\(X\)</span> 与 curve <span
class="math inline">\(Y\)</span> 的最小距离，则
<code>correlation factor</code>计算如下：<span
class="math inline">\(CF=max(m,n)/D_{m-1,n-1}\)</span></li>
</ul></li>
</ol>
<h3 id="总实验">总实验</h3>
<p><code>StatuScale</code>
的目标有三个：①降低响应延时、②降低SLO违背率、③维持CPU利用率在目标水平（<span
class="math inline">\(\pm1\%\)</span>）。</p>
<center>
<img src="/imgs/statuscale/sockshop-performance.png"/>
</center>
<ul>
<li>Fig. 9(a)
展示了不同scaler的平均延时和P95延时的分布（Locust可以求得），可以看出<code>StatuScale</code>的延时分布是比较偏低的，均值维持在50~70ms，P99维持在300多ms左右</li>
<li>Fig. 9(b) 想展示的与Fig.
9(a)差不多，展示的是四个scaler的延时的累积分布直方图（CDF），P95基本维持在250ms左右，说明几个scaler都很有作用（<strong>私以为应该加上一个没有设置采样器的方法作为对比</strong>）</li>
<li>Fig. 9(c) 计算了不同SLO阈值下的违背情况</li>
<li>Fig. 9(d) 计算了
<code>correlation factor</code>，说明<code>StatuScale</code>的资源分配的曲线与负载波动（资源需求曲线）很相似。Fig.10
展示了CPU使用（这里难道不应该是分配的CPU吗？）和负载的相似度。表格是对图像结果的数据展示</li>
</ul>
<p><code>StatuScale</code>也在Hotel-Reservation上进行了实验，结果比较相似，就不贴上来了。</p>
<center>
<img src="/imgs/statuscale/sockshop-performance2.png"/>
</center>
<h3 id="消融实验">消融实验</h3>
<h4 id="消融-status-detector-module">消融 Status Detector Module</h4>
<p><code>Status Detector</code>判断当前负载是否
<strong>“stable”</strong>，如果 <strong>“stable”</strong>，则选择用
LightGBM预测负载，然后转换成CPU需求；如果
<strong>“unstable”</strong>，则用 A-PID
将CPU利用率控制在某个阈值。文章选择了3个变体，衡量它们的延时（为什么还要消融A-PID和LightGBM？为什么不衡量其他指标？）。实验在
Sock-Shop 上做，每组实验做3次</p>
<ul>
<li><code>StatusScale</code><span
class="math inline">\(^\Delta\)</span>：消融 horizontal scaler</li>
<li><code>StatusScale</code><span
class="math inline">\(^\circ\)</span>：消融 horizontal scaler，load
status detector 和 A-PID 控制器</li>
<li><code>StatusScale</code><span
class="math inline">\(^*\)</span>：消融 horizontal scaler，load status
detector 和 load prediction（LightGBM）</li>
</ul>
<center>
<img src="/imgs/statuscale/ablate-sd.png" width='700'/>
</center>
<p>上述实验说明了 load status detector 对 load
prediction的影响很大（<code>StatusScale</code><span
class="math inline">\(^\circ\)</span>）</p>
<h4 id="消融-scaling-modes">消融 Scaling Modes</h4>
<p>vertical scaling
虽然能细粒度调节资源，但依然受限于单个机器硬件；horizontal scaling
又容易造成资源浪费。文章设计了2个变体，实验在 Sock-Shop
上做，每组实验做3次，但是加入了CPU使用率的对比：</p>
<ul>
<li><code>StatuScale</code><span
class="math inline">\(^\square\)</span>：只使用 vertical scaling</li>
<li><code>StatuScale</code><span
class="math inline">\(^*\)</span>：只使用 horizontal scaling</li>
</ul>
<center>
<img src="/imgs/statuscale/ablate-sm.png" width='700'/>
</center>
<p>可以看出，horizontal scaling （<code>StatuScale</code><span
class="math inline">\(^*\)</span>）确实能最大限度降低延时，但是CPU资源利用率偏低；vertical
scaling（<code>StatuScale</code><span
class="math inline">\(^\square\)</span>）很难保证延时，但是CPU利用率高；<code>StatusScale</code>相当于在两者间做了均衡。</p>
<div>
<p><a name="ATOM"></a> [1] Alim Ul Gias, et.al. 2019. ATOM: Model-Driven
Autoscaling for Microservices. In 2019 IEEE 39th ICDCS. 1994–2004.
https://doi.org/10.1109/ICDCS.2019.00197</p>
</div>
<div>
<p><a name="statuscale-alibaba-trace"></a> [2]
https://github.com/alibaba/clusterdata/blob/master/cluster-trace-v2018/trace_2018.md</p>
</div>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>资源管理</tag>
        <tag>弹性伸缩</tag>
        <tag>arxiv</tag>
        <tag>2024</tag>
      </tags>
  </entry>
  <entry>
    <title>前缀树（Prefix Tree）</title>
    <url>/2024/11/25/trie_tree/</url>
    <content><![CDATA[<p>Trie 树，又叫前缀树，字典树，
是一种有序的树形数据结构，用于高效地存储和检索字符串数据集中的键。下图是维基百科上关于trie树的一个典型例子，我们可以很清晰地看到，这棵树存储了许多前缀相似的字符串，给定一个字符串，我们可以很容易知道这个字符串是否被存储，而不需要遍历比较。</p>
<center>
<p><img src="/imgs/trie/trie.png"/></p>
</center>
<p>这一数据结构有相当多的应用情景，例如：</p>
<ul>
<li>自动补全：
<ul>
<li>搜索提示：输入网址，跳出可能的选择</li>
<li>输入提示：根据已经输入的字符预测可能的词组和句子</li>
</ul></li>
<li>拼写检查：存储合法的单词列表，快速查找是否存在合法的单词</li>
<li>前缀匹配</li>
<li>IP路由查找</li>
</ul>
<h2 id="题目">题目</h2>
<p>leetcode 208 实现Trie（前缀树）</p>
<blockquote>
<p>请你实现 Trie 类：</p>
<ul>
<li><code>Trie()</code> 初始化前缀树对象。</li>
<li><code>void insert(String word)</code> 向前缀树中插入字符串
<code>word</code> 。</li>
<li><code>boolean search(String word)</code> 如果字符串
<code>word</code> 在前缀树中，返回
<code>true</code>（即，在检索之前已经插入）；否则，返回
<code>false</code> 。</li>
<li><code>boolean startsWith(String prefix)</code>
如果之前已经插入的字符串 <code>word</code> 的前缀之一为
<code>prefix</code> ，返回 <code>true</code> ；否则，返回
<code>false</code> 。</li>
</ul>
</blockquote>
<p><strong>示例：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入</span><br><span class="line">[&quot;Trie&quot;, &quot;insert&quot;, &quot;search&quot;, &quot;search&quot;, &quot;startsWith&quot;, &quot;insert&quot;, &quot;search&quot;]</span><br><span class="line">[[], [&quot;apple&quot;], [&quot;apple&quot;], [&quot;app&quot;], [&quot;app&quot;], [&quot;app&quot;], [&quot;app&quot;]]</span><br><span class="line">输出</span><br><span class="line">[null, null, true, false, true, null, true]</span><br><span class="line"></span><br><span class="line">解释</span><br><span class="line">Trie trie &#x3D; new Trie();</span><br><span class="line">trie.insert(&quot;apple&quot;);</span><br><span class="line">trie.search(&quot;apple&quot;);   &#x2F;&#x2F; 返回 True</span><br><span class="line">trie.search(&quot;app&quot;);     &#x2F;&#x2F; 返回 False</span><br><span class="line">trie.startsWith(&quot;app&quot;); &#x2F;&#x2F; 返回 True</span><br><span class="line">trie.insert(&quot;app&quot;);</span><br><span class="line">trie.search(&quot;app&quot;);     &#x2F;&#x2F; 返回 True</span><br></pre></td></tr></table></figure>
<p><strong>提示：</strong></p>
<ul>
<li><code>1 &lt;= word.length, prefix.length &lt;= 2000</code></li>
<li><code>word</code> 和 <code>prefix</code> 仅由小写英文字母组成</li>
<li><code>insert</code>、<code>search</code> 和 <code>startsWith</code>
调用次数 <strong>总计</strong> 不超过 <code>3 * 104</code> 次</li>
</ul>
<h2 id="分析">分析</h2>
<p>这道题有几个地方需要注意：</p>
<ul>
<li><code>insert</code>时，需要标记单词是否截止，因为trie中的节点既有可能是前缀，也有可能是单词</li>
<li><code>search</code>与 <code>startswith</code>的区别在于，
<code>startswith</code>只需要搜索下去，看看有没有对应的节点；而<code>search</code>还需要判断这个节点是否有截止信号</li>
</ul>
<h2 id="实现">实现</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trie</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.root = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type word: str</span></span><br><span class="line"><span class="string">        :rtype: None</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cur_node = self.root</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> c <span class="keyword">in</span> cur_node.keys():</span><br><span class="line">                cur_node = cur_node[c]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur_node[c]=&#123;&#125;</span><br><span class="line">                cur_node = cur_node[c]</span><br><span class="line">                cur_node[<span class="number">0</span>]=<span class="number">0</span> <span class="comment"># 标记是否截止</span></span><br><span class="line">        <span class="comment"># 标记这个cur_node，标注上截止信号，代表这是一个词</span></span><br><span class="line">        cur_node[<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type word: str</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cur_node = self.root</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> c <span class="keyword">in</span> cur_node.keys():</span><br><span class="line">                cur_node = cur_node[c]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 判断有没有截止信号</span></span><br><span class="line">        <span class="keyword">if</span> cur_node[<span class="number">0</span>]==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startsWith</span><span class="params">(self, prefix)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type prefix: str</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cur_node = self.root</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> prefix:</span><br><span class="line">            <span class="keyword">if</span> c <span class="keyword">in</span> cur_node.keys():</span><br><span class="line">                cur_node = cur_node[c]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">trie = Trie()</span><br><span class="line">trie.insert(<span class="string">"app"</span>)</span><br><span class="line">trie.insert(<span class="string">"apple"</span>)</span><br><span class="line">trie.insert(<span class="string">"beer"</span>)</span><br><span class="line">trie.insert(<span class="string">"add"</span>)</span><br><span class="line">trie.insert(<span class="string">"jam"</span>)</span><br><span class="line">trie.insert(<span class="string">"rental"</span>)</span><br><span class="line">trie.insert(<span class="string">"rental"</span>)</span><br><span class="line">print(trie.search(<span class="string">"apps"</span>))</span><br><span class="line">print(trie.startsWith(<span class="string">"app"</span>))</span><br><span class="line">print(trie.search(<span class="string">"app"</span>))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>leetcode基础算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>leetcode</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>[FSE 2025] L4: Diagnosing Large-scale LLM Training Failures via Automated Log Analysis</title>
    <url>/2025/04/28/l4/</url>
    <content><![CDATA[<blockquote>
<p>题目：L4: Diagnosing Large-scale LLM Training Failures via Automated
Log Analysis</p>
<p>FSE 2025</p>
<p>作者：香港中文大学</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>训练个性化的大语言模型（LLM）需要大量的计算资源和训练时间。这个过程中，故障（failure）是不可避免的，而故障的出现使得LLM的训练浪费了大量的资源和时间。此外，在
LLM 训练中诊断故障也是一个费时费力的任务，因为 LLM
的训练通常设计多个计算节点：</p>
<ul>
<li><strong>Node-level Complexity</strong>: 在单个节点上训练的 AI
模型，通常包含 AI accelerator（GPUs 或 NPUs）、AI toolkit（CUDA）、AI
framework（Pytorch）以及AI
algorithm。故障可能存在于上述任意一个地方。</li>
<li><strong>CLuster-level Complexity</strong>: LLM 的训练通常涉及上千个
AI
节点，这些节点之间有复杂的通信范式，这使得发生故障时很难通过自动化的方式定位到对应的故障
AI 节点</li>
</ul>
<p>这篇文章首先进行了大量的实证分析，得出以下结论：</p>
<ol type="1">
<li><strong>故障时间</strong>：大多数（74.1%）的故障发生在模型迭代训练时，这个阶段发生故障会导致大量的训练时间和资源的浪费</li>
<li><strong>故障根因</strong>：随然故障原因多样化，但主要集中在
<strong>hardware</strong> 和 <strong>user-side faults</strong>。</li>
<li><strong>诊断方法</strong>：日志在故障诊断中发挥重要作用，但是89.9%的案例仍然需要人工日志分析来进行故障诊断。并且日志量太大（每天产生
TBs），只有非常小一部分的日志是有用的</li>
</ol>
<p>因此，文章提出了一种诊断 LLM
训练的故障诊断方法：L4，目的在于自动化识别故障相关的日志（failure-indicating
logs），此外，L4 还会提供 failure-indicating nodes、failure-indicating
stages、failure-indicating events 以及 failure-indicating iterations
等重要信息来辅助 SREs 进行故障诊断。</p>
<center>
<img src="/imgs/l4/background.png" width='500'/>
</center>
<h2 id="实证分析">实证分析</h2>
<p>文章分析了某平台上一年的428份故障报告，从以下三个方面进行分析：</p>
<ol type="1">
<li>RQ1：LLM 训练中的故障现象</li>
<li>RQ2：LLM 训练中的故障根因</li>
<li>RQ3：LLM 训练故障诊断中常用的数据源</li>
</ol>
<h3 id="rq1故障现象">RQ1：故障现象</h3>
<p>LLM 训练中的故障现象分为四大类：① <code>launching failure</code>，②
<code>training crash</code>，③ <code>abnormal behavior</code>，④
<code>others</code></p>
以下是四种故障现象的分布：
<center>
<img src="/imgs/l4/symptom.png" width='500'/>
</center>
<blockquote>
<ol type="1">
<li>对于
<code>launching failure</code>（21.3%），这类现象通常发生在迭代训练开始前，原因一般是配置与版本问题，比如
GPU 驱动与 CUDA 版本不匹配，模型并行化配置错误等。</li>
<li>对于
<code>training crash</code>（57.5%），这类现象发生在迭代训练时，起因一般是硬件故障（GPU、network），这种故障影响很大，会浪费大量训练时间和计算资源，即使有
checkpoint 这样的状态保存机制，时间的浪费也是不可忽视的</li>
<li>对于
<code>abnormal behaviors</code>（16.6%），这种现象有点类似于性能降级，比如某个epoch花费了两倍的时间，训练突然停滞</li>
<li>对于
<code>others</code>（4.7%），这类故障一般是基础设施类的，比如平台和存储，比例比较少</li>
</ol>
</blockquote>
<p><font color='blue'><strong>Finding
1：大部分故障（74.1%）发生在迭代训练时，会导致大量计算资源和训练时间的浪费。</strong></font></p>
<h3 id="rq2故障根因">RQ2：故障根因</h3>
<p>LLM 训练中的故障根因分为四大类：① <code>hardware fault</code>，②
<code>user fault</code>，③ <code>platform fault</code>，④
<code>framework fault</code></p>
<h4 id="rq2-1-hardware-fault">RQ2-1 hardware fault</h4>
<p><code>Hardware Fault</code> 又可细分为</p>
<ul>
<li><strong>Network Fault</strong>：最常见，本质上是因为有太多 AI node
在交互协同，网络问题极容易导致训练的失败</li>
<li><strong>Accelerator Fault</strong>：Accelerator
就是GPU、TPU那些计算资源，与普通深度学习任务类似，单个 Accelerator
也会有内存故障等</li>
<li><strong>Node Fault</strong>：Node
是资源分配的单元，比如虚拟机。也会遭遇断电、磁盘问题等故障</li>
<li><strong>Storage
Fault</strong>：训练涉及的数据集、模型、checkpoints等有几百GB，用户一般会存储到远程存储库中，在训练时下载，因此可能会出现访问问题</li>
</ul>
<p><font color='blue'><strong>Finding 2：LLM训练需要大量计算资源，极易受
Hardware Fault 影响。其中，Network Fault 和 Accelerator Fault
是最常见的</strong></font></p>
<h4 id="rq2-2-user-fault">RQ2-2 user fault</h4>
<p><code>user fault</code> 又可细分为</p>
<ul>
<li><strong>Configuration Error</strong></li>
<li><strong>Program/Script Bug</strong></li>
<li><strong>Software Incompatibility</strong></li>
<li><strong>Misoperation</strong></li>
</ul>
<p><font color='blue'><strong>Finding 3：User faults
是第二大故障根因，源于用户的误操作、脚本bug等 </strong></font></p>
<h4 id="rq2-3-framework-fault-and-platform-fault">RQ2-3 Framework Fault
and Platform Fault</h4>
<p>这两类故障发生的极少，<code>Framework Fault</code> 一般是 PyTorch
那些深度学习框架中的故障。<code>Paltform Fault</code>
是训练平台的问题，包括资源管理不恰当等。这两类故障极难诊断故障，需要较深的领域经验</p>
<p><font color='blue'><strong>Finding 4：Framework Fault 和 Paltform
Fault 虽然发生极少，但诊断起来相当困难</strong></font></p>
<h3 id="rq3故障诊断数据源">RQ3：故障诊断数据源</h3>
<p>现在诊断 LLM 训练故障还是人工分析居多，文章中提到：</p>
<blockquote>
<p>诊断 LLM 训练故障平均需要 34.7 小时，而 41.9% 的故障需要 24
小时以上的诊断时间</p>
</blockquote>
<p>这就迫切需要自动化的故障诊断方法来减少人工分析成本。文章首先对428份故障案例进行分类，分为以下三种：</p>
<ul>
<li>Log-only diagnosable</li>
<li>Non-log diagnosable</li>
<li>Hybrid diagnosable</li>
</ul>
<p>然后分别分析每个案例的 training log
来进行故障诊断，以下是分类结果</p>
<center>
<img src="/imgs/l4/log-failure.png" width='500'/>
</center>
<p>同时发现每个故障案例在故障时间段平均有 16.92GB 的 training log</p>
<p><font color='blue'><strong>Finding 5：training log 能解决 89.9%
的故障，但是日志量太大，需要自动化分析手段来减少人力成本</strong></font></p>
<h2 id="现有方法">现有方法</h2>
<p>现有方法大致通过
<code>logging level</code>，<code>event frequency</code>，<code>error semantic</code>
来提取异常log，即
<code>failure-indicating logs</code>。文章通过统计发现这些方式都有一定的局限性：</p>
<center>
<img src="/imgs/l4/failure-log-dist.png" width='500'/>
</center>
<blockquote>
<ul>
<li><code>logging level</code>：现有方法有许多是根据日志级别来进行筛选的。文章首先统计了
<code>failure-indicating logs</code> 中不同级别的日志分布，如 Fig.4 (a)
所示，虽然大部分是 Error-level，但是仍有大量其他级别的 logs
是故障相关的。此外，并不是所有 Error-level 的 logs
是与当前故障相关的，比如有些 Error logs 是无法写入
checkpoints，但一些故障容忍策略可能会采用重写措施，只要重写成功，那么对当前训练是没有影响的</li>
<li><code>Event Frequency</code>：有些方法是根据日志的频率来筛选故障相关日志，比如低频日志更有可能是异常的。如
Fig.4 (b) 所示，仍然有大量 <code>failure-indicating logs</code>
的频率并不低。<strong>此外，基于频率的筛选可能更适合微服务，不适用于 LLM
训练</strong>，因为微服务是无状态的，日志的频率分布可能并不会随时间分布发生较大改变，而
LLM
训练是顺序的、分阶段的，有些日志只会出现在特定阶段，这些日志的频率很低，但与故障无关。</li>
<li><code>Error Semantic</code>：也有些方法通过深度学习来提取日志的错误语义信息来识别<code>failure-indicating logs</code>，但是这种方法是不稳定的，因为即使是成功训练的job也包含有error
semantic</li>
</ul>
</blockquote>
<p><font color='blue'><strong>Finding 6：现有的 failure-indicating logs
提取方法基于level, frequency 和 semantic，但都不适用于 LLM
训练的故障诊断</strong></font></p>
<h2 id="l4-方法">L4 方法</h2>
<p>文章提出了自动化的面向 LLM 训练的故障诊断框架：L4</p>
<center>
<img src="/imgs/l4/structure.png" width='900'/>
</center>
<h3 id="log-preprocessing">1. Log Preprocessing</h3>
<p>这一块与之前的方法相同，都是用 Drain 提取日志的模板，将 log sequence
变成 event sequence</p>
<h3 id="cross-job-filtering">2. Cross-job Filtering</h3>
<p>这一块动机很直观，因为用户在提交作业到平台进行大规模训练（large-scale
nodes）时，通常在小规模的节点上已经测试通过了。所以这一块会将成功执行的日志（Normal
logs）与在大规模节点上的失败日志（failed logs）进行比对，删除 failed
logs 中噪声 events。</p>
<blockquote>
<p>具体做法为：将 Normal logs 进行解析得到一个 normal event pool：<span
class="math inline">\(N=\{e_{n1}, e_{n2}, ...\}\)</span>，然后将这些
events 按照时间顺序排列，并逐个移除 failed logs
中对应的events，这些events都是正常的，failed logs 剩下的 events
都是极大概率是故障相关的。<strong>这里有个问题，不同规模的训练节点会不会导致日志模式发生变化？</strong></p>
</blockquote>
<p>这个模块的前提是有小规模训练成功的日志，当这个前提不满足时，则无法删减
failed logs的噪声events，就直接将 failed logs 解析后进行后续步骤</p>
<h3 id="spatial-pattern-comparison">3. Spatial Pattern Comparison</h3>
<p>这个步骤主要是为了定位 <code>failure-indicating nodes</code> 和
<code>failure-indicating logs</code>。核心思想是：<strong>由于负载均衡，正常情况下所有的
AI nodes 的日志几乎是一样的</strong>。所以可以很轻松找到
<code>failure-indicating nodes</code></p>
<p>首先将每个 node 的 event sequence
按照<strong>发生次数</strong>进行特征构建，得到 event vector：<span
class="math inline">\(V=[c_1,c_2,...c_n]\)</span>，<span
class="math inline">\(c_i\)</span> 代表 <span
class="math inline">\(i\)</span>-th event 的次数。然后对所有 node 的
event vector 采用 <code>Isolation Forest</code> 进行异常检测。</p>
<p>由于 <code>Isolation Forest</code>
可以记录特征参与分割的次数，从而能够判断特征（event）的重要性，所以进一步找到
<code>failure-indicating logs</code></p>
<h3 id="temporal-pattern-comparison">4. Temporal Pattern Comparison</h3>
<p>这个步骤主要是为了定位 <code>failure-indicating stage</code> 和
<code>failure-indicating iteration</code>，即故障发生的时间。</p>
<p>stage 的确定很简单，L4 好像直接使用的是日志中自带的规则，能直观知道
event 在哪个 stage</p>
<p>文章还要定位到故障的 iteration。首先文章有个前提假设，即 event
sequence 在迭代时基本遵循一定的模式，如果某个 iteration
发生了明显偏移，则视为异常</p>
<p>具体做法为：首先将 iterations 按照 10 个一组进行滑动窗口划分。采用
Dynamic time warping（DTW）计算不同窗口间的距离，然后通过 3-sigma
方法对距离进行异常检测，就能找到异常的 iteration 了</p>
<p>最后，就能将故障相关的 logs、events、nodes、stages、iterations 交给
SREs 了。</p>
<h2 id="实验设计">实验设计</h2>
<p>实验主要是对 <code>failure-indicating logs</code> 和
<code>failure-indicating nodes</code>
的定位进行了准确率评估，然后给了几个成功案例</p>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>FSE</tag>
        <tag>大模型</tag>
        <tag>日志</tag>
        <tag>故障诊断</tag>
        <tag>2025</tag>
      </tags>
  </entry>
  <entry>
    <title>[ISSRE 2025] Self-Evolutionary Group-wise Log Parsing Based on Large Language Model</title>
    <url>/2025/04/29/selfLog/</url>
    <content><![CDATA[<blockquote>
<p>题目：Self-Evolutionary Group-wise Log Parsing Based on Large
Language Model</p>
<p>ISSRE 2025</p>
<p>作者：中科大杭州高等研究院，清华大学</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>日志解析是一种将半结构化日志转化为结构化模板的技术，它是各种日志分析任务（比如异常检测、日志理解）的前提。</p>
<p>现有的日志解析方法大多基于领域专家制定的启发式规则，这些规则在系统发生变更时就无法适用了。因此，不少研究采用大模型来进行系统无关的日志解析，但仍然存在两个显著问题：</p>
<ol type="1">
<li>大模型需要在 Prompt 中加入人工标注的日志模板</li>
<li>大模型的日志解析效率太低</li>
</ol>
<p>因此，文章提出一种自演化的日志解析方法 <a
href="https://github.com/CSTCloudOps/SelfLog">SelfLog</a>，将相似的历史解析的模板作为
Prompt
中的提示词，以实现自我演化和零标注。此外，还引入一种基于N-Gram的日志分组器与日志匹配器，按组处理和解析日志，通过减少大模型调用次数来提升效率。</p>
<h2 id="背景">背景</h2>
<h3 id="日志解析">日志解析</h3>
<p>日志解析会将每条日志分割为<strong>常量部分</strong>和<strong>变量部分</strong>。常量部分也称为<strong>模板</strong>。如图4所示，在有源码的前提下，我们能够很轻松区分这两个部分。然而大多数时候大量第三方依赖库的源码是不可见的，</p>
<center>
<img src="/imgs/selfLog/parse.png" width='500'/>
</center>
<p>所以开发了许多数据驱动的日志解析技术，这些技术分为有监督和无监督两类：</p>
<ul>
<li><strong>无监督</strong>：通过启发式规则和频率统计来提取模板。缺陷是制定规则需要领域经验，且对于新log数据集要重新制定规则</li>
<li><strong>有监督</strong>：通过人工标注的 &lt;log, template&gt;
键值对来训练，缺陷是对训练数据的分布敏感且在新log上表现较差</li>
</ul>
<p>由于日志本质上是程序员写的语句，包含了大量语义信息。大模型技术擅长于理解语句，并且有很强的
zero-shot
推理能力，所以现有研究开始尝试用大模型进行日志解析，但仍然存在两个显著问题：</p>
<ol type="1">
<li><strong>大模型需要在 Prompt
中加入人工标注的日志模板案例</strong>。这个案例的质量非常重要，随着系统更新，需要人工重新标注案例</li>
<li><strong>大模型的日志解析效率太低</strong>。现有的 LLM
日志解析方法每秒只能处理不到 15000
条日志，如果低于日志产生速度，则非常危险。</li>
</ol>
<p>所以现在迫切需要一种高效率、高准确率的日志解析方法</p>
<h2 id="selflog-方法">SelfLog 方法</h2>
<center>
<img src="/imgs/selfLog/structure.png" width='900'/>
</center>
<p>文章提出了一个基于大语言模型自演化日志解析工具
SelfLog，架构图如上所示，整体分为四个部分：</p>
<ol type="1">
<li><strong>N-Gram-based
Grouper</strong>：这个部分先对日志进行聚类和分组，并提取常量部分，以组为单位让大模型进行解析，减少大模型的调用次数</li>
<li><strong>Log Hitter</strong>：这个部分会检查 Grouper
的常量部分是否有现有模板与之匹配，如果是已知的，则无需调用后续步骤</li>
<li><strong>LLM-based Log
Parser</strong>：以组为单位进行日志解析，输出组的模板</li>
<li><strong>Tree-basd Merger</strong>：修正错误的日志和模板</li>
</ol>
<h3 id="预处理">预处理</h3>
<p>预处理部分主要关注几个部分： -
将日志中的时间、级别去掉，重点关注日志的内容 -
设置分隔符——满足“[A-Za-z0-9*]+”正则表达式的视为token，其他都被视为分隔符（比如_,
|） - 移除日志中的纯数字 -
移除日志中的超低频token（可能是前缀或者后缀）——3个或者少于3个</p>
<p>预处理后，每个日志将会转变为一个 token 列表</p>
<h3 id="n-gram-based-grouper">N-Gram-based Grouper</h3>
<p>这一个模块的目标是：识别并移除 token 列表中的 variable，将剩余的
token 用作分组</p>
<center>
<img src="/imgs/selfLog/grouper.png" width='500'/>
</center>
<p>上图是这一块的伪代码：</p>
<ol type="1">
<li><code>TX = get_token_list(X)</code> 输入：预处理后的 token list</li>
<li><code>position = get_2gram_constant_index(TX)</code>
这一步是输出权重最大的长度为两个词的 token
的索引。假设常量部分一般都是高频词，因为它会稳定出现在衍生的日志中，所以高频词会给较高的权重。</li>
<li><code>variable_list_right = PILAR_gram(TX, position)</code> 从
position 开始向右移动，每个 token 的分数计算为 （与邻居共同出现的次数 /
邻居出现的次数），然后通过3-sigma
判断分数是否异常，如果低于阈值，则判断为 variable</li>
<li><code>variable_list_right = PILAR_gram(TX, position)</code> 与 3
同理</li>
</ol>
<p>经过上述伪代码，token 列表中将只剩下常量 token，然后依据常量 token
列表进行分组，组的 key 值即为常量 token 列表</p>
<p>值得注意的是，即使这一块没有识别出所有的 variable，也可以让后续的
<strong>LLM-based Log Parser</strong> 和 <strong>Tree-basd
Merger</strong> 来修正</p>
<h3 id="log-hitter">Log Hitter</h3>
<p>这个模块的目标是：检查不同组的 key
是否在历史中出现过，如果命中，则直接返回模板；如果没命中，则将这个组中三个编辑距离最大的logs作为
<strong>LLM-based Log Parser</strong> 的输入。</p>
<p><strong>Log Hitter</strong> 比较简单，维护了一个字典，键是 token
列表，值是模板。<strong>LLM-based Log Parser</strong>
的输出也会更新到字典中</p>
<h3 id="llm-based-log-parser">LLM-based Log Parser</h3>
<p>这个模块的目标是：对三个同组的 logs 提取模板</p>
<center>
<img src="/imgs/selfLog/prompt.png" width='500'/>
</center>
<p>上面是 prompt 模板，用的大模型是 GPT 3.5，输出固定为
<strong>分析过程</strong> 和 <strong>模板</strong></p>
<p>需要注意的是 prompt 中的蓝色板块，这一块是实现 self-evolution
的关键，核心思想是： &gt; 建立一个 <strong>Prompt Database</strong>
记录日志和模板，当日志需要解析时，采用类似 RAG
的技术检索到最相似的历史日志和模板作为 example，放到 prompt 中去</p>
<h3 id="tree-based-merger">Tree-based Merger</h3>
<p>事实上，上述过程仍然还会有变量被遗漏，如下图所示</p>
<center>
<img src="/imgs/selfLog/tree-merge.png" width='500'/>
</center>
<p>最开始的时候，只有 user 为 cyrus 的日志，所以会把 cyrus
视为常量，但是随着越来越多 user 的出现，cyrus
应该被划分为变量，所以需要一种后处理机制</p>
<p>如上图所示，SelfLog
维护了一棵树，这棵树是实时更新的，然后执行合并操作，这一块感觉讲的不是很清楚，也不知道是人工合并还是自动化合并</p>
<h2 id="实验">实验</h2>
主实验是在 LogPAI 上进行性能对比
<center>
<img src="/imgs/selfLog/main-exp.png" width='700'/>
</center>
<p>参数和消融实验就不放了</p>
<p>效率实验是在不同日志产生速度和解析速度上的对比</p>
<center>
<img src="/imgs/selfLog/efficiency.png" width='700'/>
</center>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>日志</tag>
        <tag>ISSRE</tag>
        <tag>日志解析</tag>
        <tag>2025</tag>
      </tags>
  </entry>
  <entry>
    <title>[FSE 2025] Cross-System Categorization of Abnormal Traces in Microservice-Based Systems via Meta-Learning</title>
    <url>/2025/05/07/TraFaultDia/</url>
    <content><![CDATA[<blockquote>
<p>题目：Cross-System Categorization of Abnormal Traces in
Microservice-Based Systems via Meta-Learning</p>
<p>FSE 2025</p>
<p>作者：赫尔辛基大学</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>微服务系统（MSS）可能因为复杂性和动态性发生可靠性问题。虽然现有的
AIOps 方法能够通过异常 traces
来定位根因服务，但是仍需要人力来进一步确定根因的故障类型。因此，文章提出了一种故障诊断框架
<code>TraFaultDia</code>，将故障根因和故障类型绑定在一起，给出结果。</p>
<p>文章将故障诊断视作一个分类任务。文章引入了元学习（meta-learning），<u>即每次在有限样本和有限标签组合下进行训练，然后在测试时用极少的有标签样本微调，以应对<strong>新故障类型</strong>和<strong>新系统</strong></u>。文章在两个公开数据集上进行评估，无论是在新故障类型还是新系统上都取得了非常高的性能。</p>
<h2 id="背景">背景</h2>
<p>现有的 AIOps
算法大多是定位到根因服务，没有进一步给出故障类型。特别是对于有些根因不是微服务的故障，比如机器资源不足、虚拟环境资源配置错误等，简单地定位根因服务没有意义。此外，大型
MSS 的 trace 数据量巨大，人工去分析故障类型显得不切实际。</p>
<p>因此，文章决定将根因服务和故障类型一起给出，整体规范如表所示，文章定义了一个概念：</p>
<blockquote>
<p>failure category: pod associated with a fault type</p>
</blockquote>
<p>比如表格中的 (F1-F30， B1-B32) 都是 failure category。注意，有些
failure category 是没有对应的 pod 的。</p>
<center>
<img src="/imgs/TraFaultDia/fault_categories.png" width='700'/>
</center>
<p><font color='blue'>总之，文章把问题简化为了一个多分类问题，但是我有点疑问，对于从未出现过的根因和故障类型的组合，应该怎么办？
</font></p>
<h2 id="挑战">挑战</h2>
<p>文章总结了进行故障诊断的三个挑战：</p>
<ol type="1">
<li><strong>MSS 异质性</strong>：每个 MSS
在业务逻辑和服务组成上有显著不同，比如 TrainTicket 有 45
个微服务，OnlineBoutique 有 12
个微服务，很难设计一个统一的、适用于所有系统的故障诊断方法</li>
<li><strong>MSS 高维度、多模态 trace
数据</strong>：现在很多系统的trace与日志是关联在一起的，包含时序数据、文本数据以及ID
<center>
<img src="/imgs/TraFaultDia/trace.png" width='700'/>
</center></li>
<li><strong>不同故障类型的trace数据量不均衡</strong>：如下图所示，以
TrainTicket 为例，有些故障类型只有26条数据，而有的却有 2546 条</li>
</ol>
<center>
<img src="/imgs/TraFaultDia/statistic.png" width='700'/>
</center>
<h2 id="方法">方法</h2>
<p><code>TraFaultDia</code> 的框架非常清晰，分为两个部分：</p>
<ol type="1">
<li><strong>AttenAE</strong>：这一块用无监督的方式训练了一个 trace
的编码器，结构是常见的自编码器。这一块用于对 trace 进行编码得到特征</li>
<li><strong>TEMAML</strong>：这一块对 trace 特征进行故障诊断，backbone
是一个 transformer-encoder 网络，采用了 meta-learning
的机制进行学习</li>
</ol>
<center>
<img src="/imgs/TraFaultDia/structure.png" width='900'/>
</center>
<h3 id="attenae">AttenAE</h3>
<p>有大量工作设计了专门针对 trace 的表征方法，<code>TraFaultDia</code>
的表征方式也有其独到的地方，由于某些系统 trace 和 log
是关联在一起的，所以需要分别进行表征，具体流程如下图所示：</p>
<center>
<img src="/imgs/TraFaultDia/AttenAE.png" width='900'/>
</center>
<ol type="1">
<li><p>针对 trace：trace 包含的信息大体分为：</p>
<ul>
<li><strong>time-based</strong>：比如每个span的延时，文章直接把每个span的延时拼接为一个数组
<span class="math inline">\(V_{numeric}\)</span></li>
<li><strong>identity</strong>：对于不同的
spanID，这里采用了分层统计的方法，比如上面 Fig.2 的 Span A, Span B, Span
C, Span D, 和 Span E 的 spanID 可以重写为：<span
class="math inline">\(V_{span_id}\)</span>=[1.0, 1.1, 1.2, 2.0,
3.0]，共同的前缀表示这些 span 属于同一个父级操作或服务</li>
<li><strong>textual</strong>：如何表示一个trace经过了哪些操作呢？这里将所有的
span 的 service operation 进行编码得到特征然后平均池化得到 <span
class="math inline">\(V_{operation}\)</span>。这里的具体操作分为：①预处理
② tokenization（用 WordPiece 分割成 subwords） ③ BERT 提取语义信息</li>
</ul></li>
</ol>
<p>最后拼接三个特征得到trace级别的表征 <span
class="math inline">\(V_{span} = Concat(V_{numeric}, V_{span\_id},
V_{operation})\)</span></p>
<ol start="2" type="1">
<li>针对
log：因为系统不断变化，日志模板也会变化，所以文章认为通过日志模板得到特征不可靠，应该关注语义信息，编码方式还是
trace 表征方法中<strong>textual</strong>的三步，得到 <span
class="math inline">\(V_{log}\)</span></li>
</ol>
<p>现在已经得到了初步的表征 <span
class="math inline">\(V_{span}\)</span> 和 <span
class="math inline">\(V_{log}\)</span>，接下来就是自编码，文章在此处引入了多头注意力机制：</p>
<center>
<img src="/imgs/TraFaultDia/atten.png" width='500'/>
</center>
<p>所以，对于一个 trace 集合 <span class="math inline">\(Tr = \{Tr_1,
Tr_2,...,Tr_n\}\)</span>，可以通过上述方法得到表征集合 <span
class="math inline">\(Z=\{Z_1,Z_2,...,Z_n\}\)</span>，然后再解码得到还原后的特征
<span class="math inline">\(\hat{V}_{span}\)</span> 和 <span
class="math inline">\(\hat{V}_{log}\)</span>，Loss是与原特征的L2范数：</p>
<center>
<img src="/imgs/TraFaultDia/attenae-loss.png" width='500'/>
</center>
<h3 id="temaml">TEMAML</h3>
<p>TEMAML 的骨架是 transformer-encoder（TE），输入是 trace 表征集合
<span class="math inline">\(Z\)</span>，输出是 failure
category。整个过程分为两部分：</p>
<center>
<img src="/imgs/TraFaultDia/TEMAML.png" width='900'/>
</center>
<ul>
<li><p><strong>Meta-training</strong>：该阶段旨在训练 TE
找到能够快速适应任何 MSS 分类任务的鲁棒参数。首先定义一个概念
meta-training tasks，表示为 <span class="math inline">\(T=(S,
Q)\)</span>。其中每一个 meta-training task， <span
class="math inline">\(T_i=(S_i, Q_i)\)</span> 都是一个多分类任务，<span
class="math inline">\(S_i\)</span> 和 <span
class="math inline">\(Q_i\)</span> 分别表示第 <span
class="math inline">\(i\)</span> 个任务的 support set 和 query
set。注意，<u>support set 和 query set 均来自训练集</u></p>
<ul>
<li><strong><em>support set</em></strong> <span
class="math inline">\(S_i=\{(z_{ij}^{spt},
y_{ij}^{spt})\}_{j=1}^{N\times K}\)</span> 遵循 N-way K-shot
的初始化方法，即有 N 个独特的标签（failure category），每个标签下有 K
个样本。<span class="math inline">\((z_{ij}^{spt},
y_{ij}^{spt})\)</span> 表示 trace表征集合和对应的failure
category。注意，meta-training tasks 中每个任务虽然都是 N 个标签，但是 N
个标签的组合可能不一样，比如总共有 20 个 failure categories，N 为
5，则每个任务的标签都是从 20 个中选 5 个</li>
<li><strong><em>query set</em></strong> <span
class="math inline">\(Q_i=\{(z_{ig}^{qry},
y_{ig}^{qry})\}_{g=1}^{N\times K}\)</span> 有 N 个独特的标签（failure
category），每个标签下有 M 个样本，M &gt; K，i.e., (<span
class="math inline">\(|Q_i|&gt;|S_i|\)</span>)</li>
</ul></li>
</ul>
<blockquote>
<p>训练过程有两个循环：① inner loop 和 ② outer loop，定义基础模型为
<span class="math inline">\(f\)</span></p>
<ul>
<li><p><strong><em>inner loop</em></strong>: 在 <span
class="math inline">\(S_i\)</span>
上操作，负责对每个任务进行分别学习，并更新模型 <span
class="math inline">\(f\)</span> 参数 <span
class="math inline">\(\theta\)</span></p>
<ul>
<li>针对任务 <span class="math inline">\(T_i\)</span>，学习率为 <span
class="math inline">\(\alpha\)</span>，参数更新如下：<span
class="math inline">\(\theta_i{&#39;}=\theta-\alpha
\nabla_{\theta}\mathcal{L}_{T_i}(f_{\theta}(S_i))\)</span></li>
</ul></li>
<li><p><strong><em>outer loop</em></strong>: 在 <span
class="math inline">\(Q_i\)</span>
上操作，对于所有任务进行优化，并更新模型 <span
class="math inline">\(f\)</span></p>
<ul>
<li>在所有任务上进行优化：<span
class="math inline">\(\min\limits_{\theta} \mathcal{L}(\theta) =
\sum_{T_i \in T} \mathcal{L}_{T_i}(f_{\theta&#39;_i}(Q_i))\)</span></li>
</ul></li>
</ul>
</blockquote>
<ul>
<li><strong>Meta-testing</strong>：该阶段目的是适应任意 MSS
的新的多分类任务。对于某个特定的测试任务 <span
class="math inline">\(T_{ts}=(S_{ts}, Q_{ts})\)</span>，在 <span
class="math inline">\(S_{ts}\)</span> 上进行微调，然后用 <span
class="math inline">\(Q_{ts}\)</span> 进行测试</li>
</ul>
<h2 id="实验">实验</h2>
<p>实验采用的数据集来自 TrainTicket 和 OnlineBoutique，配置如下：</p>
<ul>
<li><strong>TrainTicket</strong>：20 基础故障类型 + 10 新故障类型</li>
<li><strong>OnlineBoutique</strong>：22 基础故障类型 + 10
新故障类型</li>
</ul>
<p>训练配置：</p>
<ul>
<li><strong>meta-training</strong>：4 meta-training tasks
(基础故障类型)；5-way 5-shot；query set M=15</li>
<li><strong>meta-testing</strong>：50 meta-testing tasks
(新故障类型)；5-way 10-shot；query set M=15</li>
</ul>
<p>文章设计了四个实验来证明 <code>TraFaultDia</code>
在不同场景下的性能：</p>
<ol type="1">
<li>E1 (TrainTicket → TrainTicket) 在 TrainTicket 的 4 meta-training
tasks 训练，在 TrainTicket 的 50 meta-testing tasks 测试</li>
<li>E2 (OnlineBoutique → OnlineBoutique) 在 OnlineBoutique 的 4
meta-training tasks 训练，在 OnlineBoutique 的 50 meta-testing tasks
测试</li>
<li>E3 (OnlineBoutique → TrainTicket) 在 OnlineBoutique 的 4
meta-training tasks 训练，在 TrainTicket 的 50 meta-testing tasks
测试</li>
<li>E4 (TrainTicket → OnlineBoutique) 在 TrainTicket 的 4 meta-training
tasks 训练，在 OnlineBoutique 的 50 meta-testing tasks 测试</li>
</ol>
<p>下面是实验结果</p>
<center>
<img src="/imgs/TraFaultDia/exp1.png" width='700'/>
</center>
<center>
<img src="/imgs/TraFaultDia/exp2.png" width='700'/>
</center>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>trace</tag>
        <tag>根因定位</tag>
        <tag>FSE</tag>
        <tag>2025</tag>
      </tags>
  </entry>
  <entry>
    <title>[ICLR 2025] OpenRCA: Can Large Language Models Locate the Root Cause of Software Failures</title>
    <url>/2025/05/08/openRCA/</url>
    <content><![CDATA[<blockquote>
<p>题目：OpenRCA: Can Large Language Models Locate the Root Cause of
Software Failures</p>
<p>来源：ICLR 2025</p>
<p>作者：香港中文大学（深圳）</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>大模型（LLM）推动了软件工程领域的实质性进步。然而现有的研究大多关注
LLM
在软件开发阶段的作用，比如代码生成，忽视了在开发后阶段（<em>post-development</em>）的工作，而这个阶段往往直接关乎用户的体验。文章推出了
OpenRCA，包含一个benchmark数据集和一个评估框架，用于衡量 LLM
在定位软件故障根因上的能力。</p>
<p>OpenRCA 包含 335 个真实场景的 failure case，这些 failure case 来自 3
个企业系统，并附带有 68 GB 的多模态遥测数据（metric，trace，log）。LLM
需要接收给定的 failure case
和对应的遥测数据，加以推理，然后识别出故障根因。实验表明，即使是表现最好的
LLM，也只能解决 11.34% 的 failure case。</p>
<h2 id="背景">背景</h2>
<p>对于在线系统的软件维护和debug是非常困难的，虽然有大量工作在致力于通过多模态遥测数据来定位故障根因（RCA），但是这个任务仍然具有较大的挑战性，因为线上软件系统有着难以估计的复杂性。</p>
<center>
<img src="/imgs/openRCA/background.png" width='900'/>
</center>
<p>LLM 是否能胜任 RCA 工作呢？文章为此提出了一个 benchmark
数据集：OpenRCA， 包含 335 个真实场景的 failure case，这些 failure case
来自 3 个企业系统，并附带有 68 GB
的多模态遥测数据（metric，trace，log）。如上图所示，对于每个 failure
case 以及一个对应的自然语言的 query， LLM
需要分析大佬的多模态遥测数据，理解系统之间的内在关联，并推理出可能的故障根因。</p>
<h2 id="openrca">OpenRCA</h2>
<h3 id="特点">特点</h3>
<p>OpenRCA 包含了大佬真实场景的 failure case，有如下特点：</p>
<ul>
<li>真实场景</li>
<li>目标驱动的任务设计（不再是简单定位一个故障组件，而是通过自然语言表达任务）</li>
<li>多模态异构遥测数据</li>
<li>完整的 LLM 评估</li>
<li>支持新标签和新遥测数据的集成</li>
</ul>
<h3 id="问题建模">问题建模</h3>
<p>故障根因有三个元素：<strong>根因组件</strong>（originating
component）、<strong>开始时间</strong>（start
time）、<strong>故障原因</strong>（failure
reason）。每个人的目标可能不一样，所以需要对上述元素进行组合得到目标。</p>
<blockquote>
<p>OpenRCA 定义了7个目标，是3个元素的组合（<span
class="math inline">\(C_3^1+C_3^2+C_3^3\)</span>， LLM
的输出应该是7种目标中的一种 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;task_7&quot;: &#123;</span><br><span class="line">    &quot;input&quot;: [</span><br><span class="line">        &quot;time range: &#123;time_period&#125;&quot;,</span><br><span class="line">        &quot;number of failures: &#123;num&#125;&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;output&quot;: [</span><br><span class="line">        &quot;root cause component: &#123;component&#125;&quot;,</span><br><span class="line">        &quot;root cause occurrence time: &#123;datetime&#125;&quot;,</span><br><span class="line">        &quot;root cause reason: &#123;reason&#125;&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p><strong>评估</strong>：对于每个 failure
case，如果输出的内容符合实际，则加一分，否则不得分。这里要避免文本表达差异而导致的评估错误，prompt
中预先提供了所有可能的故障原因和原始组件。最终计算 <span
class="math inline">\(accuracy\)</span> ，是所有分数的均值。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;1&quot;: &#123;</span><br><span class="line">        &quot;root cause occurrence datetime&quot;: (A time in ’%Y-%m-%d %H:%M:%S’ format),</span><br><span class="line">        &quot;root cause component&quot;: (A component selected from the given’ possible root cause component’),</span><br><span class="line">        &quot;root cause reason&quot;: (A reason selected from the given’ possible root cause reason’),</span><br><span class="line">    &#125;,</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="数据集构成">数据集构成</h3>
<p>OpenRCA 使用的数据集均来自历年的 AIOps
大赛，由于脏数据较多、某些故障标签缺乏（failure
reason）等问题，首先采用四步处理方式预处理：</p>
<center>
<img src="/imgs/openRCA/process.png" width='900'/>
</center>
<ol type="1">
<li><strong>System Selection</strong>:
历年数据集中有些系统因为数据和标签不完备被淘汰，选用满足要求的3个系统</li>
<li><strong>Data Balancing</strong>:
系统之间数据规模差距太大，对大数据量系统进行下采样</li>
<li><strong>Data Calibration</strong>: 规范命名以及人工筛选 failure
case</li>
<li><strong>Query Synthesis</strong>: 3个元素组合而成7个目标</li>
</ol>
<p>最终数据集组成如下：</p>
<center>
<img src="/imgs/openRCA/dataset.png" width='900'/>
</center>
<h2 id="rca-agent">RCA-Agent</h2>
<p>为了解决 OpenRCA 中的任务，首先要面对两个挑战</p>
<p>第一个挑战是<strong>如何处理超大规模的遥测数据</strong></p>
<blockquote>
<p>直观的解决方法是将所有遥测数据整合为一个
chunk，但是低效且开销巨大的；另一个方法是采样一个遥测数据的子集，但是有丢失关键信息的风险</p>
</blockquote>
<p>第二个挑战是<strong>遥测数据不是自然语言</strong>， LLM
可能无法有效处理：</p>
<blockquote>
<p>可选的处理方法是先对所有遥测数据进行代码处理，然后提取关键信息到
LLM</p>
</blockquote>
<p>针对上述挑战，文章也提出了一个多智能体的解决方案（RCA-Agent），RCA-Agent
包含两个 LLM 智能体（Controller 和 Executor）</p>
<center>
<img src="/imgs/openRCA/RCA-agent.png" width='900'/>
</center>
<ul>
<li><p><strong>Controller</strong>：负责整个流程的调度，指导 LLM 按照
anomaly detection -&gt; fault identification -&gt; root cause
localization 进行诊断；指导 LLM 按照 metric -&gt; trace -&gt; log
的顺序进行分析</p></li>
<li><p><strong>Executor</strong>：在 Controller 的指导下写 Python
代码、执行代码、返回结果给 Controller。由两部分组成</p>
<ul>
<li>code generator: 生成 Python 代码</li>
<li>code executor: 有个 Python kernel 负责执行</li>
</ul></li>
</ul>
<blockquote>
<p>RCA-Agent 工作流：</p>
<ol type="1">
<li>Controller 指示 Executor 加载遥测数据（Executor
自己生成并执行代码）</li>
<li>Executor 返回结果给 Controller</li>
<li>Controller 分析决策并决定下一个动作</li>
<li>Controller 和 Executor 不断交互直到最终结果给出</li>
</ol>
</blockquote>
<h2 id="实验">实验</h2>
<p>之前提到不可能把所有的遥测数据都输入到
LLM，所以只能用采样来减轻负担，比如只取用每分钟的第一条记录，此外，进一步引入了两个
KPIs 采样方式作为 RCA-Agent 的对比：</p>
<ul>
<li>Oracle Sampling：工程师选出最有价值的 KPIs</li>
<li>Balanced Sampling：随机采样与 Oracle Sampling 同等数量的 KPIs</li>
</ul>
<p>文章比较了现有的开源模型在 OpenRCA 上的表现</p>
<center>
<img src="/imgs/openRCA/exp1.png" width='700'/>
</center>
<h2 id="prompt">Prompt</h2>
<p>这里附上 RCA-Agent 细节和主要 Prompt</p>
<p><strong>Controller System Prompt</strong> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">You are the Administrator of a DevOps Assistant system for failure</span><br><span class="line">diagnosis. To solve each given issue, you should iteratively instruct</span><br><span class="line">an Executor to write and execute Python code for data analysis on</span><br><span class="line">telemetry files of target system. By analyzing the execution results,</span><br><span class="line">you should approximate the answer step-by-step.</span><br><span class="line"></span><br><span class="line">There is some domain knowledge for you:</span><br><span class="line">&#123;BACKGROUND KNOWLEDGE OF SYSTEM&#125;</span><br><span class="line"></span><br><span class="line">## RULES OF FAILURE DIAGNOSIS:</span><br><span class="line"></span><br><span class="line">What you SHOULD do:</span><br><span class="line">1. **Follow the workflow of ‘preprocess -&gt; anomaly detection -&gt; fault</span><br><span class="line">identification -&gt; root cause localization‘ for failure diagnosis.**</span><br><span class="line">...</span><br><span class="line">What you SHOULD NOT do:</span><br><span class="line">1. DO NOT include any programming language in your response.</span><br><span class="line">...</span><br><span class="line">The issue you are going to solve is:</span><br><span class="line"></span><br><span class="line">&#123;PROBLEM TO SOLVE&#125;</span><br><span class="line">Solve the issue step-by-step. In each step, your response should follow</span><br><span class="line">the JSON format below:</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;analysis&quot;: (Your analysis of the code execution result from Executor</span><br><span class="line">    in the last step, with detailed reasoning of ’what have been done’</span><br><span class="line">    and ’what can be derived’. Respond ’None’ if it is the first step</span><br><span class="line">    .),</span><br><span class="line">    &quot;completed&quot;: (&quot;True&quot; if you believe the issue is resolved, and an</span><br><span class="line">    answer can be derived in the ’instruction’ field. Otherwise &quot;False</span><br><span class="line">    &quot;),</span><br><span class="line">    &quot;instruction&quot;: (Your instruction for the Executor to perform via code</span><br><span class="line">    execution in the next step. Do not involve complex multi-step</span><br><span class="line">    instruction. Keep your instruction atomic, with clear request of ’ what to do’ and ’how to do’. Respond a summary by yourself if you believe the issue is resolved.)</span><br><span class="line">&#125;</span><br><span class="line">Let’s begin.</span><br></pre></td></tr></table></figure></p>
<p><strong>Executor System Prompt</strong> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">You are a DevOps assistant for writing Python code to answer DevOps</span><br><span class="line">questions. For each question, you need to write Python code to solve</span><br><span class="line">it by retrieving and processing telemetry data of the target system.</span><br><span class="line">Your generated Python code will be automatically submitted to a</span><br><span class="line">IPython Kernel. The execution result output in IPython Kernel will be</span><br><span class="line">used as the answer to the question.</span><br><span class="line">## RULES OF PYTHON CODE WRITING:</span><br><span class="line">1. Reuse variables as much as possible for execution efficiency since the</span><br><span class="line">IPython Kernel is stateful, i.e., variables define in previous steps</span><br><span class="line">can be used in subsequent steps.</span><br><span class="line">...</span><br><span class="line">There is some domain knowledge for you:</span><br><span class="line">&#123;BACKGROUND KNOWLEDGE OF SYSTEM&#125;</span><br><span class="line">Your response should follow the Python block format below:</span><br><span class="line">‘‘‘python</span><br><span class="line">(YOUR CODE HERE)</span><br><span class="line">‘‘‘</span><br></pre></td></tr></table></figure></p>
<p><strong>Summary Prompt</strong>。Controller 认为任务已经完成，则这个
Prompt将会发给 Controller <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Now, you have decided to finish your reasoning process. You should now</span><br><span class="line">provide the final answer to the issue. The candidates of possible</span><br><span class="line">root cause components and reasons are provided to you. The root cause</span><br><span class="line">components and reasons must be selected from the provided candidates</span><br><span class="line">.</span><br><span class="line">&#123;BACKGROUND KNOWLEDGE OF SYSTEM&#125;</span><br><span class="line">Recall the issue is: &#123;PROBLEM TO SOLVE&#125;</span><br><span class="line">Please first review your previous reasoning process to infer an exact</span><br><span class="line">answer of the issue. Then, summarize your final answer of the root</span><br><span class="line">causes using the following JSON format at the end of your response:</span><br><span class="line">&#123;OPENRCA ANSWER FORMAT&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>根因定位</tag>
        <tag>大模型</tag>
        <tag>2025</tag>
        <tag>多模态</tag>
        <tag>benchmark</tag>
        <tag>ICLR</tag>
      </tags>
  </entry>
  <entry>
    <title>[ISSTA 2025] TRACEZIP: Efficient Distributed Tracing via Trace Compression</title>
    <url>/2025/05/09/TraceZip/</url>
    <content><![CDATA[<blockquote>
<p>题目：TRACEZIP: Efficient Distributed Tracing via Trace
Compression</p>
<p>来源：FSE 2025</p>
<p>作者：中山大学</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>分布式追踪（distributed
tracing）是监控微服务系统内部执行逻辑的重要手段，但同时也给系统带来了巨大的计算和存储的负担。现有的方式是通过采样（sampling）来捕获更少的trace，然而，现有的两种
sampling 方式面临采样质量和系统负载的抉择：</p>
<ul>
<li><em>head-based
sampling</em>：无差别选择某些请求进行追踪，容易遗漏关键数据</li>
<li><em>tail-based sampling</em>：追踪所有请求，存储 edge-case
traces，但 trace 收集和传输时的负载仍然很大</li>
</ul>
<p>文章从另一个角度触发，提出了 TraceZip，通过 trace compression
来减少各阶段的负载。核心思想是 <strong>trace
数据结构之间存在显著的冗余，导致微服务和后端之间大量相同数据的重复传输</strong>。文章设计了一种新的数据结构
Span Retrival Tree（SRT），能够持续压缩这种冗余，以轻量化的方式传输
traces；在后端，完整的 traces 可以基于先前的通用数据重建。TraceZip
已经在 OpenTelemetry Collector 中实现，能够与现有的 tracing API 兼容</p>
<h2 id="背景">背景</h2>
<p>trace
是理解微服务系统行为，诊断微服务故障的重要工具。在生产环境中，捕获所有请求的
traces 会造成显著的负载，这些开销来自于 trace
的生产、收集和存储。为了减少负担，通过会对 trace 进行采样。head-based
sampling 在请求执行前随机选择一定比例的请求进行追踪并产生
traces，极大的减少了trace
的生产、收集和存储的开销，但是容易无差别遗漏一些值得关注的
traces，比如异常行为形成的 edge-case traces（e.g., 高延时）。tail-based
sampling 追踪所有的 traces，但是会在后端根据某些属性选择哪些 traces
进行保留。由于 traces
的价值具有不可预测性，很难完全覆盖测试和故障诊断所需的所有 traces。同时
tail-based sampling 的计算和传输开销依旧很大</p>
<center>
<img src="/imgs/tracezip/procedure.png" width='300'/>
</center>
<p>因此，文章提出了一种针对 trace 的压缩方法
TraceZip，在微服务端用一种更简洁的数据结构压缩 trace 中的
span，在后端能够无缝还原为原始的
span，这样能够大幅减少传输中的开销。而传统的日志压缩无法适用于 trace
这种流式数据</p>
<h2 id="动机">动机</h2>
<p>文章部署了 Train Ticket 系统，并通过 OpenTelemetry
提供的无侵入方式进行监控，收集了大约 40 GB 的
traces。<u>文章通过每个微服务产生的重复 key-value (KV) 对的比例来量化
trace 的冗余</u>。对于每个 KV 对，冗余比即为它在所有 KV
对出现次数的比例，比如一个 trace 数据集有 100个 KV 对，如果有两个 KV
对分别出现 1 次 和 10 次，则冗余比为 1% 和 10%。</p>
<center>
<img src="/imgs/tracezip/empirical.png" width='700'/>
</center>
<p>通过实证分析，左图是 Train Ticket
常规微服务，右图是常见的中间件，文章得到两个结论：</p>
<ul>
<li><strong>Trace
是高度冗余的</strong>。文章将冗余次数阈值设置为1000，计算&gt;1000和&lt;1000的
KV对在每个服务中的比例（这里有点怪，从冗余比计算变成了冗余次数）。从图2的第一张图可以看出，微服务中大概
70% 的 KV
对是高度冗余的；从图2的第二张图可以看出，除了个别组件（Kafka），因为
kafka 的 traces
包含了消息队列中的数据信息，有随机性。<font color='blue'>这是因为服务经常参与标准交互，生成的trace具有相似的模式。通过在微服务端捕获
span
生成时的冗余，我们可以提前消除后端已经存在的重复数据的传输。后端可以根据冗余模式轻松重建完整的
span </font></li>
<li><strong>属性之间存在结构化冗余</strong>。OpenTelemetry 为 span
的通用属性（Key）提供了标准的命名规范，用于保持跨语言的一致性，这个命名机制也存在一些冗余，比如
network.local.address, network.local.port, network.peer.address
有一些共同的单词；值（Value）也存在一些冗余，比如 Java Exception has
java.io.IIOException, java.io.EOFException 等，<font color='blue'>
通过移除这些细粒度的冗余，也可以进一步减少 span 传输的负载 </font></li>
</ul>
<h2 id="方法">方法</h2>
<p>TraceZip 架构图如下所示，主要是加了两个模块 compression 和
decompression</p>
<ul>
<li><strong>compression</strong>：在微服务端，维护两个数据结构：Span
Retrieval Tree (SRT) 和一个字典，持续捕获和压缩 spans 的冗余。如果 span
携带了新的冗余信息，则会无缝集成到上述数据结构中。</li>
<li><strong>decompression</strong>：在后端 collector 入口处，spans 基于
SRT 和字典进行重建。</li>
<li><strong>differential update mechanism</strong>：微服务端和后端的 SRT
和字典需要持续同步，这个机制会同步两侧数据结构的增量变化，确保低开销的高效协同</li>
</ul>
<center>
<img src="/imgs/tracezip/structure.png" width='500'/>
</center>
<h3 id="span-格式">Span 格式</h3>
<p>Trace 是树状结构，Span 是树中的节点。Span 本身是一个 JSON 结构的 KV
对集合，键是 String 类型，而值可以是原始数据类型（String, number,
boolean,
null），也可以是结构化类型（KV字典，array），为了对齐现有框架，一般包含以下字段：</p>
<blockquote>
<ul>
<li><em>name</em>,</li>
<li><em>parentSpanId</em>,</li>
<li><em>Start</em> and <em>End</em> Timestamps,</li>
<li><em>Span Context</em> (the context of the span including the trace
ID, the span ID, etc.),</li>
<li><em>Attributes</em>,</li>
<li><em>Span Events</em></li>
</ul>
</blockquote>
<p>文章在这里有个前提，就是一个操作的 Span
结构不会发生变化，也就是说某个操作生成的 所有 spans 的 Keys
是相同的，只有 Values 会变化</p>
<h3 id="span-压缩和解压">Span 压缩和解压</h3>
<p>Span 的直观压缩方式就是使用字典，将每个独特的 KV
对赋予并替换为对应的独特 ID。但这些 KV
对之间还存在细粒度的冗余。除此之外，如果大量 Span 共享同一套 KV
对组合，就可以用一个 ID 来表示这个组合，进一步减少冗余</p>
<center>
<img src="/imgs/tracezip/SRT.png" width='500'/>
</center>
<p>如图所示，文章提出了一种 SRT 的多叉树来减少冗余，其中有三种节点：</p>
<ul>
<li><strong>root</strong>：存储一个基础时间，可以让 leaf
只存时间偏移</li>
<li><strong>non-leaf</strong>：存有较多重复的 Value，比如 name 和 status
的 Value 只有几种，则把这些 Value 存到 non-leaf 中</li>
<li><strong>leaf</strong>：只存 Value 可选数量太多的 Key，比如
SpanId，time_offset 之类</li>
</ul>
<center>
<img src="/imgs/tracezip/raw_spans.png" width='700'/>
</center>
<p>比如 Fig.4 的 SRT 实际上就是由上述两个表格演变来的，每个 span
都可以看作从 root 节点到 leaf 节点的一条路径。演变算法如下所示：</p>
<center>
<img src="/imgs/tracezip/algo.png" width='500'/>
</center>
<blockquote>
<ol type="1">
<li>接收一个 span 数据流，并处理每一个span</li>
<li>如果 span 的 name 不存在于 SRT
中（根节点的子节点，level=1），则将这个 span 的所有 values
串成一条路径，放到 SRT 中。
<ul>
<li>比如 Table 1-(a) 的 span 可以表示为 Access Mem <span
class="math inline">\(\to\)</span> WRITE <span
class="math inline">\(\to\)</span> address1 <span
class="math inline">\(\to\)</span> 64 bytes <span
class="math inline">\(\to\)</span> id1</li>
</ul></li>
<li>如果 span 的 name 存在于 SRT 中，则从 SRT 中从上到下逐层检验 span
的所有 KV 对是否存在于 SRT 中
<ul>
<li>如果不存在，则需要将新 Value 作为新分支加进 SRT</li>
<li>比如 READ <span class="math inline">\(\to\)</span> address2 <span
class="math inline">\(\to\)</span> 128 bytes <span
class="math inline">\(\to\)</span> id2 将作为新分支加入到 Access Mem
下</li>
</ul></li>
<li>一旦有新分支加入（即执行了步骤3），则需要检验 SRT
各层的值个数，如果超过阈值 𝜓，则这一层对应的 Key 将会被移到 leaf
<ul>
<li>比如 id 那一层有太多值，则移到 leaf</li>
<li>对于某些值是 KV 字典这种复合结构的，则按照如下案例处理：
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   “attributes”: &#123;</span><br><span class="line">      “ip”: “172.17.0.1”, </span><br><span class="line">      “port”: 26040</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line">&gt;------------------------------------------</span><br><span class="line">&#123;</span><br><span class="line">     “attributes-ip”: “172.17.0.1”, </span><br><span class="line">     “attributes-port”: 26040</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>此时只需要 Span 在 SRT 中的路径（可以用路径ID表示）和 leaf 中 key
对应的值就可以还原出 Span 了</li>
</ol>
</blockquote>
<h3 id="srt-内存的优化">SRT 内存的优化</h3>
<p>虽然 SRT 已经极大节省了传输成本，但仍存在较大的空间复杂度，因为 SRT
的潜在增长空间太大，所以文章进行了进一步的优化</p>
<ol type="1">
<li><strong>SRT 重建</strong>： Fig.5 是从 Table1.(b) 中建立来的
SRT，可以看到，如果 key
的顺序不当，会造成大量重复。正如上章节算法中第15行，<strong>应该在每次
SRT 更新时把 Values 较多的 Key 往下移</strong>。</li>
</ol>
<center>
<img src="/imgs/tracezip/SRT_reconstruct.png" width='500'/>
</center>
<ol start="2" type="1">
<li><strong>基于字典的压缩</strong>：Fig.4 中可以看到还是有一些 Value
重复，比如 64 bytes。文章构建了一个字典，这个字典的键是 spans 的
K/V（只包括 SRT 中的内容，不包括 leaf 中 Key 对应的 Value），值是
ID。ID由[0-9a-zA-Z]组成，先从长度为一开始（ ’0’ to ’9,’，’A’ to
’Z.’），然后再往上递增长度。只要字典发生更新，就会传输到后端。</li>
</ol>
<h3 id="srt-搜索的优化">SRT 搜索的优化</h3>
<p>文章还优化了 SRT 的搜索部分，因为 SRT
涉及频繁的修改，所以最直观的实现方式是链表。然而，链表中各节点不是存储在连续的内存空间中，所以查询时会导致较多的预读失效，影响路径搜索时的效率。</p>
<blockquote>
<p>TraceZip 在微服务端维护了一个 hash 表：{path: identifier}，当 span
到来时，就不用遍历 SRT 来查询是否存在路径了，直接通过 hash
表获取路径ID</p>
</blockquote>
<h3 id="differential-data-synchronization">Differential Data
Synchronization</h3>
<p>为了 Span 压缩和解压，微服务端和后端必须要时刻保持 SRT
和字典的同步。最直观的方式就是<strong>发生更改时就将 SRT
和字典发向后端</strong>，但有的时候更改往往是一小段，如果将整个数据都发向后端，就会造成额外的网络开销，甚至会阻塞解压操作。</p>
<blockquote>
<p>TraceZip 的后端，也就是 trace 的接收器，不再维护一个与微服务端一致的
SRT，转而维护了一个 hash 表 {identifier: path}，这样只需要同步两端的
{path: identifier} 和 {identifier:
path}，只有路径发生新增或删除时才会同步</p>
<ul>
<li>对于字典的更新，只要发生了更改就会同步到接收器，这里为了保证接收器的数据不过时，用了
OpenTelemetry Collector
的批处理功能，在压缩缓冲区中的Span后，将确保在发布数据之前 SRT
和字典已经与接收方同步。</li>
</ul>
</blockquote>
<h2 id="实验">实验</h2>
<p>比较的指标是压缩中常用的压缩率： <span class="math display">\[
    CR=\frac{Original File Size}{Compressed File Size}
\]</span></p>
<p>对比方法采用的是常见的压缩算法：gzip, bzip2, lzma.</p>
<center>
<img src="/imgs/tracezip/exp1.png" width='700'/>
</center>
<p>同时探究了不同压缩算法的吞吐量 (MB/s)</p>
<center>
<img src="/imgs/tracezip/throughput.png" width='500'/>
</center>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>trace</tag>
        <tag>2025</tag>
        <tag>ISSTA</tag>
        <tag>trace压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>[arxiv 2024] The Vision of Autonomic Computing: Can LLMs Make It a Reality</title>
    <url>/2025/05/15/ACV-LLM/</url>
    <content><![CDATA[<blockquote>
<p>题目：The Vision of Autonomic Computing: Can LLMs Make It a
Reality</p>
<p>来源：arxiv 2024</p>
<p>作者：南京大学，微软</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>智能运维的最终梦想是想让微服务系统能够自主诊断和恢复，这篇文章向这个方向努力探出了一步。文章的目标是实现微服务系统的自主计算的愿景（ACV,
Vision of Autonomic Computing），大模型的出现让 ACV
的实现出现了可能性。</p>
<p>文章推出了一个基于大模型的分层多智能体架构，用于维护微服务系统的可靠性。其中，高级别群组管理者（high-level
group
manager）用于接收声明式任务，比如优化延时到200ms以下；低级别自主智能体（low-level
autonomic agent）聚焦实施各种具体任务。</p>
<p>为了评估此套自动化系统，文章提出了一种五层分类法，重点关注自优化和自恢复；此外，文章还在
SockShop
中进行了实战演练，通过混沌工程注入故障并观察系统如何自恢复。</p>
<h2 id="背景">背景</h2>
<h3 id="自主计算">自主计算</h3>
<p>在智能运维中，自主计算的目标是减少微服务系统维护的复杂性，提升可靠性和性能，之前的
ACV 文章提出了四个自主计算的目标：</p>
<ul>
<li><strong>自配置</strong>：可以配置和重配置系统，以满足目标</li>
<li><strong>自优化</strong>：可以持续监控系统，并找到机会优化系统以提升性能和减少开销</li>
<li><strong>自恢复</strong>：发生故障时恢复，甚至预测故障</li>
<li><strong>自保护</strong>：防御恶意进攻和故障传播</li>
</ul>
<p>现有的自主计算一般采用 MAPE-K 的架构，即
Monitor，Analyze，Plan，Execute，Knowledge Base。有大量工作基于
rule-based
方法，在特定场景下有用，但是在复杂动态的微服务系统中需要作出自适应和上下文感知的决策，rule-based
方法无法做到，所以越来越多的方法采用 AI 来替代。</p>
<p>云原生应用自主管理显得更为困难，因为大多拥有复杂的系统结构，安全和可靠性也存在高要求。虽然有大量工具来帮助管理，比如
Kubernetes 和
Prometheus，但是这些工具都无法将人类意图直接转化为对应的功能，有着极高的学习和操作成本。</p>
<p>随着 LLM 的快速发展，大量研究尝试将 LLM 集成到 Kubernetes
中参与微服务系统的管理，比如 <a
href="https://arxiv.org/abs/2405.19954">GenKubeSec</a> 和 <a
href="https://docs.k8sgpt.ai/">K8sGPT</a> ，也有些工作将 LLM
集成到智能运维中，但这些工作都无法实现自管理。</p>
<h2 id="方法">方法</h2>
<h3 id="方法概述">方法概述</h3>
<center>
<img src="/imgs/ACV/structure.png" width='400'/>
</center>
<p>文章提出了一种 ACV 架构， 整体上分为2层：</p>
<ol type="1">
<li><code>低级别自主智能体</code>: 用于执行简单的维护代码，充分使用 LLM
的代码生成和执行能力</li>
<li><code>高级别群组管理者</code>: 分解复杂任务为多个 sub-tasks，制定
step-by-step 计划，下发 sub-task
到具体的低级别自主智能体，接收反馈并判断有没有完成目标</li>
</ol>
<p>而对于每个智能体的设计，不同于传统的 MAPE-K 架构，基于 LLM 的 ACV
中每个智能体都是由两个模块组成：<strong>Planner</strong> 和
<strong>Executor</strong>，<strong>Planner</strong>
负责制定执行计划（监控、分析），<strong>Executor</strong>
负责执行具体步骤，并反馈结果给 <strong>Planner</strong></p>
<h3 id="三种负载机制">三种负载机制</h3>
<ol type="1">
<li><p><strong>低级别自主智能体</strong>独立工作</p>
<ul>
<li>简单的任务（扩容副本到3）可以直接下发到一个<strong>低级别自主智能体</strong></li>
</ul></li>
<li><p><strong>高级别群组管理者</strong>引导下的多智能体协同</p>
<ul>
<li>复杂的任务（将延时控制到200ms以下），按照如下顺序：分解任务、制定计划、分配任务到低级智能体、接收反馈并调整计划</li>
</ul></li>
<li><p><strong>低级别自主智能体</strong>的协同（无<strong>高级别群组管理者</strong>）</p>
<ul>
<li>如果一个低级别智能体无法处理工作时，可能不经过高级别智能体，直接找另一个低级智能体协同</li>
</ul></li>
</ol>
<p>智能体之间的通信方式是<strong>消息队列</strong></p>
<h3 id="sockshop-实施案例">SockShop 实施案例</h3>
<center>
<img src="/imgs/ACV/example.png" width='900'/>
</center>
<p>## 实验评估</p>
<p>首先文章推出了<strong>五层分类法</strong>来评估智能体系统的能力：</p>
<ol type="1">
<li><strong>L1</strong>：智能体是否能选择正确的运维操作指令</li>
<li><strong>L2</strong>：智能体能否有计划能力，将任务分解为多步执行</li>
</ol>
<p>L1 和 L2 针对的都是祈使任务（imperative
task），直接表达要做某个具体的任务，智能体只是被动执行。更高级的自主智能体应该能够应对声明式任务（declarative
task），即主动采取一些动作来完成目标</p>
<p>如下图所示，文章在自主管理微服务系统中用具体案例表示了剩下的 L3、L4
和 L5</p>
<center>
<img src="/imgs/ACV/taxonomy.png" width='900'/>
</center>
<p>为了评估 ACV 的能力，首先是评估 L1 和
L2，这些测试任务中既包括部署阶段的，也包括运行时的</p>
<center>
<img src="/imgs/ACV/L1-L2.png" width='900'/>
</center>
<p>对于 L3，L4 和 L5，文章在 SockShop
中用混沌工程注入故障，观察故障自愈情况，注入故障包括三类：</p>
<ul>
<li>Pod Failure：将 Catalogue
服务的镜像换成一个假的、没有功能的镜像</li>
<li>CPU Stress：将 Catalogue 的 POD 的 CPU 打到 100%</li>
<li>Rising Traffic：逐步升高负载，直至无法承受</li>
</ul>
<p>为了评估故障自愈情况，定义了如下 SLO：</p>
<ol type="1">
<li>所有的服务需要处于 READY 状态</li>
<li>所有的服务 CPU 和 内存的使用率需要在 50% 以下</li>
<li>所有服务的 P99 延时控制在 200ms 以下</li>
</ol>
<p>实验流程和实验结果如下，每个实验跑三次：</p>
<center>
<img src="/imgs/ACV/exp.png" width='900'/>
</center>
<p>同时文章也给出了一个高级群组智能体规划的复杂任务案例：</p>
<center>
<img src="/imgs/ACV/case.png" width='900'/>
</center>
]]></content>
      <categories>
        <category>文献精读</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>arxiv</tag>
        <tag>大模型</tag>
        <tag>2024</tag>
        <tag>多智能体</tag>
        <tag>自主维护</tag>
      </tags>
  </entry>
</search>
