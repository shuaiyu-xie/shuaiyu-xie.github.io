[{"title":"[ICLR 2025] OpenRCA: Can Large Language Models Locate the Root Cause of Software Failures","date":"2025-05-08T04:44:00.000Z","path":"2025/05/08/openRCA/","text":"题目：OpenRCA: Can Large Language Models Locate the Root Cause of Software Failures 来源：ICLR 2025 作者：香港中文大学（深圳） 摘要 大模型（LLM）推动了软件工程领域的实质性进步。然而现有的研究大多关注 LLM 在软件开发阶段的作用，比如代码生成，忽视了在开发后阶段（post-development）的工作，而这个阶段往往直接关乎用户的体验。文章推出了 OpenRCA，包含一个benchmark数据集和一个评估框架，用于衡量 LLM 在定位软件故障根因上的能力。 OpenRCA 包含 335 个真实场景的 failure case，这些 failure case 来自 3 个企业系统，并附带有 68 GB 的多模态遥测数据（metric，trace，log）。LLM 需要接收给定的 failure case 和对应的遥测数据，加以推理，然后识别出故障根因。实验表明，即使是表现最好的 LLM，也只能解决 11.34% 的 failure case。 背景 对于在线系统的软件维护和debug是非常困难的，虽然有大量工作在致力于通过多模态遥测数据来定位故障根因（RCA），但是这个任务仍然具有较大的挑战性，因为线上软件系统有着难以估计的复杂性。 LLM 是否能胜任 RCA 工作呢？文章为此提出了一个 benchmark 数据集：OpenRCA， 包含 335 个真实场景的 failure case，这些 failure case 来自 3 个企业系统，并附带有 68 GB 的多模态遥测数据（metric，trace，log）。如上图所示，对于每个 failure case 以及一个对应的自然语言的 query， LLM 需要分析大佬的多模态遥测数据，理解系统之间的内在关联，并推理出可能的故障根因。 OpenRCA 特点 OpenRCA 包含了大佬真实场景的 failure case，有如下特点： 真实场景 目标驱动的任务设计（不再是简单定位一个故障组件，而是通过自然语言表达任务） 多模态异构遥测数据 完整的 LLM 评估 支持新标签和新遥测数据的集成 问题建模 故障根因有三个元素：根因组件（originating component）、开始时间（start time）、故障原因（failure reason）。每个人的目标可能不一样，所以需要对上述元素进行组合得到目标。 OpenRCA 定义了7个目标，是3个元素的组合（\\(C_3^1+C_3^2+C_3^3\\)， LLM 的输出应该是7种目标中的一种 1234567891011&quot;task_7&quot;: &#123; &quot;input&quot;: [ &quot;time range: &#123;time_period&#125;&quot;, &quot;number of failures: &#123;num&#125;&quot; ], &quot;output&quot;: [ &quot;root cause component: &#123;component&#125;&quot;, &quot;root cause occurrence time: &#123;datetime&#125;&quot;, &quot;root cause reason: &#123;reason&#125;&quot; ]&#125; 评估：对于每个 failure case，如果输出的内容符合实际，则加一分，否则不得分。这里要避免文本表达差异而导致的评估错误，prompt 中预先提供了所有可能的故障原因和原始组件。最终计算 \\(accuracy\\) ，是所有分数的均值。 12345678&#123; &quot;1&quot;: &#123; &quot;root cause occurrence datetime&quot;: (A time in ’%Y-%m-%d %H:%M:%S’ format), &quot;root cause component&quot;: (A component selected from the given’ possible root cause component’), &quot;root cause reason&quot;: (A reason selected from the given’ possible root cause reason’), &#125;,...&#125; 数据集构成 OpenRCA 使用的数据集均来自历年的 AIOps 大赛，由于脏数据较多、某些故障标签缺乏（failure reason）等问题，首先采用四步处理方式预处理： System Selection: 历年数据集中有些系统因为数据和标签不完备被淘汰，选用满足要求的3个系统 Data Balancing: 系统之间数据规模差距太大，对大数据量系统进行下采样 Data Calibration: 规范命名以及人工筛选 failure case Query Synthesis: 3个元素组合而成7个目标 最终数据集组成如下： RCA-Agent 为了解决 OpenRCA 中的任务，首先要面对两个挑战 第一个挑战是如何处理超大规模的遥测数据 直观的解决方法是将所有遥测数据整合为一个 chunk，但是低效且开销巨大的；另一个方法是采样一个遥测数据的子集，但是有丢失关键信息的风险 第二个挑战是遥测数据不是自然语言， LLM 可能无法有效处理： 可选的处理方法是先对所有遥测数据进行代码处理，然后提取关键信息到 LLM 针对上述挑战，文章也提出了一个多智能体的解决方案（RCA-Agent），RCA-Agent 包含两个 LLM 智能体（Controller 和 Executor） Controller：负责整个流程的调度，指导 LLM 按照 anomaly detection -&gt; fault identification -&gt; root cause localization 进行诊断；指导 LLM 按照 metric -&gt; trace -&gt; log 的顺序进行分析 Executor：在 Controller 的指导下写 Python 代码、执行代码、返回结果给 Controller。由两部分组成 code generator: 生成 Python 代码 code executor: 有个 Python kernel 负责执行 RCA-Agent 工作流： Controller 指示 Executor 加载遥测数据（Executor 自己生成并执行代码） Executor 返回结果给 Controller Controller 分析决策并决定下一个动作 Controller 和 Executor 不断交互直到最终结果给出 实验 之前提到不可能把所有的遥测数据都输入到 LLM，所以只能用采样来减轻负担，比如只取用每分钟的第一条记录，此外，进一步引入了两个 KPIs 采样方式作为 RCA-Agent 的对比： Oracle Sampling：工程师选出最有价值的 KPIs Balanced Sampling：随机采样与 Oracle Sampling 同等数量的 KPIs 文章比较了现有的开源模型在 OpenRCA 上的表现 ## Prompt 这里附上 RCA-Agent 细节和主要 Prompt Controller System Prompt 12345678910111213141516171819202122232425262728293031323334353637You are the Administrator of a DevOps Assistant system for failurediagnosis. To solve each given issue, you should iteratively instructan Executor to write and execute Python code for data analysis ontelemetry files of target system. By analyzing the execution results,you should approximate the answer step-by-step.There is some domain knowledge for you:&#123;BACKGROUND KNOWLEDGE OF SYSTEM&#125;## RULES OF FAILURE DIAGNOSIS:What you SHOULD do:1. **Follow the workflow of ‘preprocess -&gt; anomaly detection -&gt; faultidentification -&gt; root cause localization‘ for failure diagnosis.**...What you SHOULD NOT do:1. DO NOT include any programming language in your response....The issue you are going to solve is:&#123;PROBLEM TO SOLVE&#125;Solve the issue step-by-step. In each step, your response should followthe JSON format below:&#123; &quot;analysis&quot;: (Your analysis of the code execution result from Executor in the last step, with detailed reasoning of ’what have been done’ and ’what can be derived’. Respond ’None’ if it is the first step .), &quot;completed&quot;: (&quot;True&quot; if you believe the issue is resolved, and an answer can be derived in the ’instruction’ field. Otherwise &quot;False &quot;), &quot;instruction&quot;: (Your instruction for the Executor to perform via code execution in the next step. Do not involve complex multi-step instruction. Keep your instruction atomic, with clear request of ’ what to do’ and ’how to do’. Respond a summary by yourself if you believe the issue is resolved.)&#125;Let’s begin. Executor System Prompt 1234567891011121314151617You are a DevOps assistant for writing Python code to answer DevOpsquestions. For each question, you need to write Python code to solveit by retrieving and processing telemetry data of the target system.Your generated Python code will be automatically submitted to aIPython Kernel. The execution result output in IPython Kernel will beused as the answer to the question.## RULES OF PYTHON CODE WRITING:1. Reuse variables as much as possible for execution efficiency since theIPython Kernel is stateful, i.e., variables define in previous stepscan be used in subsequent steps....There is some domain knowledge for you:&#123;BACKGROUND KNOWLEDGE OF SYSTEM&#125;Your response should follow the Python block format below:‘‘‘python(YOUR CODE HERE)‘‘‘ Summary Prompt。Controller 认为任务已经完成，则这个 Prompt将会发给 Controller 1234567891011Now, you have decided to finish your reasoning process. You should nowprovide the final answer to the issue. The candidates of possibleroot cause components and reasons are provided to you. The root causecomponents and reasons must be selected from the provided candidates.&#123;BACKGROUND KNOWLEDGE OF SYSTEM&#125;Recall the issue is: &#123;PROBLEM TO SOLVE&#125;Please first review your previous reasoning process to infer an exactanswer of the issue. Then, summarize your final answer of the rootcauses using the following JSON format at the end of your response:&#123;OPENRCA ANSWER FORMAT&#125;","tags":[{"name":"微服务","slug":"微服务","permalink":"https://shuaiyuxie.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"根因定位","slug":"根因定位","permalink":"https://shuaiyuxie.github.io/tags/%E6%A0%B9%E5%9B%A0%E5%AE%9A%E4%BD%8D/"},{"name":"大模型","slug":"大模型","permalink":"https://shuaiyuxie.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"2025","slug":"2025","permalink":"https://shuaiyuxie.github.io/tags/2025/"},{"name":"多模态","slug":"多模态","permalink":"https://shuaiyuxie.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"},{"name":"benchmark","slug":"benchmark","permalink":"https://shuaiyuxie.github.io/tags/benchmark/"},{"name":"ICLR","slug":"ICLR","permalink":"https://shuaiyuxie.github.io/tags/ICLR/"}]},{"title":"[FSE 2025] Cross-System Categorization of Abnormal Traces in Microservice-Based Systems via Meta-Learning","date":"2025-05-07T02:46:00.000Z","path":"2025/05/07/TraFaultDia/","text":"题目：Cross-System Categorization of Abnormal Traces in Microservice-Based Systems via Meta-Learning FSE 2025 作者：赫尔辛基大学 摘要 微服务系统（MSS）可能因为复杂性和动态性发生可靠性问题。虽然现有的 AIOps 方法能够通过异常 traces 来定位根因服务，但是仍需要人力来进一步确定根因的故障类型。因此，文章提出了一种故障诊断框架 TraFaultDia，将故障根因和故障类型绑定在一起，给出结果。 文章将故障诊断视作一个分类任务。文章引入了元学习（meta-learning），即每次在有限样本和有限标签组合下进行训练，然后在测试时用极少的有标签样本微调，以应对新故障类型和新系统。文章在两个公开数据集上进行评估，无论是在新故障类型还是新系统上都取得了非常高的性能。 背景 现有的 AIOps 算法大多是定位到根因服务，没有进一步给出故障类型。特别是对于有些根因不是微服务的故障，比如机器资源不足、虚拟环境资源配置错误等，简单地定位根因服务没有意义。此外，大型 MSS 的 trace 数据量巨大，人工去分析故障类型显得不切实际。 因此，文章决定将根因服务和故障类型一起给出，整体规范如表所示，文章定义了一个概念： failure category: pod associated with a fault type 比如表格中的 (F1-F30， B1-B32) 都是 failure category。注意，有些 failure category 是没有对应的 pod 的。 总之，文章把问题简化为了一个多分类问题，但是我有点疑问，对于从未出现过的根因和故障类型的组合，应该怎么办？ 挑战 文章总结了进行故障诊断的三个挑战： MSS 异质性：每个 MSS 在业务逻辑和服务组成上有显著不同，比如 TrainTicket 有 45 个微服务，OnlineBoutique 有 12 个微服务，很难设计一个统一的、适用于所有系统的故障诊断方法 MSS 高维度、多模态 trace 数据：现在很多系统的trace与日志是关联在一起的，包含时序数据、文本数据以及ID 不同故障类型的trace数据量不均衡：如下图所示，以 TrainTicket 为例，有些故障类型只有26条数据，而有的却有 2546 条 方法 TraFaultDia 的框架非常清晰，分为两个部分： AttenAE：这一块用无监督的方式训练了一个 trace 的编码器，结构是常见的自编码器。这一块用于对 trace 进行编码得到特征 TEMAML：这一块对 trace 特征进行故障诊断，backbone 是一个 transformer-encoder 网络，采用了 meta-learning 的机制进行学习 AttenAE 有大量工作设计了专门针对 trace 的表征方法，TraFaultDia 的表征方式也有其独到的地方，由于某些系统 trace 和 log 是关联在一起的，所以需要分别进行表征，具体流程如下图所示： 针对 trace：trace 包含的信息大体分为： time-based：比如每个span的延时，文章直接把每个span的延时拼接为一个数组 \\(V_{numeric}\\) identity：对于不同的 spanID，这里采用了分层统计的方法，比如上面 Fig.2 的 Span A, Span B, Span C, Span D, 和 Span E 的 spanID 可以重写为：\\(V_{span_id}\\)=[1.0, 1.1, 1.2, 2.0, 3.0]，共同的前缀表示这些 span 属于同一个父级操作或服务 textual：如何表示一个trace经过了哪些操作呢？这里将所有的 span 的 service operation 进行编码得到特征然后平均池化得到 \\(V_{operation}\\)。这里的具体操作分为：①预处理 ② tokenization（用 WordPiece 分割成 subwords） ③ BERT 提取语义信息 最后拼接三个特征得到trace级别的表征 \\(V_{span} = Concat(V_{numeric}, V_{span\\_id}, V_{operation})\\) 针对 log：因为系统不断变化，日志模板也会变化，所以文章认为通过日志模板得到特征不可靠，应该关注语义信息，编码方式还是 trace 表征方法中textual的三步，得到 \\(V_{log}\\) 现在已经得到了初步的表征 \\(V_{span}\\) 和 \\(V_{log}\\)，接下来就是自编码，文章在此处引入了多头注意力机制： 所以，对于一个 trace 集合 \\(Tr = \\{Tr_1, Tr_2,...,Tr_n\\}\\)，可以通过上述方法得到表征集合 \\(Z=\\{Z_1,Z_2,...,Z_n\\}\\)，然后再解码得到还原后的特征 \\(\\hat{V}_{span}\\) 和 \\(\\hat{V}_{log}\\)，Loss是与原特征的L2范数： TEMAML TEMAML 的骨架是 transformer-encoder（TE），输入是 trace 表征集合 \\(Z\\)，输出是 failure category。整个过程分为两部分： Meta-training：该阶段旨在训练 TE 找到能够快速适应任何 MSS 分类任务的鲁棒参数。首先定义一个概念 meta-training tasks，表示为 \\(T=(S, Q)\\)。其中每一个 meta-training task， \\(T_i=(S_i, Q_i)\\) 都是一个多分类任务，\\(S_i\\) 和 \\(Q_i\\) 分别表示第 \\(i\\) 个任务的 support set 和 query set。注意，support set 和 query set 均来自训练集 support set \\(S_i=\\{(z_{ij}^{spt}, y_{ij}^{spt})\\}_{j=1}^{N\\times K}\\) 遵循 N-way K-shot 的初始化方法，即有 N 个独特的标签（failure category），每个标签下有 K 个样本。\\((z_{ij}^{spt}, y_{ij}^{spt})\\) 表示 trace表征集合和对应的failure category。注意，meta-training tasks 中每个任务虽然都是 N 个标签，但是 N 个标签的组合可能不一样，比如总共有 20 个 failure categories，N 为 5，则每个任务的标签都是从 20 个中选 5 个 query set \\(Q_i=\\{(z_{ig}^{qry}, y_{ig}^{qry})\\}_{g=1}^{N\\times K}\\) 有 N 个独特的标签（failure category），每个标签下有 M 个样本，M &gt; K，i.e., (\\(|Q_i|&gt;|S_i|\\)) 训练过程有两个循环：① inner loop 和 ② outer loop，定义基础模型为 \\(f\\) inner loop: 在 \\(S_i\\) 上操作，负责对每个任务进行分别学习，并更新模型 \\(f\\) 参数 \\(\\theta\\) 针对任务 \\(T_i\\)，学习率为 \\(\\alpha\\)，参数更新如下：\\(\\theta_i{&#39;}=\\theta-\\alpha \\nabla_{\\theta}\\mathcal{L}_{T_i}(f_{\\theta}(S_i))\\) outer loop: 在 \\(Q_i\\) 上操作，对于所有任务进行优化，并更新模型 \\(f\\) 在所有任务上进行优化：\\(\\min\\limits_{\\theta} \\mathcal{L}(\\theta) = \\sum_{T_i \\in T} \\mathcal{L}_{T_i}(f_{\\theta&#39;_i}(Q_i))\\) Meta-testing：该阶段目的是适应任意 MSS 的新的多分类任务。对于某个特定的测试任务 \\(T_{ts}=(S_{ts}, Q_{ts})\\)，在 \\(S_{ts}\\) 上进行微调，然后用 \\(Q_{ts}\\) 进行测试 实验 实验采用的数据集来自 TrainTicket 和 OnlineBoutique，配置如下： TrainTicket：20 基础故障类型 + 10 新故障类型 OnlineBoutique：22 基础故障类型 + 10 新故障类型 训练配置： meta-training：4 meta-training tasks (基础故障类型)；5-way 5-shot；query set M=15 meta-testing：50 meta-testing tasks (新故障类型)；5-way 10-shot；query set M=15 文章设计了四个实验来证明 TraFaultDia 在不同场景下的性能： E1 (TrainTicket → TrainTicket) 在 TrainTicket 的 4 meta-training tasks 训练，在 TrainTicket 的 50 meta-testing tasks 测试 E2 (OnlineBoutique → OnlineBoutique) 在 OnlineBoutique 的 4 meta-training tasks 训练，在 OnlineBoutique 的 50 meta-testing tasks 测试 E3 (OnlineBoutique → TrainTicket) 在 OnlineBoutique 的 4 meta-training tasks 训练，在 TrainTicket 的 50 meta-testing tasks 测试 E4 (TrainTicket → OnlineBoutique) 在 TrainTicket 的 4 meta-training tasks 训练，在 OnlineBoutique 的 50 meta-testing tasks 测试 下面是实验结果","tags":[{"name":"微服务","slug":"微服务","permalink":"https://shuaiyuxie.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"trace","slug":"trace","permalink":"https://shuaiyuxie.github.io/tags/trace/"},{"name":"根因定位","slug":"根因定位","permalink":"https://shuaiyuxie.github.io/tags/%E6%A0%B9%E5%9B%A0%E5%AE%9A%E4%BD%8D/"},{"name":"FSE","slug":"FSE","permalink":"https://shuaiyuxie.github.io/tags/FSE/"},{"name":"2025","slug":"2025","permalink":"https://shuaiyuxie.github.io/tags/2025/"}]},{"title":"[ISSRE 2025] Self-Evolutionary Group-wise Log Parsing Based on Large Language Model","date":"2025-04-29T07:09:00.000Z","path":"2025/04/29/selfLog/","text":"题目：Self-Evolutionary Group-wise Log Parsing Based on Large Language Model ISSRE 2025 作者：中科大杭州高等研究院，清华大学 摘要 日志解析是一种将半结构化日志转化为结构化模板的技术，它是各种日志分析任务（比如异常检测、日志理解）的前提。 现有的日志解析方法大多基于领域专家制定的启发式规则，这些规则在系统发生变更时就无法适用了。因此，不少研究采用大模型来进行系统无关的日志解析，但仍然存在两个显著问题： 大模型需要在 Prompt 中加入人工标注的日志模板 大模型的日志解析效率太低 因此，文章提出一种自演化的日志解析方法 SelfLog，将相似的历史解析的模板作为 Prompt 中的提示词，以实现自我演化和零标注。此外，还引入一种基于N-Gram的日志分组器与日志匹配器，按组处理和解析日志，通过减少大模型调用次数来提升效率。 背景 日志解析 日志解析会将每条日志分割为常量部分和变量部分。常量部分也称为模板。如图4所示，在有源码的前提下，我们能够很轻松区分这两个部分。然而大多数时候大量第三方依赖库的源码是不可见的， 所以开发了许多数据驱动的日志解析技术，这些技术分为有监督和无监督两类： 无监督：通过启发式规则和频率统计来提取模板。缺陷是制定规则需要领域经验，且对于新log数据集要重新制定规则 有监督：通过人工标注的 &lt;log, template&gt; 键值对来训练，缺陷是对训练数据的分布敏感且在新log上表现较差 由于日志本质上是程序员写的语句，包含了大量语义信息。大模型技术擅长于理解语句，并且有很强的 zero-shot 推理能力，所以现有研究开始尝试用大模型进行日志解析，但仍然存在两个显著问题： 大模型需要在 Prompt 中加入人工标注的日志模板案例。这个案例的质量非常重要，随着系统更新，需要人工重新标注案例 大模型的日志解析效率太低。现有的 LLM 日志解析方法每秒只能处理不到 15000 条日志，如果低于日志产生速度，则非常危险。 所以现在迫切需要一种高效率、高准确率的日志解析方法 SelfLog 方法 文章提出了一个基于大语言模型自演化日志解析工具 SelfLog，架构图如上所示，整体分为四个部分： N-Gram-based Grouper：这个部分先对日志进行聚类和分组，并提取常量部分，以组为单位让大模型进行解析，减少大模型的调用次数 Log Hitter：这个部分会检查 Grouper 的常量部分是否有现有模板与之匹配，如果是已知的，则无需调用后续步骤 LLM-based Log Parser：以组为单位进行日志解析，输出组的模板 Tree-basd Merger：修正错误的日志和模板 预处理 预处理部分主要关注几个部分： - 将日志中的时间、级别去掉，重点关注日志的内容 - 设置分隔符——满足“[A-Za-z0-9*]+”正则表达式的视为token，其他都被视为分隔符（比如_, |） - 移除日志中的纯数字 - 移除日志中的超低频token（可能是前缀或者后缀）——3个或者少于3个 预处理后，每个日志将会转变为一个 token 列表 N-Gram-based Grouper 这一个模块的目标是：识别并移除 token 列表中的 variable，将剩余的 token 用作分组 上图是这一块的伪代码： TX = get_token_list(X) 输入：预处理后的 token list position = get_2gram_constant_index(TX) 这一步是输出权重最大的长度为两个词的 token 的索引。假设常量部分一般都是高频词，因为它会稳定出现在衍生的日志中，所以高频词会给较高的权重。 variable_list_right = PILAR_gram(TX, position) 从 position 开始向右移动，每个 token 的分数计算为 （与邻居共同出现的次数 / 邻居出现的次数），然后通过3-sigma 判断分数是否异常，如果低于阈值，则判断为 variable variable_list_right = PILAR_gram(TX, position) 与 3 同理 经过上述伪代码，token 列表中将只剩下常量 token，然后依据常量 token 列表进行分组，组的 key 值即为常量 token 列表 值得注意的是，即使这一块没有识别出所有的 variable，也可以让后续的 LLM-based Log Parser 和 Tree-basd Merger 来修正 Log Hitter 这个模块的目标是：检查不同组的 key 是否在历史中出现过，如果命中，则直接返回模板；如果没命中，则将这个组中三个编辑距离最大的logs作为 LLM-based Log Parser 的输入。 Log Hitter 比较简单，维护了一个字典，键是 token 列表，值是模板。LLM-based Log Parser 的输出也会更新到字典中 LLM-based Log Parser 这个模块的目标是：对三个同组的 logs 提取模板 上面是 prompt 模板，用的大模型是 GPT 3.5，输出固定为 分析过程 和 模板 需要注意的是 prompt 中的蓝色板块，这一块是实现 self-evolution 的关键，核心思想是： &gt; 建立一个 Prompt Database 记录日志和模板，当日志需要解析时，采用类似 RAG 的技术检索到最相似的历史日志和模板作为 example，放到 prompt 中去 Tree-based Merger 事实上，上述过程仍然还会有变量被遗漏，如下图所示 最开始的时候，只有 user 为 cyrus 的日志，所以会把 cyrus 视为常量，但是随着越来越多 user 的出现，cyrus 应该被划分为变量，所以需要一种后处理机制 如上图所示，SelfLog 维护了一棵树，这棵树是实时更新的，然后执行合并操作，这一块感觉讲的不是很清楚，也不知道是人工合并还是自动化合并 实验 主实验是在 LogPAI 上进行性能对比 参数和消融实验就不放了 效率实验是在不同日志产生速度和解析速度上的对比","tags":[{"name":"大模型","slug":"大模型","permalink":"https://shuaiyuxie.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"日志","slug":"日志","permalink":"https://shuaiyuxie.github.io/tags/%E6%97%A5%E5%BF%97/"},{"name":"ISSRE","slug":"ISSRE","permalink":"https://shuaiyuxie.github.io/tags/ISSRE/"},{"name":"日志解析","slug":"日志解析","permalink":"https://shuaiyuxie.github.io/tags/%E6%97%A5%E5%BF%97%E8%A7%A3%E6%9E%90/"},{"name":"2025","slug":"2025","permalink":"https://shuaiyuxie.github.io/tags/2025/"}]},{"title":"[FSE 2025] L4: Diagnosing Large-scale LLM Training Failures via Automated Log Analysis","date":"2025-04-28T05:37:00.000Z","path":"2025/04/28/l4/","text":"题目：L4: Diagnosing Large-scale LLM Training Failures via Automated Log Analysis FSE 2025 作者：香港中文大学 摘要 训练个性化的大语言模型（LLM）需要大量的计算资源和训练时间。这个过程中，故障（failure）是不可避免的，而故障的出现使得LLM的训练浪费了大量的资源和时间。此外，在 LLM 训练中诊断故障也是一个费时费力的任务，因为 LLM 的训练通常设计多个计算节点： Node-level Complexity: 在单个节点上训练的 AI 模型，通常包含 AI accelerator（GPUs 或 NPUs）、AI toolkit（CUDA）、AI framework（Pytorch）以及AI algorithm。故障可能存在于上述任意一个地方。 CLuster-level Complexity: LLM 的训练通常涉及上千个 AI 节点，这些节点之间有复杂的通信范式，这使得发生故障时很难通过自动化的方式定位到对应的故障 AI 节点 这篇文章首先进行了大量的实证分析，得出以下结论： 故障时间：大多数（74.1%）的故障发生在模型迭代训练时，这个阶段发生故障会导致大量的训练时间和资源的浪费 故障根因：随然故障原因多样化，但主要集中在 hardware 和 user-side faults。 诊断方法：日志在故障诊断中发挥重要作用，但是89.9%的案例仍然需要人工日志分析来进行故障诊断。并且日志量太大（每天产生 TBs），只有非常小一部分的日志是有用的 因此，文章提出了一种诊断 LLM 训练的故障诊断方法：L4，目的在于自动化识别故障相关的日志（failure-indicating logs），此外，L4 还会提供 failure-indicating nodes、failure-indicating stages、failure-indicating events 以及 failure-indicating iterations 等重要信息来辅助 SREs 进行故障诊断。 实证分析 文章分析了某平台上一年的428份故障报告，从以下三个方面进行分析： RQ1：LLM 训练中的故障现象 RQ2：LLM 训练中的故障根因 RQ3：LLM 训练故障诊断中常用的数据源 RQ1：故障现象 LLM 训练中的故障现象分为四大类：① launching failure，② training crash，③ abnormal behavior，④ others 以下是四种故障现象的分布： 对于 launching failure（21.3%），这类现象通常发生在迭代训练开始前，原因一般是配置与版本问题，比如 GPU 驱动与 CUDA 版本不匹配，模型并行化配置错误等。 对于 training crash（57.5%），这类现象发生在迭代训练时，起因一般是硬件故障（GPU、network），这种故障影响很大，会浪费大量训练时间和计算资源，即使有 checkpoint 这样的状态保存机制，时间的浪费也是不可忽视的 对于 abnormal behaviors（16.6%），这种现象有点类似于性能降级，比如某个epoch花费了两倍的时间，训练突然停滞 对于 others（4.7%），这类故障一般是基础设施类的，比如平台和存储，比例比较少 Finding 1：大部分故障（74.1%）发生在迭代训练时，会导致大量计算资源和训练时间的浪费。 RQ2：故障根因 LLM 训练中的故障根因分为四大类：① hardware fault，② user fault，③ platform fault，④ framework fault RQ2-1 hardware fault Hardware Fault 又可细分为 Network Fault：最常见，本质上是因为有太多 AI node 在交互协同，网络问题极容易导致训练的失败 Accelerator Fault：Accelerator 就是GPU、TPU那些计算资源，与普通深度学习任务类似，单个 Accelerator 也会有内存故障等 Node Fault：Node 是资源分配的单元，比如虚拟机。也会遭遇断电、磁盘问题等故障 Storage Fault：训练涉及的数据集、模型、checkpoints等有几百GB，用户一般会存储到远程存储库中，在训练时下载，因此可能会出现访问问题 Finding 2：LLM训练需要大量计算资源，极易受 Hardware Fault 影响。其中，Network Fault 和 Accelerator Fault 是最常见的 RQ2-2 user fault user fault 又可细分为 Configuration Error Program/Script Bug Software Incompatibility Misoperation Finding 3：User faults 是第二大故障根因，源于用户的误操作、脚本bug等 RQ2-3 Framework Fault and Platform Fault 这两类故障发生的极少，Framework Fault 一般是 PyTorch 那些深度学习框架中的故障。Paltform Fault 是训练平台的问题，包括资源管理不恰当等。这两类故障极难诊断故障，需要较深的领域经验 Finding 4：Framework Fault 和 Paltform Fault 虽然发生极少，但诊断起来相当困难 RQ3：故障诊断数据源 现在诊断 LLM 训练故障还是人工分析居多，文章中提到： 诊断 LLM 训练故障平均需要 34.7 小时，而 41.9% 的故障需要 24 小时以上的诊断时间 这就迫切需要自动化的故障诊断方法来减少人工分析成本。文章首先对428份故障案例进行分类，分为以下三种： Log-only diagnosable Non-log diagnosable Hybrid diagnosable 然后分别分析每个案例的 training log 来进行故障诊断，以下是分类结果 同时发现每个故障案例在故障时间段平均有 16.92GB 的 training log Finding 5：training log 能解决 89.9% 的故障，但是日志量太大，需要自动化分析手段来减少人力成本 现有方法 现有方法大致通过 logging level，event frequency，error semantic 来提取异常log，即 failure-indicating logs。文章通过统计发现这些方式都有一定的局限性： logging level：现有方法有许多是根据日志级别来进行筛选的。文章首先统计了 failure-indicating logs 中不同级别的日志分布，如 Fig.4 (a) 所示，虽然大部分是 Error-level，但是仍有大量其他级别的 logs 是故障相关的。此外，并不是所有 Error-level 的 logs 是与当前故障相关的，比如有些 Error logs 是无法写入 checkpoints，但一些故障容忍策略可能会采用重写措施，只要重写成功，那么对当前训练是没有影响的 Event Frequency：有些方法是根据日志的频率来筛选故障相关日志，比如低频日志更有可能是异常的。如 Fig.4 (b) 所示，仍然有大量 failure-indicating logs 的频率并不低。此外，基于频率的筛选可能更适合微服务，不适用于 LLM 训练，因为微服务是无状态的，日志的频率分布可能并不会随时间分布发生较大改变，而 LLM 训练是顺序的、分阶段的，有些日志只会出现在特定阶段，这些日志的频率很低，但与故障无关。 Error Semantic：也有些方法通过深度学习来提取日志的错误语义信息来识别failure-indicating logs，但是这种方法是不稳定的，因为即使是成功训练的job也包含有error semantic Finding 6：现有的 failure-indicating logs 提取方法基于level, frequency 和 semantic，但都不适用于 LLM 训练的故障诊断 L4 方法 文章提出了自动化的面向 LLM 训练的故障诊断框架：L4 1. Log Preprocessing 这一块与之前的方法相同，都是用 Drain 提取日志的模板，将 log sequence 变成 event sequence 2. Cross-job Filtering 这一块动机很直观，因为用户在提交作业到平台进行大规模训练（large-scale nodes）时，通常在小规模的节点上已经测试通过了。所以这一块会将成功执行的日志（Normal logs）与在大规模节点上的失败日志（failed logs）进行比对，删除 failed logs 中噪声 events。 具体做法为：将 Normal logs 进行解析得到一个 normal event pool：\\(N=\\{e_{n1}, e_{n2}, ...\\}\\)，然后将这些 events 按照时间顺序排列，并逐个移除 failed logs 中对应的events，这些events都是正常的，failed logs 剩下的 events 都是极大概率是故障相关的。这里有个问题，不同规模的训练节点会不会导致日志模式发生变化？ 这个模块的前提是有小规模训练成功的日志，当这个前提不满足时，则无法删减 failed logs的噪声events，就直接将 failed logs 解析后进行后续步骤 3. Spatial Pattern Comparison 这个步骤主要是为了定位 failure-indicating nodes 和 failure-indicating logs。核心思想是：由于负载均衡，正常情况下所有的 AI nodes 的日志几乎是一样的。所以可以很轻松找到 failure-indicating nodes 首先将每个 node 的 event sequence 按照发生次数进行特征构建，得到 event vector：\\(V=[c_1,c_2,...c_n]\\)，\\(c_i\\) 代表 \\(i\\)-th event 的次数。然后对所有 node 的 event vector 采用 Isolation Forest 进行异常检测。 由于 Isolation Forest 可以记录特征参与分割的次数，从而能够判断特征（event）的重要性，所以进一步找到 failure-indicating logs 4. Temporal Pattern Comparison 这个步骤主要是为了定位 failure-indicating stage 和 failure-indicating iteration，即故障发生的时间。 stage 的确定很简单，L4 好像直接使用的是日志中自带的规则，能直观知道 event 在哪个 stage 文章还要定位到故障的 iteration。首先文章有个前提假设，即 event sequence 在迭代时基本遵循一定的模式，如果某个 iteration 发生了明显偏移，则视为异常 具体做法为：首先将 iterations 按照 10 个一组进行滑动窗口划分。采用 Dynamic time warping（DTW）计算不同窗口间的距离，然后通过 3-sigma 方法对距离进行异常检测，就能找到异常的 iteration 了 最后，就能将故障相关的 logs、events、nodes、stages、iterations 交给 SREs 了。 实验设计 实验主要是对 failure-indicating logs 和 failure-indicating nodes 的定位进行了准确率评估，然后给了几个成功案例","tags":[{"name":"FSE","slug":"FSE","permalink":"https://shuaiyuxie.github.io/tags/FSE/"},{"name":"大模型","slug":"大模型","permalink":"https://shuaiyuxie.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"日志","slug":"日志","permalink":"https://shuaiyuxie.github.io/tags/%E6%97%A5%E5%BF%97/"},{"name":"故障诊断","slug":"故障诊断","permalink":"https://shuaiyuxie.github.io/tags/%E6%95%85%E9%9A%9C%E8%AF%8A%E6%96%AD/"},{"name":"2025","slug":"2025","permalink":"https://shuaiyuxie.github.io/tags/2025/"}]},{"title":"[ICPP 2023] On Optimizing Traffic Scheduling for Multi-replica Containerized Microservices","date":"2024-12-21T02:29:27.000Z","path":"2024/12/21/OptTraffic/","text":"题目：On Optimizing Traffic Scheduling for Multi-replica Containerized Microservices ICPP 2023 作者：中国科学技术大学，华为 摘要 基于容器的微服务系统部署已经变得越来越重要，为了应对高并发（high concurrency）和提升错误容忍度（fault tolerance），微服务系统通常会引入多个副本（replicas）。目前有两种类别的服务部署方法： resource-friendly：将 pod 尽可能分散部署到不同机器上，目标是平均各机器的资源使用率。（e.g., Kubernetes，Docker Swarm，OpenShift） traffic localization：将流量（traffic）交互频繁的 pod放到同一台主机上，目标是减少跨主机流量导致的性能降级。（e.g., CA-WFD，Blender，NetMARKS） 这两种方法各有利弊：① resource-friendly存在大量跨主机流量（cross-machine traffic），可能会引发性能降级；② traffic localization在多replicas情况下，由于机器资源不足，也会存在大量cross-machine traffic。 文章提出了一种网络感知的流量调度（traffic scheduling）方法 OptTraffic，首先通过轻量级监控方法估计容器之间的流量大小，然后通过算法分配容器之间的流量比例（默认应该是负载均衡），目标是减少 cross-machine traffic，并尽可能平均各主机的资源利用率。 背景 基于容器的微服务 文章首先介绍了微服务是什么，这里就不进行赘述了。然后介绍了基于容器的微服务部署方法： 首先是传统的容器部署，代表为K8s，文章说是基于resource-friendly的，简而言之，就是只考虑容器的CPU和内存需求。这里我先查阅了K8s调度器 kube-scheduler 的说明，以下是文档的内容： 调度器执行步骤如下： 找出该 Pod 的所有 可选节点，这个过程称为过滤（filtering） 按照某种方式对每一个 可选节点 评分，这个过程称为打分（scoring） 选择评分最高的 可选节点 将最终选择结果通知 API Server，这个过程称为绑定（binding） 文章的重点在打分，假设全部节点皆可部署，则有如下打分策略： SelectorSpreadPriority：将 Pod 分散到不同的节点，主要考虑同属于一个 Service、StatefulSet、Deployment的情况 InterPodAffinityPriority：遍历 weightedPodAffinityTerm 并求和，找出结果最高的节点 LeastRequestedPriority：已被消耗的资源最少的节点得分最高。如果节点上的 Pod 越多，被消耗的资源越多，则评分约低 MostRequestedPriority：已被消耗的资源最多的节点得分最高。此策略会把 Pod 尽量集中到集群中的少数节点上 RequestedToCapacityRatioPriority：按 requested / capacity 的百分比评分 BalancedResourceAllocation：资源使用均衡的节点评分高 NodePreferAvoidPodsPriority：根据节点的 annotation scheduler.alpha.kubernetes.io/preferAvoidPods 评分。可使用此 annotation （容忍度 Toleration 和 污点 Taint）标识哪些 Pod 不能够运行在同一个节点上 NodeAffinityPriority：基于 PreferredDuringSchedulingIgnoredDuringExecution 指定的 node affinity 偏好评分。参考 将容器组调度到指定的节点 TaintTolerationPriority： 根据节点上不可容忍的污点数评分 ImageLocalityPriority：有限选择已经有该 Pod 所需容器镜像的节点 ServiceSpreadingPriority：确保 Service 的所有 Pod 尽量分布在不同的节点上。 CalculateAntiAffinityPriorityMap：anti-affinty，参考将容器组调度到指定的节点 EqualPriorityMap：为每个节点指定相同的权重 这些打分策略都有一定的权重，最终的分数计算如下： 1finalScoreNode &#x3D; (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + ... 经过查阅资料，默认开启的调度代码如下，可以看出，在所有节点都有对应镜像，没有亲和性和污点干扰的情况下，k8s其实偏向于resource-friendly，即分布更加均匀，相关策略有BalancedAllocationName、LeastAllocatedName： 123456789101112131415Score: &amp;schedulerapi.PluginSet&#123; Enabled: []schedulerapi.Plugin&#123; &#123;Name: noderesources.BalancedAllocationName, Weight: 1&#125;, &#123;Name: imagelocality.Name, Weight: 1&#125;, &#123;Name: interpodaffinity.Name, Weight: 1&#125;, &#123;Name: noderesources.LeastAllocatedName, Weight: 1&#125;, &#123;Name: nodeaffinity.Name, Weight: 1&#125;, &#123;Name: nodepreferavoidpods.Name, Weight: 10000&#125;, &#x2F;&#x2F; Weight is doubled because: &#x2F;&#x2F; - This is a score coming from user preference. &#x2F;&#x2F; - It makes its signal comparable to NodeResourcesLeastAllocated. &#123;Name: podtopologyspread.Name, Weight: 2&#125;, &#123;Name: tainttoleration.Name, Weight: 1&#125;, &#125;, &#125;, 总的来说，K8s默认是resource-friendly的说法倒也没啥问题~~~ 然后文章介绍了集中式部署，即以减少cross-machine traffic为目标的traffic localization方法，这里进行了一个实证分析来论证cross-machine traffic的影响，具体实验方法为： 将经典应用 socialNetwork 部署到2台主机，并用 sockperf 模拟两个主机之间的流量干扰，测量不同QPS下不同带宽的延时的P99分布 图中显示QPS增大时，同样带宽下的P99延时明显增大，所以cross-machine traffic受带宽限制，会影响延时。但这有没有可能是 POD 资源不够导致的（这里应该给 POD 设置足够充足的资源来消除影响）？ Traffic localization 及其局限 为了解决cross-machine traffic带来的性能降级问题，许多工作提出了 traffic localization 的部署方法，有两种实现方式： 将属于同一个application的所有容器部署到相同或邻近的主机 将流量交互密切的容器对（container pairs）调度到相同或邻近的主机 traffic localization 不仅可以减少 cross-machine traffic 带来的网络延迟（由传输路径和带宽大小决定），也可以减少对网络数据包操作（包的封装和解析）带来的计算开销。 为了验证 traffic localization 对于节省网络流量和降低响应延时的作用，文章在两种部署方式上进行实验： 部署 socialNetwork 到 5 台主机上 部署 socialNetwork 到 1 台主机上（traffic localization） 图（a）显示 traffic localization 可以减少大量的cross-machine traffic。图（b）显示 traffic localization 可以大幅度减少响应延时 Limitation：traffic localization虽然能显著减少cross-machine traffic来提升性能，但是通常很难完全将一个 application 的所有微服务都部署到一台主机上，原因在于微服务还会产生多个replicas，除此之外，监控容器之间的流量也会引入较大的CPU负载，traffic localization也会造成机器资源使用不均。具体如下： （1）当 POD 分布在不同主机时。因为负载均衡，流量会均匀流向不同主机的POD，造成大量的cross-machine traffic （2）高CPU负载。对容器间流量的监控一般需要抓取数据包，对数据包的封装和解析过程会造成高昂的CPU开销。比如socialNetwork有27个容器，消耗13个CPU核，而加上监控装置（e.g., iftop）会多消耗 1.4-2 个 CPU核【istio也存在这样的问题，甚至sidecar消耗更多】 （3）不均衡的资源使用。许多pod有着相同的资源需求，比如都是CPU密集型，将他们放置在一台主机上会造成资源使用不均衡，即CPU过载，但是内存利用率低。除此之外，将所有pod放在一台主机上会使得fault tolerance降低。 方法设计 OptTraffic 的目标是最小化 cross-machine traffic。整体架构如上图所示。包含三个组件： Traffic Monitor。这个组件用轻量级的方式估计每个 container pair 之间的流量大小。做法为收集每个容器的 incoming 和 outgoing traffic，然后构建一个 traffic graph，最后通过简单的数学计算估计每个 container pair 之间的流量大小 Traffic Allocator。流量调度模块，决定 upstream container 流向 每个 downstream container 的流量比例。具体策略为本地优先（local-first），以减少跨主机流量 Container Scheduler。异步调整 container pairs 的部署，以实现机器之间的负载均衡。 Traffic Monitor Traffic Monitor 有两个目的：① 识别频繁交互（heavy-traffic）的链路 ② 为链路调度进行优先级排序 现有两种方式进行容器间流量的监控： 在容器所在POD加一个sidecar，接管流量的转发和监控（e.g., istio）。 使用网络包嗅探工具在userspace进行解析（e.g., tcpdump and iftop）。 这两种方式都会造成高昂的CPU负载。文章还特别提到了在kernel space 工作的 eBPF，可以减少监控负载，但仍需要分析网络包，并且实现复杂。所以作者想用数学方法来减少监控带来的负载。 eBPF 避免用户态与内核态切换：传统上，像tcpdump这样的工具是在用户空间工作的，它们需要将网络数据包从内核空间复制到用户空间进行分析。这个过程涉及上下文切换和内存拷贝，带来了额外的CPU开销。而eBPF程序则可以在内核空间内部处理数据包，无需这种昂贵的数据传输。 Step 1: 监控容器流量。 如 fig 6 所示，一个upstream container 可以与多个 downstream container进行交互，OptTraffic首先使用 Prometheus 监控每个container接收和发送的traffic Prometheus 只是一个收集工具，数据源来自于 操作系统记录的每个容器接收和发送的包的大小（这些数据已经保存在主机上，所以只需要周期性地获取就行，几乎没有监控成本），数据保存在 proc 文件系统中，经过查资料，应该用的是这两个指标： container_network_receive_bytes_total container_network_transmit_bytes_total Step 2: 计算链路流量。链路流量（Link Traffic）指的是特定的两个容器之间的流量大小，OptTraffic 直接使用数学方法计算得到，减少监控的负载（注意：上一步只能监控到每个容器的接收和发送的traffic大小，不能知道与特定容器之间的traffic大小）。计算步骤如下，从入度为 1 的节点 \\(p\\) （根节点）开始，先将与 \\(p\\) 相关的两条边赋值（即&lt;\\(p\\),\\(q\\)&gt;和&lt;\\(q\\),\\(p\\)&gt;），这两条边的值完全等于 \\(p\\) 的 send 和 receive 的 traffic 。然后将 \\(p\\) 相关的流量从 \\(q\\) 中减去，从图中删去 \\(p\\)，重复如上过程，直到所有节点计算完毕。 构造微服务系统的 DAG。（可以通过通过配置文件、官方文档或者其他监控手段得到） 基于 DAG 构造 traffic graph。traffic graph有着和DAG相同的构造，唯一的不同是，traffic graph是双向边，代表 traffic 也可以从 downstream container 发往 upstream container （response） 基于 traffic graph 计算每个 link 的traffic大小，算法如下： GraphMerge 和 BreakCycle两个函数用于解决图中存在环的情况。如下图所示，环有两种情况： upstream containers 来自不同微服务 （Fig. 7(b)） upstream containers 来自同一个微服务的不同replicas （Fig. 7(c)） 对于 case1，BreakCycle 函数首先选择环中 incoming 和 outgoing traffic 最少的 container 进行讨论，然后通过网络包嗅探工具监控这个 container 每条边 send 和 receive 的traffic。然后在相关 containers 中移除这个 container 的traffic，最后删除这个container，这样就可以破坏环。 对于case2，GraphMerge 函数将属于同一个微服务的 replicas 进行合并，至于每个 replica 分得的 traffic，可以通过流量分配策略进行估算 负载分析：从设计上看，OptTrace 确实轻量级，因为不需要监控每条 link 的 traffic，而每个 vertex 的 traffic 本身就被os记录下来了。即使有环，也只需要对一部分容器进行网络包嗅探分析，破环即可。算法复杂度为O(N)，N为节点数量。 Traffic Allocation 这个模块是核心，决定了 upstream 微服务向某个 downstream 微服务的不同 replicas 发送 traffic 的比例。downstream 微服务的replicas可能部署在不同主机上，所以策略不再是简单的负载均衡 如上图所示，有2个 upstream containers 和 3个downstream containers，并部署在2台主机上。如果是按照负载均衡的原则，假设每个upstream container发送 1 traffic，则发往每个downstream container \\(\\frac{1}{3}\\) traffic，每个downstream containers总共需要接收 \\(\\frac{2}{3}\\) 的traffic。从Fig 8 (a)中看出，有 \\(1=\\frac{1}{3}+\\frac{1}{3}+\\frac{1}{3}\\) （一半）的traffic是cross-machine的。 文章介绍了 Two-step allocation 方法来进行 traffic 分配： 第一步，采用 local-first 原则，即traffic尽可能分配到同台主机的containers。假设机器 \\(i\\) 有 \\(m_i\\) 个upstream containers 和 \\(n_i\\) 个 downstream containers，则每个upstream container需要发送给每个downstream container比例为 \\(min(\\frac{m/n}{m_i}, \\frac{1}{n_i})\\)的traffic。这里的比例很简单也很有趣，每个downstream container接收到的traffic是 \\(m_i\\times min(\\frac{m/n}{m_i}, \\frac{1}{n_i})=min(m/n, m_i/n_i)\\)，第一个是全局负载均衡的traffic大小，第二个是单机上负载均衡的traffic大小。这里的原理是：使得低于负载均衡的traffic尽可能发往本机的container，但这个traffic尽量不高于全局负载均衡的值。所以 Fig 8 (b) 中 DC3 的 traffic 大小为 \\(min(\\frac{2}{3}, 1)=\\frac{2}{3}\\) 第二步，这一步叫padding，即将第一步中剩余未分配的traffic导向其余downstream containers，这些traffic就是cross-machine traffic了。如 Fig 8 (c)所示。 看了一下源代码，流量调度用的是iptable实现的，具体指令类似如下这样 1sudo iptables -t nat -I KUBE-SVC-KVWZRLJ3RRU55EYE 1 -s 0.0.0.0&#x2F;0 -j KUBE-SEP-5EMJYGBVFF6DIW4U -m statistic --mode random --probability 0.17 以上是论文的核心，至于动态调度那一块，首先是没看懂，可能以后有时间再读读","tags":[{"name":"微服务","slug":"微服务","permalink":"https://shuaiyuxie.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"流量调度","slug":"流量调度","permalink":"https://shuaiyuxie.github.io/tags/%E6%B5%81%E9%87%8F%E8%B0%83%E5%BA%A6/"},{"name":"ICPP","slug":"ICPP","permalink":"https://shuaiyuxie.github.io/tags/ICPP/"},{"name":"2023","slug":"2023","permalink":"https://shuaiyuxie.github.io/tags/2023/"}]},{"title":"[arxiv 2024] StatuScale: Status-aware and Elastic Scaling Strategy for Microservice Applications","date":"2024-12-12T02:29:27.000Z","path":"2024/12/12/StatusScaler/","text":"题目：StatuScale: Status-aware and Elastic Scaling Strategy for Microservice Applications 来源：arxiv 2024 作者：中国科学院深圳先进技术研究院 摘要 相比于单体架构，微服务架构具有更好的弹性，可以进行微服务级别的弹性伸缩。然而，现有的弹性伸缩无法检测突发流量，突发流量可能由很多原因引起： 商店促销 特殊活动 软件故障 ... 这些突发流量通常是短暂（short-lived）且超出预期的（unexpected），会造成瞬间的性能降级。微服务必须要快速分配足够数量的资源以保证性能。 本文推出了一种状态感知的弹性伸缩控制器 StatusScale，能够感知负载的趋势，预测流量峰值，并进行水平伸缩和垂直伸缩。此外，本文提出了一种新的指标： correlation factor：评估资源使用的效率 文章在 Sock-Shop 和 Hotel-Reservation 应用上进行评估，将响应延时降低了接近10%，资源利用效率得到提高。 方法 StatuScale 根据当前负载状态选择不同的资源分配方案（垂直和水平），架构如下： Load Preprocessor是产生流量的模块 Auto Scaler是全文核心，分为： Vertical Scaling：对负载状态进行预测，制定垂直伸缩方案 Horizontal Scaling：负载状态不稳定时，进行水平伸缩 Performance Evaluator是评估模块，主要评估响应延时、SLO违背率、资源利用效率、资源消耗等 系统建模 弹性伸缩的优化目标和约束如下： \\[ \\mathop{\\sum_{m\\in M}{P_m/A_M \\times \\sum_{p\\in P}{R_p/A_p}+\\omega^t\\sum_{m\\in M}RT_m/A_M}} \\] \\[ s.t. \\quad P_m \\geq 1, \\quad R_p, RT_m \\geq 0,\\quad A_p\\geq A_M\\geq 0 \\] \\(M\\) 代表微服务集合，\\(A_M\\) 代表应用中微服务的数量（即 \\(|M|\\)），\\(P\\) 代表应用中pod集合，\\(A_p\\) 代表应用中pod的数量（即 \\(|P|\\)），\\(P_m\\) 代表微服务 \\(m\\) 的pod数，\\(R_p\\) 代表为pod \\(P\\) 垂直分配的资源配额，\\(RT_m\\) 代表微服务 \\(m\\) 的响应延时。 优化目标的前半部分是资源消耗（服务的平均pod数\\(\\times\\)pod平均资源额度），后半部分是平均延时，权重设置为 \\(\\omega^t\\)。目标是同时优化资源消耗和响应延时。 垂直伸缩 微服务的负载可能由于特殊的用户活动（e.g., 商品促销）而陡然升高，进而导致服务资源不足而性能下降。所以监控、分析和理解负载的趋势是非常重要的。 基于 LightGBM 的负载预测器 为了能高效地进行负载预测，作者没有采用较重的深度学习或者机器学习模型，而是准备选用基于集成模型的 LightGBM 来进行预测。 遗憾的是，LightGBM 面临准确性问题，在Fig. 2中，作者试图用 LightGBM 预测 Alibaba 数据集的负载，但是出现了大量负载低估的情况。这些低估会导致资源分配较少，进而导致性能下降。 Load Status Detector 文章另辟蹊径，不再直接训练和预测负载的准确值，而是判断负载是否处于 “stable” 状态。文章引入了金融分析中常用的 resistance line 和 support line 两个概念用来辅助判断。 在 Fig. 3 (a)中，作者展示了如何根据 resistance line 和 support line 进行负载状态感知。图中橙色的点是预测时的低估点。首先将x轴分为6个时间窗口，每个时间窗口有5个数据点。 首先，StatuScale使用第1个窗口的数据去生成 resistance line 和 support line 然后，去判断第2个窗口是否违背了第1个窗口的resistance line 和 support line。如果违背了，则判断状态为 “unstable”，采取特定的伸缩策略；如果没有违背，则判断状态为 “stable”，则能够继续进行负载预测。Fig. 3 (a) 中状态为 “stable” 合并第1个和第2个窗口的数据来更新resistance line 和 support line，进而判断第3个窗口超过了resistance line，所以标记第3个窗口状态为 “unstable” 第4个窗口重新生成resistance line 和 support line，重复上述过程 总的来说，resistance line 和 support line 相当于负载波动的上下边界，在边界内的负载一般都是 “stable” 的。 对于resistance line 和 support line的建模，作者摒弃了复杂的非线性函数，因为担心会导致过拟合。此外，作者又担心线性函数过于简单，所以决定采用分段线性函数（因为分段线性函数可以解决周期负载的判断问题），所以就有了 “unstable” 后进行resistance line 和 support line重置的环节。resistance line的定义如下： \\[ f(t) = kt+b+\\lambda c_v \\] \\(k\\), \\(b\\) 分别代表斜率和截距，可以通过多项式拟合数据点得到。\\(t\\) 代表时间。\\(c_v\\) 代表变异系数（\\(\\frac{\\mu}{\\sigma}\\)），用来表示样本的分散程度，也为resistance line留有了一定的容错空间。\\(\\lambda\\) 是权重超参数。 自适应PID控制器 在上节中，StatuScale 已经能判断出服务的负载状态，当负载状态为 “unstable” 时，采用 PID 控制器来维持状态的稳定，目标是使得CPU利用率稳定，以及满足SLO。 PID 控制器是广泛使用的控制器，由 1）比例 proportional、 2）积分 integral、 3）导数 derivative 组成。PID 控制器旨在根据feedback更新参数，调整输出，使得状态稳定在目标值附加，输出的分数公式如下： \\[ y(t) = k_Pe(t)+k_I\\int_{t-w}^{t}{e(\\tau)d\\tau+k_D\\frac{d}{dt}e(t)} \\] 其中，\\(e(t)\\)代表时刻t的误差（给定值-测量值）。\\(k_P\\)，\\(k_I\\)，\\(k_D\\) 分别是比例增益（proportional）、积分系数（integral）以及导数系数（derivative），分别代表当前误差，过去一段时间的误差以及预测未来的误差的权重。 PID 控制器的各项权重 \\(k_P\\)，\\(k_I\\)，\\(k_D\\) 对系统的稳定性影响很大，StatuScale 引入了一种自适应调节各项权重的 A-PID 控制器。如 Fig.6 所示，A-PID 引入了 BP 网络来调节 PID 的参数（i.e., \\(k_P\\)，\\(k_I\\)，\\(k_D\\)）。BP 网络的配置如下： 输入：输出值、目标值、误差、bias 中间层：hidden size = 5，激活函数为 tanh 输出：\\(k_P\\)，\\(k_I\\)，\\(k_D\\)，激活函数为 sigmoid 这里文章漏掉了最关键的 loss，我只能猜测 loss 的原理是，如果当前误差较大，我们希望新的参数能够更大幅度地改变，以便更快地纠正误差；反之，如果误差较小，则应谨慎调整参数，避免过度校正，所以猜测 loss 为 误差相关的函数。 此外，A-PID的 output 是如何转化为 CPU 的分配？CPU target是多少？垂直伸缩这一块并不是讲的很清楚 水平伸缩 文章认为水平伸缩比垂直伸缩更难，因为水平伸缩需要时间去创建和移除POD，并需要时间去进行负载均衡，同时不必要的水平伸缩操作会导致资源浪费。此处引用了ATOM1的结论： 低负载：垂直伸缩更有优势，因为资源分配快 高负载：水平伸缩更有优势，因为多个pod分布在多个机器上，将负载均衡了，大大降低了单个pod的压力 所以 StatuScale 会优先考虑垂直伸缩（在低负载下更有优势）；如果垂直伸缩无法满足需求，才会考虑用水平伸缩进行粗粒度调整。再用垂直伸缩进行细粒度调整。 首先，StatuScale 将会判断是否需要进行水平伸缩操作，计算当前CPU利用率 \\(C_t\\) 与目标CPU利用率 \\(CPU_{tar}\\) 之间的差距，并根据这个差距生成一个转换后的结果 \\(S_t\\)： \\[ S_t = \\begin{cases} 1-K^{(CPU_{tar}-C_t)}&amp; C_t ＜ CPU_{tar}\\\\ K^{C_t-(CPU_{tar})}-1&amp; C_t \\geq CPU_{tar} \\end{cases} \\] 当前CPU利用率 \\(C_t\\) 接近目标值 \\(CPU_{tar}\\)，\\(|S_t|\\)接近0；否则，\\(|S_t|\\)值将以指数倍数增长。StatuScale 统计一段滑动窗口内的不同时间点的 \\(S_t\\) 的和（减少突发流量的影响），并与上下阈值进行比较，以决定是否进行弹性伸缩。 但是文章并没有给出上下阈值的计算方式？ 当决定采用弹性伸缩时，给定当前副本数（\\(R_c\\)），伸缩比例（\\(\\delta\\)），伸缩的副本数定义如下： \\[ R_n = max(\\delta R_c, 1) \\] 因为水平伸缩的pod需要一段时间才能生效，所以这段时间可能会频繁触发弹性伸缩，所以 StatuScale 引入了 cooling-off 周期来减少伸缩次数（这段时间不会触发第二次伸缩，默认为5min） 接下来文章用垂直伸缩来进行细粒度资源调整，具体来说，就是通过一个衰减率来周期地减少资源配额，资源值设置如下： \\[ V(t) = Vk^{\\beta^t-1} \\] \\(0&lt;\\beta&lt;1\\) 是衰减率，\\(V\\) 是资源初始值。 &gt; 这里只有减少垂直资源分配，相当于减少水平伸缩多余的那部分资源 联合伸缩 文章的讲述顺序和方法流程是不一样的，所以最开始让我有点费解，真正的整体流程如上图所示： 首先判断是否需要水平伸缩，判断方式为上文中提到的计算一段时间的 \\(S_t\\)，并与上下阈值比较 如果需要水平伸缩，则计算\\(R_n\\)，然后垂直细粒度资源调整（计算\\(V(t)\\)） 如果不需要，则需要进行垂直伸缩判断 垂直伸缩检测需要对负载状态进行判断 如果状态为 “stable”，则用 LightGBM 预测负载 如果状态为 unstable，则使用 A-PID 控制器将资源利用率维持在稳定状态 值得注意的是，k8s的垂直伸缩应该会让容器重启（假如有10个副本，采用垂直伸缩后，相当于这10个副本都需要滚动更新），这真的会比水平伸缩快吗？ 整体算法如下图所示： 实验评估 实验配置 集群配置：1 master + 2 worker，每个节点都是 4GB 内存 和 4 CPU cores。这个配置算比较小的了 负载：文章的负载数据来自于 alibaba 的 cluster-trace-v20182，这个数据集里记录了8天内集群里机器和容器的资源使用情况，并调研了CPU负载和QPS的对应关系，如Fig.8所示，将CPU负载转化为QPS，文章使用这个作业负载作为实验的输入流量。负载的注入工具选择 Locust benchmark：选用 Sock-Shop 和 Hotel-Reservation 对比方法：选用了 GBMScaler，Showar，Hyscale GBMScaler：选用 LightGBM 进行负载预测，但原文并没有提到如何用预测的结果进行 resource scaling Showar：经典的混合伸缩方法，基于 3-\\(\\sigma\\) 准则进行垂直伸缩，每T秒预估当前CPU分配为过去一段窗口的\\(\\mu+3\\sigma\\)；基于 PID 控制器进行水平伸缩（target设置为CPU利用率，\\(k_P\\)，\\(k_I\\)，\\(k_D\\)的更新与 StatusScale一致） Hyscale：与kubernetes 默认弹性伸缩器很像，只需要指定CPU阈值，然后通过水平和垂直伸缩来达到目标 评估指标 文章主要考虑系统性能和资源消耗，使用的指标如下： response time （相同资源配额下，\\(\\int{R_t}dt\\)，\\(R_t\\)是t时刻分配的资源） SLO violation（相同资源配额下，\\(\\int{R_t}dt\\)） Accuracy of supply-demand relationships. 这个是看 resource supply 是否准确。在一段时间 \\(T\\)内，总共可分配资源为 \\(R\\)，t 时刻的资源需求为 \\(d_t\\) （\\(d_t\\) 是通过 Fig.8 拟合出来的），有点类似于误差 \\(a_U=\\frac{1}{T\\cdot R}\\sum_{t=1}^{T}{(d_t-s_t)^+\\Delta t}\\) \\(a_O=\\frac{1}{T\\cdot R}\\sum_{t=1}^{T}{(s_t-d_t)^+\\Delta t}\\) Correlation factor of supply-demand relationships. 这个指标用于衡量 supply curve 与 demand curve 的相似度（与上一个指标差别不大），本来应该用 Locust 收集的 QPS 转化为 demand 的 CPU 利用率，以及用 Prometheus 收集的 supply 的 CPU 利用率，然后计算两个曲线的 R-square （评估回归模型的性能指标）。但文章认为 Locust 与 Prometheus 是两套监控系统，收集周期和统计方式有所区别，所以改用 Dynamic Time Warping 算法来衡量两个时间序列的相似度: 首先，将两个curve进行量纲对齐。比如将第一个 curve （\\(X=\\{x_1,x_2,\\dots,x_m\\}\\)）转到第二个 curve （\\(Y=\\{y_1,y_2,\\dots,y_m\\}\\)）的量纲下：\\(x^\\prime_i=(x_i-\\mu_x)\\times\\frac{\\sigma_Y}{\\sigma_X}+\\mu_Y\\)，\\(x^\\prime_i\\) 是转化后的值 定义距离矩阵 \\(D\\)，\\(D_{i,j}\\)代表 \\(X\\) 的时间点 \\(i\\) 和 \\(Y\\) 的时间点 \\(j\\)的距离，\\(d(x_i,y_j)\\) 代表 \\(x_i\\) 和 \\(y_j\\) 的欧氏距离（也可以用其他距离），\\(D_{i,j}\\)计算方式如下： \\[ D_{i,j}=min\\begin{cases} D_{i-1,j}+d(x_i,y_j)\\\\ D_{i,j-1}+d(x_i,y_j)\\\\ D_{i-1,j-1}+d(x_i,y_j) \\end{cases} \\] \\(D_{m-1,n-1}\\) 代表 curve \\(X\\) 与 curve \\(Y\\) 的最小距离，则 correlation factor计算如下：\\(CF=max(m,n)/D_{m-1,n-1}\\) 总实验 StatuScale 的目标有三个：①降低响应延时、②降低SLO违背率、③维持CPU利用率在目标水平（\\(\\pm1\\%\\)）。 Fig. 9(a) 展示了不同scaler的平均延时和P95延时的分布（Locust可以求得），可以看出StatuScale的延时分布是比较偏低的，均值维持在50~70ms，P99维持在300多ms左右 Fig. 9(b) 想展示的与Fig. 9(a)差不多，展示的是四个scaler的延时的累积分布直方图（CDF），P95基本维持在250ms左右，说明几个scaler都很有作用（私以为应该加上一个没有设置采样器的方法作为对比） Fig. 9(c) 计算了不同SLO阈值下的违背情况 Fig. 9(d) 计算了 correlation factor，说明StatuScale的资源分配的曲线与负载波动（资源需求曲线）很相似。Fig.10 展示了CPU使用（这里难道不应该是分配的CPU吗？）和负载的相似度。表格是对图像结果的数据展示 StatuScale也在Hotel-Reservation上进行了实验，结果比较相似，就不贴上来了。 消融实验 消融 Status Detector Module Status Detector判断当前负载是否 “stable”，如果 “stable”，则选择用 LightGBM预测负载，然后转换成CPU需求；如果 “unstable”，则用 A-PID 将CPU利用率控制在某个阈值。文章选择了3个变体，衡量它们的延时（为什么还要消融A-PID和LightGBM？为什么不衡量其他指标？）。实验在 Sock-Shop 上做，每组实验做3次 StatusScale\\(^\\Delta\\)：消融 horizontal scaler StatusScale\\(^\\circ\\)：消融 horizontal scaler，load status detector 和 A-PID 控制器 StatusScale\\(^*\\)：消融 horizontal scaler，load status detector 和 load prediction（LightGBM） 上述实验说明了 load status detector 对 load prediction的影响很大（StatusScale\\(^\\circ\\)） 消融 Scaling Modes vertical scaling 虽然能细粒度调节资源，但依然受限于单个机器硬件；horizontal scaling 又容易造成资源浪费。文章设计了2个变体，实验在 Sock-Shop 上做，每组实验做3次，但是加入了CPU使用率的对比： StatuScale\\(^\\square\\)：只使用 vertical scaling StatuScale\\(^*\\)：只使用 horizontal scaling 可以看出，horizontal scaling （StatuScale\\(^*\\)）确实能最大限度降低延时，但是CPU资源利用率偏低；vertical scaling（StatuScale\\(^\\square\\)）很难保证延时，但是CPU利用率高；StatusScale相当于在两者间做了均衡。 [1] Alim Ul Gias, et.al. 2019. ATOM: Model-Driven Autoscaling for Microservices. In 2019 IEEE 39th ICDCS. 1994–2004. https://doi.org/10.1109/ICDCS.2019.00197 [2] https://github.com/alibaba/clusterdata/blob/master/cluster-trace-v2018/trace_2018.md","tags":[{"name":"微服务","slug":"微服务","permalink":"https://shuaiyuxie.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"资源管理","slug":"资源管理","permalink":"https://shuaiyuxie.github.io/tags/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/"},{"name":"弹性伸缩","slug":"弹性伸缩","permalink":"https://shuaiyuxie.github.io/tags/%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9/"},{"name":"arxiv","slug":"arxiv","permalink":"https://shuaiyuxie.github.io/tags/arxiv/"},{"name":"2024","slug":"2024","permalink":"https://shuaiyuxie.github.io/tags/2024/"}]},{"title":"[ISCA 2024]  ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models","date":"2024-11-27T02:29:27.000Z","path":"2024/11/27/ElasticRec/","text":"题目：ElasticRec: A Microservice-based Model Serving Architecture Enabling Elastic Resource Scaling for Recommendation Models 来源：ISCA 2024 作者：韩国科学技术院 摘要 推荐系统（RecSys）广泛应用在许多线上服务中，为增加RecSys的推理时的吞吐量，数据中心通常对RecSys进行模型级别（model-wise）的资源管理。然而，RecSys中不同模块有着异构的资源需求，比如： RecSys中MLP模块 对于计算资源的需求高 RecSys中Embedding Table模块对于内存资源的需求高 如果将RecSys模型看作一个整体进行服务部署、资源分配等操作，势必会造成大量的资源浪费；但对RecSys模型中的每一层进行资源管理又是非常具有挑战性的（这里类似于单体应用和微服务应用的关系）。因此，作者提出了ElasticRec，一种基于微服务架构的推荐系统细粒度资源分配方法，目标是减少部署时的内存消耗，提升RecSys的吞吐量。 背景 文章背景主要介绍了深度推荐模型的结构，以及深度推荐模型如何集成到Kubernetes集群中，为用户提供线上推理服务。 深度推荐模型（DLRM） 如图所示，深度推荐模型（DLRM）包含3个主要组件： Bottom MLP 输入：dense input（比如 用户年龄） 输出：dense output（高维特征） 类型：计算敏感型 Embedding Table 输入：多个 sparse input（比如 商品ID） 输出：dense output（高维特征） 类型：内存敏感型 作用：根据稀疏输入得到Embedding Table中的高维特征。一般来说，一次查询中所有的sparse input得到的dense output会执行pool操作进行池化，得到单个dense output Top MLP 输入：Bottom MLP的输出 拼接 Embedding Table的输出 输出：给商品打分 在生产环境中，由于商品（item）的种类非常多，比如Amazon有数亿的商品种类。Embedding Table为每个商品种类都维护了一个特征，导致Embedding Table的大小可以达到几十GB，相比于Bottom MLP，有几点需要关注： Embedding Table对于计算不敏感，即pool操作并不需要太多计算资源；相反，对于内存带宽限制极为敏感，特别是有非常多的dense output需要pool时 在DLRM中，Embedding Table通常有多个，对内存的压力是极大的 模型服务架构（Model Serving Architectures） 模型容器化 这篇文章关注的是DLRM的推理。在线上应用中，DLRM被打包成镜像，镜像中包含了模型参数以及常用的机器学习库，以容器的方式运行在Kubernetes集群中，如Fig. 2（a）所示。 模型的自动伸缩 Kubernetes是一个容器编排工具，它能管理容器的生命周期，对容器进行自动化调度、资源分配。 吞吐量是一个衡量在线服务性能的指标，单位是QPS（query per second），吞吐量越高，代表在线服务单位时间内处理请求的数量 对于DLRM而言，处理单个请求的时间基本可以看作变化很小的，那么为提高吞吐量，可以采用Kubernetes的水平pod伸缩（Horizontal Pod Autoscaling，HPA）机制对DLRM进行副本复制，如Fig. 2 (b) 所示，增加DLRM的副本数可以提高系统的并行处理能力，从而增大吞吐量 然而，HPA 是一种 model-wise 的分配方案，它将整个DLRM模型进行复制，包括内存占用非常大的Embedding Table模块，但实际上Embedding Table并不涉及复杂的计算，所以一般不是（不是绝对）吞吐量的瓶颈所在。无脑进行HPA势必会造成大量内存浪费。 模型的硬件约束 因为 大型 DLRM 的 Embedding Table 通常有几十GB，将 Embedding Table 全部放在高内存带宽的GPU中通常不太可行，所以会退而求其次的使用如下两种方式：① CPU-only ② CPU-GPU。 CPU-only：Bottom MLP 和 Embedding Table 均运行在CPU CPU-GPU：Bottom MLP 运行在GPU， Embedding Table 均运行在CPU 可以看到， Embedding Table 都运行在CPU关联的内存上，如果能优化这一部分的内存使用，就可以提升DLRM的最大副本数量，从而提高系统的吞吐量。 动机 文章的动机从两点出发，阐述为什么现有的资源分配方案会导致次优性能（sub-optimal performance）： RecSys的不同模块具有异构资源需求 Embedding Table不同部分的访问频率相差极大 异构资源需求 Fig. 3 (a) 展示了三个推荐模型（RM1，RM2，RM3）的不同模块Bottom MLP 和 Embedding Table 在 ①计算复杂度（FLOPS）和 ②内存大小 上的差别。可以看出：Bottom MLP在计算复杂度上远高于 Embedding Table，但是内存占用远远小于 Embedding Table Fig.3 (b) 展示了三个推荐模型的不同模块在两种硬件约束下的延时占比。原文虽然没有讨论，但可以推测，在CPU-only架构下，推理的延时开销主要集中在Bottom MLP的计算；在CPU-GPU架构下，延时的开销在于将 Embedding Table 的数据从CPU传输到GPU 此外，文章还讨论了吞吐量的瓶颈问题，Fig. 4展示了作者的想法，实际上Bottom MLP 计算开销大，内存占用小，适合扩充副本来提升吞吐量；而 Embedding Table 本身吞吐量就很大，但是内存占用大，所以对于副本扩充应该谨慎。 当然，作者还通过实验，验证了不同硬件约束下不同模块的吞吐量存在差异，来支撑上述想法： 综上所述，文章说明RecSys中不同模块的异构资源需求，以及吞吐量的差异。为后续对不同模块分别进行切分提供了实验依据 Embedding Table 的倾斜访问模式 这一个实证分析较为简单，主要验证Embedding Table不同索引的访问频率的差异，如Fig. 6所示，在三个数据集中，大部分的访问集中在少数的索引（热点嵌入，hot embeddings） 换句话说，将资源选择性地分配给 hot embeddings，可以在提升吞吐量的同时，达到节省资源的目的 方法设计 Fig. 7展示了ElasticRec的系统架构，整体思路分为三个模块： 部署开销估计 基于动态规划（DP）的Embedding Table划分 推理时重索引 部署开销估计 前置处理：将Embedding Table的index按照访问频率从大到小排序，hot embeddings集中在最左侧 根据动机中提到的“将资源选择性地分配给 hot embeddings，可以在提升吞吐量的同时，达到节省资源的目的”，文章将Embedding Table切分为shards，每个shard包含了Embedding Table的一部分index。那么如何切分Embedding Table，以及如何评估切分策略的优劣呢？ 文章首先定义了如何评估切分策略的优劣，切分策略的优劣由固定吞吐量的前提下，所有shard的内存开销决定。用最少的内存达到目标吞吐量，评估算法如下： 算法入口为COST(k, j)，表示范围为[k,j]的shard的内存消耗，这个内存消耗由两部分组成： REPLICAS(k,j)： 计算特定吞吐量下，shard应该被分配的副本数量 计算shard被访问的概率probability和被访问的向量数ns，probability = CDF(j)- CDF(k)，ns = probability × nt 估计单个shard的副本在给定的访问向量数ns下能达到的QPS，estimated QPS = QPS(ns)，这里的QPS()是一个回归模型，可以线下测试得到 估计达到目标吞吐量target_traffic需要的副本数，num_replicas = target_traffic/estimated_QPS CAPACITY(k,j)：对于shard的每一个副本，计算存储embedding的内存开销 直接计算shard的副本大小：(j − k +1)×(size_of_a_single_embedding_vector) 这里需要特别注意回归模型QPS()，输入的参数除了需要访问的向量数ns外，还需要考虑向量本身的大小，如下图所示，QPS既与向量数有关，也与向量本身维度相关 基于DP的Embedding Table分区算法 在上一节中，当给定一个shard划分策略，我们可以评估每个shard在目标QPS下的内存开销，进而可以尝试找到给定QPS下最小内存开销的分区策略 这里文章有一个前提，即无论怎么划分，所有shard的目标QPS都是一样的，这样可以保证不存在多余的资源浪费 这个分区问题有两个操作：① 分多少shard ②每个shard的范围。假设我们用\\(Mem[num_{shards}][x]\\)表示Embedding Table在\\([0:x]\\)范围下，分区数量为\\(num_{shards}\\)的最小内存开销。那么这个问题具有两个明显的特性：最优子结构和重叠子问题 最优子结构：\\(Mem[num_{shards}][x]\\)可以由子问题的最优解构造而来，假设最后一个shard的大小为\\(m\\)，那么可以简化表示为\\(Mem[num_{shards}][x] = min(Mem[num_{shards}-1][x-m] + COST(m))\\)，比如下图中，\\(Mem[3][5]=min(Mem[2][5-m]+COST(m))=Mem[2][3]+COST(4,5)=4\\) 重叠子问题：求解过程中会反复遇到相同的子问题，需要将结果存储到表中，避免重复计算 因此，自然而然可以想到用动态规划（DP）求解，文章给出的算法如下： 最后根据最小内存开销回溯DP表可以得到分区策略 推理时重索引 这个部分的重点在于分区后，如何根据原始Embedding Table的index ID找到对应的shard中的某个embedding，以及确认index分别属于哪个input（为提高吞吐量，一个query包含了多个input）。 文章提出了两种索引： indices：存储一次query要从Embedding Table中查找的具体ID。 offset：指示每个input对应的的indices中的起始位置。 对于Fig. 11（a）未分区前，一个query（indices）包含了两个input，分别为红色的[1, 7]，灰色的[3,4,8]，offset表示第一个input要从indices第0个元素算起，第二个input从indices第1个元素算起，即input1为[1,7]，input2为[3,4,8] 对于Fig. 11（b）分区后，首先计算中间的indices，具体为根据indices中的index计算应该被分到哪个分区（减去之前分区的大小），可以很容易把indices划分为shard A 的[1,3,4]和shard B 的[7,8]，同时把offset进行划分（基于indices） 对于Fig. 11（b）分区后，由于各shard索引重置了，所以需要在中间的indices和offset的基础上，进行重索引，具体为减去之前分区的大小，比如Fig. 11(b) 中 shard B的[7,8]减去shard A的大小后，变成[1,2]。这样便可以直接从各shard中查找embedding了 最终部署 因为Kubernetes的 horizontal pod autoscaling 提供了弹性伸缩时参考指标的接口，对于： Embedding Table的shard，文章直接将每个shard可以承受的最大吞吐量作为参考指标，到达最大吞吐量则扩容 Bottom MLP，则采用SLA的65%作为扩容的阈值 （这里不是很明白为什么要采用不同的阈值指标，为什么不都用吞吐量？） 实验部分 文章分别验证了 ElasticRec 在 CPU-only 以及 CPU-GPU 环境下的性能表现： CPU-only：本地集群（1 master + 11 worknode） CPU-GPU：云集群（20 CPU-GPU nodes，GPU为 NVIDIA Tesla T4，节点间有31Gbps的带宽） DLRM模型的开发是基于facebook的dlrm1。Kubernetes自动伸缩器采用的custom metric来自prometheus。SLA设置为400ms。而对于验证的DLRM模型，作者选择了三个先进的推荐模型（RM1, RM2, RM3），并在RM1的基础上进行参数改造，设置了很多个microbenchmarks，参数变动和三个RM模型如下： 其中 Locality 指标 \\(P\\) 代表多少请求分布在前10%的热点向量，\\(P\\)越大，代表请求分布越集中在热点。 Microbenchmarks实验 文章一开始探讨了不同RM配置下的内存消耗： MLP size：随着MLP size的扩大，计算复杂度提高，延时会逐渐违背SLA。Model-wise的方法会扩容整个模型，而ElasticRec只需要扩容内存开销极小的Bottom MLP，所以内存消耗差距很大 Locality：访问越集中在热点，ELasticRec效果越好，因为只需要扩容热点那一部分 Number of tables：系统中可能不止一个Embedding Table，比如用户ID表和商品ID表，随着表数量的增多，ELasticRec对每个表都进行分片，降低扩容时的内存开销 Number of shards：分片数量并不是越多越好，因为每个shard会有最小内存消耗（程序运行必须的消耗），即算法1中的min_mem_alloc，所以当分片数量大于4后，效果没有那么明显了 不同 RM 在 CPU-only 环境下的性能 文章接下来在CPU-only环境下，比较了ElasticRec和Model-wise方法在三个推荐模型（RM1，RM2，RM3）的性能表现。以下实验都是在吞吐量为100QPS的下进行的 Fig. 13 展示了内存消耗的对比 Fig. 14 展示了内存利用率的对比，这里的内存利用率是作者自己定义的，表示当前shard在前1000个请求中被访问的embedding的比例，可以看出Model-wise只有一个shard（S1），并且内存利用率很低，对整个shard扩容显得很不值；ElasticRec有4个shard，前3个内存利用率很高（高频shard），最后一个非常低。 Fig. 15 展示了两种方法在吞吐量为100QPS所需要消耗的CPU服务器数量。这里我不太清楚是如何算出需要消耗的CPU服务器数量的（一般算的是虚拟CPU使用量？） 不同 RM 在 CPU-GPU 环境下的性能 在CPU-GPU环境下，ElasticRec将 MLP 模块设计为 GPU-centric 容器，将 Embedding Table 模块设计为只用 CPU 的容器；Model-wise 则将CPU和GPU都分配给一个容器。以下实验都是在吞吐量为100QPS的下进行的，实验效果如下 动态输入流量实验 前几个实验都是在固定吞吐量（QPS=100）下进行的，这个实验动态调整流量大小，然后观察ElasticRec和Model-wise的吞吐量表现、资源消耗以及尾部延时。 流量大小先逐步增大，然后降到一个固定值 可以发现，ElasticRec的吞吐量、内存消耗以及SLA违背都低于对比方法 与 GPU Embedding Caches 的对比 GPU Embedding Caches 方法是之前的一个工作，原理是把Embedding Table的hot embeddings存到GPU缓存中，能缓解CPU内存带宽压力（减少CPU与GPU的交互） 文章对比了 Model-wise、Model-wise (cache) 和 ElasticRec 在200 QPS 的内存消耗，ElasticRec的内存消耗仍然是最低的 这里我很好奇为什么不比较延时？ElasticRec延时应该比不过Model-wise (cache)，毕竟CPU和GPU交互需要时间。 [1] https://github.com/facebookresearch/dlrm","tags":[{"name":"微服务","slug":"微服务","permalink":"https://shuaiyuxie.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"推荐系统","slug":"推荐系统","permalink":"https://shuaiyuxie.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"资源管理","slug":"资源管理","permalink":"https://shuaiyuxie.github.io/tags/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/"},{"name":"面向AI的微服务","slug":"面向AI的微服务","permalink":"https://shuaiyuxie.github.io/tags/%E9%9D%A2%E5%90%91AI%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"ISCA","slug":"ISCA","permalink":"https://shuaiyuxie.github.io/tags/ISCA/"},{"name":"2024","slug":"2024","permalink":"https://shuaiyuxie.github.io/tags/2024/"}]},{"title":"前缀树（Prefix Tree）","date":"2024-11-25T03:35:01.000Z","path":"2024/11/25/trie_tree/","text":"Trie 树，又叫前缀树，字典树， 是一种有序的树形数据结构，用于高效地存储和检索字符串数据集中的键。下图是维基百科上关于trie树的一个典型例子，我们可以很清晰地看到，这棵树存储了许多前缀相似的字符串，给定一个字符串，我们可以很容易知道这个字符串是否被存储，而不需要遍历比较。 这一数据结构有相当多的应用情景，例如： 自动补全： 搜索提示：输入网址，跳出可能的选择 输入提示：根据已经输入的字符预测可能的词组和句子 拼写检查：存储合法的单词列表，快速查找是否存在合法的单词 前缀匹配 IP路由查找 题目 leetcode 208 实现Trie（前缀树） 请你实现 Trie 类： Trie() 初始化前缀树对象。 void insert(String word) 向前缀树中插入字符串 word 。 boolean search(String word) 如果字符串 word 在前缀树中，返回 true（即，在检索之前已经插入）；否则，返回 false 。 boolean startsWith(String prefix) 如果之前已经插入的字符串 word 的前缀之一为 prefix ，返回 true ；否则，返回 false 。 示例： 1234567891011121314输入[&quot;Trie&quot;, &quot;insert&quot;, &quot;search&quot;, &quot;search&quot;, &quot;startsWith&quot;, &quot;insert&quot;, &quot;search&quot;][[], [&quot;apple&quot;], [&quot;apple&quot;], [&quot;app&quot;], [&quot;app&quot;], [&quot;app&quot;], [&quot;app&quot;]]输出[null, null, true, false, true, null, true]解释Trie trie &#x3D; new Trie();trie.insert(&quot;apple&quot;);trie.search(&quot;apple&quot;); &#x2F;&#x2F; 返回 Truetrie.search(&quot;app&quot;); &#x2F;&#x2F; 返回 Falsetrie.startsWith(&quot;app&quot;); &#x2F;&#x2F; 返回 Truetrie.insert(&quot;app&quot;);trie.search(&quot;app&quot;); &#x2F;&#x2F; 返回 True 提示： 1 &lt;= word.length, prefix.length &lt;= 2000 word 和 prefix 仅由小写英文字母组成 insert、search 和 startsWith 调用次数 总计 不超过 3 * 104 次 分析 这道题有几个地方需要注意： insert时，需要标记单词是否截止，因为trie中的节点既有可能是前缀，也有可能是单词 search与 startswith的区别在于， startswith只需要搜索下去，看看有没有对应的节点；而search还需要判断这个节点是否有截止信号 实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class Trie(object): def __init__(self): self.root = &#123;&#125; def insert(self, word): \"\"\" :type word: str :rtype: None \"\"\" cur_node = self.root for c in word: if c in cur_node.keys(): cur_node = cur_node[c] else: cur_node[c]=&#123;&#125; cur_node = cur_node[c] cur_node[0]=0 # 标记是否截止 # 标记这个cur_node，标注上截止信号，代表这是一个词 cur_node[0]=1 def search(self, word): \"\"\" :type word: str :rtype: bool \"\"\" cur_node = self.root for c in word: if c in cur_node.keys(): cur_node = cur_node[c] else: return False # 判断有没有截止信号 if cur_node[0]==0: return False return True def startsWith(self, prefix): \"\"\" :type prefix: str :rtype: bool \"\"\" cur_node = self.root for c in prefix: if c in cur_node.keys(): cur_node = cur_node[c] else: return False return True trie = Trie()trie.insert(\"app\")trie.insert(\"apple\")trie.insert(\"beer\")trie.insert(\"add\")trie.insert(\"jam\")trie.insert(\"rental\")trie.insert(\"rental\")print(trie.search(\"apps\"))print(trie.startsWith(\"app\"))print(trie.search(\"app\"))","tags":[{"name":"算法","slug":"算法","permalink":"https://shuaiyuxie.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"leetcode","slug":"leetcode","permalink":"https://shuaiyuxie.github.io/tags/leetcode/"},{"name":"数据结构","slug":"数据结构","permalink":"https://shuaiyuxie.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"期望最大算法（Expectation Maximization Algorithm）","date":"2024-11-20T03:35:01.000Z","path":"2024/11/20/EM/","text":"介绍 期望最大化算法（Expectation-Maximization algorithm, EM）是一种在统计学中估计概率模型参数的方法，特别适用于包含隐变量（latent variables）的概率模型。 如果一个概率模型只有观测变量，那么我们可以基于观测得到的数据，用最大似然估计求得概率模型的参数。但是如果概率模型还包含了无法观测的变量（隐变量），则无法用上述方法估计，所以需要考虑隐变量，引入新的方法对参数进行估计。 应用举例 假设我们有一组一维数据\\(X=\\{x_1, x_2,...,x_n\\}\\)，我们认为这些数据是由2个正态分布混合而成的。我们的目标是估计这2个正态分布的参数（均值和方差）以及它们各自的权重。参数如下： 第1个分布 \\(N(\\mu_1, \\sigma_1)\\)，其中\\(\\mu_1=5\\), \\(\\sigma_1=9\\) 第2个分布 \\(N(\\mu_2, \\sigma_2)\\)，其中\\(\\mu_2=15\\), \\(\\sigma_2=0.5\\) 两个分布的权重满足：\\(\\sum_{k=1}^2\\pi_k=1\\) 我们目前手中只有这一组一维观测数据\\(X=\\{x_1, x_2,...,x_n\\}\\)，已知观测数据由2个正态分布组成，目标是求出这2个正态分布的参数以及各自的权重。 注意，我们不能直接使用观测数据去拟合2个分布，因为观测数据的分布实际上是2个正态分布混合而成，其中包含了一个隐变量： \\[ z_i= \\begin{cases} 0&amp; x_i \\in N(\\mu_1, \\sigma_1^2)\\\\ 1&amp; x_i \\in N(\\mu_2, \\sigma_2^2) \\end{cases} \\] 隐变量\\(z_i\\)表示数据点\\(x_i\\)由哪个分布生成。而隐变量\\(z_i\\)的值无法被观测，所以当我们用最大似然估计去做时，需要考虑所有可能的隐变量情况（不同取值的权重）： \\[ \\begin{aligned} L(\\theta)&amp;=\\prod_{i=1}^{n}f(x_i;\\theta) \\\\ ln L(\\theta) &amp;= \\sum_{i=1}^{n}ln f(x_i:\\theta) \\\\ &amp;= \\sum_{i=1}^{n}ln \\sum_{k=1}^{2} \\pi_k f(x_i:\\theta_k) \\end{aligned} \\] 但由于 \\(\\pi_k\\)未知，所以难以进行最大似然估计。 EM算法步骤 我们首先用两个正态分布混合生成观测数据： 1234np.random.seed(8)n_samples = ⅓data = np.concatenate((np.random.normal(5, 3, n_samples), np.random.normal(15, 0.5, n_samples))) 并随机给予两个分布初始参数以及权重，代码如下： 123456mu1, sigma1 = 10, 1mu2, sigma2 = 20, 1pi1, pi2 = 0.5, 0.5tolerance = 1e-6max_iterations = 1000 EM算法分为两个步骤： ### E步（期望步, Expectation step）： 我们需要计算每个数据点 \\(x_i\\) 属于不同分布的后验概率（本质上是隐变量z的条件期望值），此处可以使用贝叶斯公式计算得到： \\[ \\begin{aligned} P(z_i=1|x_i, \\theta)&amp;=\\frac{P(x_i|z_i=1)P(z_i=1)}{P(x_i)}=\\frac{P(x_i|z_i=1)P(z_i=1)}{P(x_i|z_i=1)P(z_i=1)+P(x_i|z_i=2)P(z_i=2)}\\\\ &amp;=\\frac{\\pi_1N(x_i|\\mu_1, \\sigma_1^2)}{\\pi_1N(x_i|\\mu_1, \\sigma_1^2) + \\pi_2N(x_i|\\mu_2, \\sigma_2^2)} \\end{aligned} \\] 同理可以求得\\(P(z_i=2|x_i, \\theta)\\)。代码如下： 1234567891011def normal_pdf(x, mu, sigma): # 计算x_i的概率密度 return (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)for iteration in range(max_iterations): # E步：计算后验概率 likelihood1 = normal_pdf(data, mu1, sigma1) likelihood2 = normal_pdf(data, mu2, sigma2) total_likelihood = pi1 * likelihood1 + pi2 * likelihood2 posterior1 = (pi1 * likelihood1) / total_likelihood posterior2 = (pi2 * likelihood2) / total_likelihood M步（最大化步, Maximization step）： 在E步得到 \\(x_i\\) 属于不同分布的后验概率后，也是隐变量\\(z_i\\)的条件期望后，我们利用这个期望来更新参数估计值，以最大化观测数据的似然函数。 因为现在已经知道了关于隐变量z_i的条件期望，所以我们可以用最大似然估计去估计各分布的参数了： \\[ \\mu_k=\\mathop{\\arg\\max}\\limits_{\\mu_k}{\\sum_{i=1}^{n}P(z_i=k|x_i, \\theta)}lnf(x_i;\\mu_k,\\sigma_k^2) \\] \\[ \\sigma_k^2=\\mathop{\\arg\\max}\\limits_{\\sigma_k^2}{\\sum_{i=1}^{n}P(z_i=k|x_i, \\theta)}lnf(x_i;\\mu_k,\\sigma_k^2) \\] 对上述两式进行求解可得各部分更新公式如下（\\(\\pi_k\\)更新同理）： \\[ \\mu_k = \\frac{\\sum_{i=1}^{n}P(z_i=k|x_i,\\theta)x_i}{\\sum_{i=1}^{n}P(z_i=k|x_i,\\theta)} \\] \\[ \\sigma_k^2 = \\frac{\\sum_{i=1}^{n}P(z_i=k|x_i, \\theta)(x_i-\\mu_k)^2}{\\sum_{i=1}^{n}P(z_i=k|x_i,\\theta)} \\] \\[ \\pi_k=\\frac{1}{n}\\sum_{i=1}^{n}P(z_i=k|x_i,\\theta) \\] 代码如下： 1234567# M步: 更新参数pi1_new = np.mean(posterior1)pi2_new = np.mean(posterior2)mu1_new = np.sum(posterior1 * data) / np.sum(posterior1)mu2_new = np.sum(posterior2 * data) / np.sum(posterior2)sigma1_new = np.sqrt(np.sum(posterior1 * (data - mu1_new) ** 2) / np.sum(posterior1))sigma2_new = np.sqrt(np.sum(posterior2 * (data - mu2_new) ** 2) / np.sum(posterior2)) 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import numpy as npimport matplotlib.pyplot as plt# 生成数据np.random.seed(8)n_samples = 100data = np.concatenate((np.random.normal(5, 3, n_samples // 2), np.random.normal(15, 0.5, n_samples // 2)))# 初始化参数mu1, sigma1 = 10, 1mu2, sigma2 = 20, 1pi1, pi2 = 0.5, 0.5tolerance = 1e-6max_iterations = 1000def normal_pdf(x, mu, sigma): return (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)for iteration in range(max_iterations): # E步: 计算后验概率 likelihood1 = normal_pdf(data, mu1, sigma1) likelihood2 = normal_pdf(data, mu2, sigma2) total_likelihood = pi1 * likelihood1 + pi2 * likelihood2 posterior1 = (pi1 * likelihood1) / total_likelihood posterior2 = (pi2 * likelihood2) / total_likelihood # M步: 更新参数 pi1_new = np.mean(posterior1) pi2_new = np.mean(posterior2) mu1_new = np.sum(posterior1 * data) / np.sum(posterior1) mu2_new = np.sum(posterior2 * data) / np.sum(posterior2) sigma1_new = np.sqrt(np.sum(posterior1 * (data - mu1_new) ** 2) / np.sum(posterior1)) sigma2_new = np.sqrt(np.sum(posterior2 * (data - mu2_new) ** 2) / np.sum(posterior2)) # 检查是否收敛 if (abs(mu1_new - mu1) &lt; tolerance and abs(mu2_new - mu2) &lt; tolerance and abs(sigma1_new - sigma1) &lt; tolerance and abs(sigma2_new - sigma2) &lt; tolerance): break # 更新参数 mu1, mu2, sigma1, sigma2, pi1, pi2 = mu1_new, mu2_new, sigma1_new, sigma2_new, pi1_new, pi2_new# 打印最终结果print(f\"迭代次数: &#123;iteration&#125;\")print(f\"μ1: &#123;mu1&#125;, σ1: &#123;sigma1&#125;, π1: &#123;pi1&#125;\")print(f\"μ2: &#123;mu2&#125;, σ2: &#123;sigma2&#125;, π2: &#123;pi2&#125;\")# 绘制结果plt.hist(data, bins=30, density=True, alpha=0.6, color='g')x = np.linspace(min(data), max(data), 100)plt.plot(x, pi1 * normal_pdf(x, mu1, sigma1), 'r-', lw=2, label=f'N(&#123;mu1:.2f&#125;, &#123;sigma1:.2f&#125;)')plt.plot(x, pi2 * normal_pdf(x, mu2, sigma2), 'b-', lw=2, label=f'N(&#123;mu2:.2f&#125;, &#123;sigma2:.2f&#125;)')plt.legend()plt.show() 结果输出如下：","tags":[{"name":"算法","slug":"算法","permalink":"https://shuaiyuxie.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"概率论","slug":"概率论","permalink":"https://shuaiyuxie.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"},{"name":"机器学习","slug":"机器学习","permalink":"https://shuaiyuxie.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"最长公共子序列（Longest Common Subsequence）","date":"2024-11-20T03:35:01.000Z","path":"2024/11/20/LCS/","text":"最长公共子序列（LCS）是一个在一个序列集合中用来查找所有序列中最长子序列的问题。这与查找最长公共子串的问题不同的地方是：子序列不需要在原序列中占用连续的位置。而最长公共子串（要求连续）和最长公共子序列是不同的。 比如：字符串 ABCBDAB 和 字符串 BDCAB 的LCS为 BCAB。 LCS在计算机领域有诸多应用，比如可以： 比较 DNA 序列或蛋白质序列。 比较不同版本的文件，找出更改的部分 文本（代码）相似性检 ... 假设有两个版本的文件： 文件 V1: The quick brown fox jumps over the lazy dog. 文件 V2: A quick brown dog jumps over the lazy cat. 通过 LCS 算法，可以找到它们的最长公共子序列为 quick brown jumps over the lazy，剩余的部分为更改，这有助于生成补丁文件和合并冲突。 题目 leetcode 1143 是最长公共子序列的经典问题： &gt; 给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。 &gt; &gt;一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。 &gt; &gt;- 例如，\"ace\" 是 \"abcde\" 的子序列，但 \"aec\" 不是 \"abcde\" 的子序列。 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。 &gt; &gt; 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。 示例 1： 输入：text1 = \"abcde\", text2 = \"ace\" 输出：3 解释：最长公共子序列是 \"ace\" ，它的长度为 3 。 示例 2： 输入：text1 = \"abc\", text2 = \"abc\" 输出：3 解释：最长公共子序列是 \"abc\" ，它的长度为 3 。 示例 3： 输入：text1 = \"abc\", text2 = \"def\" 输出：0 解释：两个字符串没有公共子序列，返回 0 。 提示： 1 &lt;= text1.length, text2.length &lt;= 1000 text1 和 text2 仅由小写英文字符组成。 分析 最长公共子序列（Longest Common Subsequence, LCS）问题非常适合使用动态规划来解决，原因在于它具备了动态规划的两个关键特性：最优子结构和重叠子问题。 最优子结构：LCS问题的最优解可以由其子问题的最优解构建而来。具体来说： 如果两个序列的最后一个字符相同，那么这个字符必定是LCS的一部分，接下来的问题就转化为了求这两个序列去掉最后一个字符之后的LCS。即 \\(LCS(i, j) = LCS(i-1, j-1) + 1\\) 如果两个序列的最后一个字符不同，则LCS不会同时包含这两个字符，问题转化为求一个序列去掉最后一个字符之后与另一个序列的LCS。即 \\(LCS(i, j) = max(LCS(i-1, j), LCS(i, j-1))\\) 这种性质允许我们将大问题分解为更小的子问题，通过解决这些子问题来构建原始问题的解。 重叠子问题：在求解 LCS 的过程中，我们会反复遇到相同的子问题。例如，在计算两个序列 \\(X\\) 和 \\(Y\\) 的LCS时，可能会多次计算 \\(X\\) 的前 \\(i\\) 个字符和 \\(Y\\) 的前 \\(j\\) 个字符的LCS。由于这些子问题会被多次求解，我们可以将它们的结果存储起来，避免重复计算，这就是动态规划中所谓的“记忆化”。 求解 动态规划的求解步骤如下： 定义状态 在最长公共子序列（LCS）问题中，状态可以用一个二维数组 \\(dp\\) 表示，其中 \\(dp[i][j]\\) 表示序列 \\(X\\) 的前 \\(i\\) 个字符和序列 \\(Y\\) 的前 \\(j\\) 个字符的最长公共子序列的长度。 状态转移方程 在上节最优子结构的判断中，已经定义出了状态转移方程： if \\(X[i]==Y[j]\\)， \\(dp[i][j] = dp[i-1][j-1] + 1\\) if \\(X[i]!=Y[j]\\)， \\(dp[i][j] = max(dp[i-1][j], dp[i][j-1])\\) python代码实现如下： 1234567891011121314151617class Solution(object): def longestCommonSubsequence(self, text1, text2): \"\"\" :type text1: str :type text2: str :rtype: int \"\"\" n, m = len(text1), len(text2) # 多声明一行一列，方便计算dp[1][1] dp = [[0 for j in range(m+1)] for i in range(n+1)] for i in range(1, n+1): for j in range(1, m+1): if text1[i-1] == text2[j-1]: dp[i][j] = dp[i-1][j-1] + 1 else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) return dp[n][m] 最终的\\(dp\\)表为： [0, 0, 0, 0] [0, 1, 1, 1] [0, 1, 1, 1] [0, 1, 2, 2] [0, 1, 2, 2] [0, 1, 2, 3] 扩充 如果题目要求我们求出具体的最长公共子序列呢？我们可以根据\\(dp\\)表进行回溯，思路为 从\\(dp[n][m]\\)开始向前回溯 如果当前\\(X[i]==Y[j]\\)，那么说明此时\\(X[i]\\)（\\(Y[j]\\)）属于LCS的一部分，则加入LCS；同时\\(X\\)和\\(Y\\)都向前推进一位 如果当前\\(X[i]!=Y[j]\\)，那么我们需要找出LCS是在\\(X[i-1]\\)和\\(Y[j]\\)中产生，还是在\\(X[i]\\)和\\(Y[j-1]\\)中产生，则只需要对比\\(dp[i-1][j]\\) 和 \\(dp[i][j-1]\\) 所以代码如下： 123456789101112131415161718192021222324252627282930313233343536class Solution(object): def longestCommonSubsequence(self, text1, text2): \"\"\" :type text1: str :type text2: str :rtype: int \"\"\" n, m = len(text1), len(text2) # 多声明一行一列，方便计算dp[1][1] dp = [[0 for j in range(m+1)] for i in range(n+1)] for i in range(1, n+1): for j in range(1, m+1): if text1[i-1] == text2[j-1]: dp[i][j] = dp[i-1][j-1] + 1 else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) # 回溯 i, j = n, m lcs = [] while (i &gt; 0 and j &gt; 0): if text1[i-1]==text2[j-1]: lcs.append(text1[i-1]) i-=1 j-=1 else: if dp[i-1][j] &gt; dp[i][j-1]: i-=1 else: j-=1 lcs_str = ''.join(lcs)[::-1] return dp[n][m], lcs_strsolution = Solution()lcs_len, lcs = solution.longestCommonSubsequence('abcde', 'ace')print(lcs_len, lcs) 回溯路径为： [0, 0, 0, 0] [0, 1, 1, 1] [0, 1, 1, 1] [0, 1, 2, 2] [0, 1, 2, 2] [0, 1, 2, 3]","tags":[{"name":"算法","slug":"算法","permalink":"https://shuaiyuxie.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"leetcode","slug":"leetcode","permalink":"https://shuaiyuxie.github.io/tags/leetcode/"},{"name":"动态规划","slug":"动态规划","permalink":"https://shuaiyuxie.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"[FSE 2024] TraStrainer: Adaptive Sampling for Distributed Traces with System Runtime State","date":"2024-06-23T07:42:10.000Z","path":"2024/06/23/TraStrainer/","text":"题目：TraStrainer: Adaptive Sampling for Distributed Traces with System Runtime State 来源：FSE 2024 作者：中山大学DDS实验室 摘要 微服务系统每天都会产生大量的trace数据，带来了极大的计算和存储成本。trace sampling 技术被用来缓解这种压力。trace sampling 分为两种： random sampling：又称 head sampling，即以固定概率决定每条trace是否采样 biased sampling：又称 tail sampling，即根据trace的状态决定是否采样 很明显，random sampling 实现起来简单，但无法保证得到高质量的采样数据；biased sampling 能够根据用户偏好进行采样（比如高延时、异常状态码）。 先前的 biased sampling 工作大多基于密度（diversity），即偏好采样那些少见（edge-case）的traces，常见（common-case）的traces则少采样一些。然而，作者认为仅根据trace的状态进行采样是不充分的，应该再考虑当前系统运行状态（system runtime state），特别是系统处在故障状态时。（作者很有想法，在trace采样中玩了多模态，引入了metric，我觉得陈鹏飞老师实验室的工作还是很扎实且新颖的） 本文提出了TraStrainer，从以下角度进行在线采样： - 考虑密度：采用一种可解释的编码方式将trace转化为向量，方便后续密度采样 - 考虑系统状态：结合当前系统各种运行指标生成偏好向量，方便后续系统采样 - 密度采样+系统采样 \\(\\to\\) 最终采样决策 动机 陈鹏飞老师实验室有大量关于微服务系统的故障诊断的工作，其中有许多是基于trace进行分析的，比如MicroRank，TraceRank和MicroSketch。 trace采样是这些工作的上游任务，先前与biased sampling相关的工作都是基于密度的，目标是采样edge-case traces，没有考虑过采样的traces对下游故障诊断工作的影响。作者从以下两点进行了分析： 仅考虑edge-case traces是不够的。作者在此举例说明 common-case traces也有很大的用处: common-case traces 可能与根因有关。比如线程池因为太多请求的到来而用尽，而这些与根因相关的请求的traces并不一定是异常的，也就被认定为common-case traces。而我们分析这些common-case traces，可以发现这个时刻有高峰流量（这个是我根据自己理解加的）。 common-case traces 有利于下游的分析任务。很多工作比如TraceRCA，T-Rank，都需要common-case traces来获得系统的正常模式，从而与故障时刻进行比对。 结合 system runtime state 有利于判断有价值的trace。作者拿了华为的一个真实场景进行分析，如Fig. 3所示，[a,b]时间段 Node A 的 MySQL服务进行全表查询，导致 Node A 的CPU被打满，到达 Node A 的请求变得异常。SREs通常先检查系统状态，发现CPU升高，然后分析经过 Node A 的traces。然而，如果只根据密度进行trace采样，那么[a,b]的traces将被采集的很少，因为还没有发生异常。如果结合系统状态进行采样，那么[a,b]的traces将给予更高的采样权重（[a,b]存在CPU攀升）。 综上，作者认为应该在trace采样时不仅仅考虑traces之间的密度，也要引入对当前系统状态的考虑。 问题定义 给定一段时间收集的traces \\(\\mathcal{T}\\)、对应的系统状态指标 \\(\\mathcal{M}\\)、采样率 \\(\\beta\\)，需要对\\(\\mathcal{T}\\)中每个trace \\(t\\) 计算采样概率 \\(\\rho\\)。整个过程定义为： \\[ S_p(\\beta, \\mathcal{T}, \\mathcal{M}, t) \\to \\rho, \\mathcal{T&#39;} \\] 其中，\\(\\mathcal{T&#39;}\\)是\\(\\mathcal{T}\\)的采样子集。 TraStrainer 概要 TraStrainer的架构和其他在线采样器相似，包含以下模块： Runtime Data Preprocessing： Trace Encoder：对trace进行结构和状态编码 System Bias Extractor：将当前系统状态指标进行编码 Comprehensive Sampling： System-Biased Sampler：优先采样与当前系统波动最相似的trace Diversity-Biased Sampler：优先采样edge cases traces Composite Sampler：结合上述两种采样器进行最终决策 Trace Encoder 如Fig.5所示，trace的编码包含结构编码和状态编码两部分： 状态编码：结合 Fig. 5 的例子进行说明，Fig. 5 的Trace Vector的上半部分展示了由指标（node，metric_name）构成的向量，比如指标\\(m_1\\)就是（\\(C\\), \\(SQLConnectionTime\\)）。一条trace由各种span构成，文章的span携带了一些tag（比如Node和annotation）。为了计算\\(m_1\\)的值\\(f_{m_1}\\)，作者将所有与\\(m_1\\)相关的span的duration结合起来，具体计算如下： \\[ f_{m_1}=(|S_a|+1)*\\sum_{i=1}^{n}s_{m_1i}.duration \\] \\(s_{m_1i}\\)即与指标\\(m_1\\)相关的span，而\\(|S_a|\\)即相关span中异常span的个数（状态码为error，Fig.5中为1） &gt; 注：最开始不太理解这种设计，后来发现是作者将指标与对应的trace的状态信息（延时+状态码）联系起来，相当于量化了指标对trace状态的影响，非常巧妙。 结构编码：这一块比较简单，即将trace看做一棵树，每层可能有多个span，这些spans由parentSpan、method以及params组成，每一层的spans都被编码为一个特征。这些特征共同组成一个vector。 System Bias Extractor 这一部分的本质是衡量当前系统哪个指标比较重要，这个重要程度由指标的异常程度决定。每个指标的异常程度组合成一个一维的preference vector数组， 作者认为基于统计模型的异常检测不准确，无法识别周期性；而基于LSTM和Transformer的深度学习模型在响应太慢，无法适应线上采样。所以最终采用DLinear algorithm1，如Fig.6所示，这个算法通过指标的历史时序数据预测当前值\\(v_k&#39;\\)，并通过以下公式计算指标异常程度： \\[ \\alpha=\\frac{v_k&#39;-v_k}{max(v_k&#39;, v_k)} \\] 这个公式通过预测值与真实值的差距计算异常度。所有指标\\(\\mathcal{M}\\)的异常度拼在一起就是preference vector \\(\\mathcal{P}\\)。 System-Biased Sampler System-Biased Sampler的核心是优先考虑与当前系统指标波动最相似的traces（与motivation中的故障诊断对上）。那么需要对新到来的trace进行注意力评估和采样概率计算。 本文定义了一个固定长度look-back window，由\\(k\\)条最近收集的历史traces组成：\\([t_1,...,t_k]\\)。System-Biased Sampler只需用到trace的状态编码部分，每条trace的状态向量由n个指标组成，表示为\\(t_i=[f_{1i},...,f_{ni}]\\)。对历史traces每一维指标计算均值\\(\\mu_i\\)和标准差\\(\\sigma_i\\)，则对新到来的trace \\(t_{k+1}\\) 的第\\(i\\)个指标注意力分数计算如下： \\[ a_i = \\frac{|f_{ik+1}-\\mu_i|}{\\sigma_i} \\] \\(t_{k+1}\\)的所有指标的注意力分数记为 \\(\\mathcal{A}=[a_1,...,a_n]\\)，TraStrainer通过将注意力分数\\(\\mathcal{A}\\)和preference vector \\(\\mathcal{P}\\) 进行点积得到面向系统的采样概率\\(p_s\\)： \\[ p_s(t_{k+1})= \\frac{2}{1+e^{-2\\mathcal{P·\\mathcal{A}(t_{k+1})}}}-1 \\] 以上操作是将点积\\(P·\\mathcal{A}(t_{k+1})\\)通过tanh函数映射到[0,1]范围，点积越大，代表当前trace与当前系统状态越相似，面向系统的采样概率越大。 Diversity-Biased Sampler Diversity-Biased Sampler的目标是考虑edge-case traces（即少见的traces），这篇文章与先前工作一样基于聚类来筛选edge-case traces。 论文将look-back window的历史traces进行聚类（基于trace的特征），并计算每个类的质量（traces数量），并把新trace \\(t_{k+1}\\) 归于最近的类 \\(c_{k+1}&#39;\\)。\\(c_{k+1}&#39;\\)的质量为\\(ma_{k+1}&#39;\\)，计算 trace \\(t_{k+1}\\) 和 所属类\\(c_{k+1}&#39;\\) 之间的Jaccard相似度\\(si(t_{k+1})\\)。 一般来说，所属类\\(c_{k+1}&#39;\\)的质量和\\(si(t_{k+1})\\)越小，代表所属类越稀有、新trace越独特，应该给予更高的采样概率。所以面向密度的采样概率\\(p_d(t_{k+1})\\)计算如下： \\[ p_d(t_{k+1})=\\frac{\\frac{1}{ma_{k+1}&#39;*si(t_{k+1})}}{\\sum_{i=1}^{k+1}\\frac{1}{ma_{i}&#39;*si(t_{i})}} \\] Composite Sampler 对于新到trace \\(t\\)，综合两个采样概率 \\(p_s(t)\\) 和 \\(p_d(t)\\) 后，考虑采样额度 \\(\\beta\\)，基于动态投票机制（dynamic voting mechanism）最终决策。 首先统计过去look-back window里采样概率 \\(\\theta\\)，如果： - \\(\\theta \\geq \\beta\\)，必须两个采样决策都为True，才采样 - \\(\\theta \\leq \\beta\\)，只需要有一个采样决策为True，即可采样 [1] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are transformers effective for time series forecasting?. In Proceedings of the AAAI conference on artificial intelligence, Vol. 37. 11121–11128.","tags":[{"name":"微服务","slug":"微服务","permalink":"https://shuaiyuxie.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"trace","slug":"trace","permalink":"https://shuaiyuxie.github.io/tags/trace/"},{"name":"trace采样","slug":"trace采样","permalink":"https://shuaiyuxie.github.io/tags/trace%E9%87%87%E6%A0%B7/"},{"name":"FSE","slug":"FSE","permalink":"https://shuaiyuxie.github.io/tags/FSE/"},{"name":"2024","slug":"2024","permalink":"https://shuaiyuxie.github.io/tags/2024/"}]},{"title":"[TSC 2024] Diagnosing Performance Issues for Large-Scale  Microservice Systems with Heterogeneous Graph","date":"2024-06-22T04:21:27.000Z","path":"2024/06/22/MicroDig/","text":"题目：Diagnosing Performance Issues for Large-Scale Microservice Systems with Heterogeneous Graph 来源：TSC 2024 作者：南开大学AIOps@NKU团队，清华大学Netman实验室 摘要 微服务系统的可用性对于业务运营和企业声誉至关重要。然而，微服务系统的动态性和复杂性给大规模微服务系统的性能问题诊断带来了重大挑战。文章分析了腾讯性能故障的真实案例后，发现故障传播的因果关系与服务之间的调用关系不一致，所以之前基于调用关系的根因定位方法准确率不高。文章提出适用于大规模微服务系统的性能问题诊断方法，MicroDig，步骤如下： 基于调用和微服务之间的因果关系构建异构传播图 采样面向异构的随机游走算法进行根因服务定位 MicroDig在腾讯、Train-Ticket、银行三个数据集上能实现至少85%的top-3 accuracy。 背景 随着微服务系统的快速演变和规模扩张，微服务自身固有的动态性和复杂性给系统的可靠性维护带来了挑战。当微服务系统发生性能异常时，需要及时定位到根因服务，并把问题工单发给对应微服务的团队。然而，由于微服务数量太过庞大（Alibaba有超过30000服务），并且服务之间交互复杂，性能异常在服务之间进行传播，导致大量服务同时异常，进而使得人工诊断变得耗时耗力。 有许多现有工作基于trace来进行根因分析。trace记录了每次请求的调用路径以及相关性能表现，然而，海量的traces会带来极大的存储开销（eBay每天产生150 billion的traces）。所以，越来越多的公司只保留两个服务之间的端到端聚合调用（end-to-end aggregated calls）。 有些工作已经使用了aggregated call（这里指的是Codewisdom团队的GMTA1），采取模式匹配的方式进行根因定位，但需要非常充足的历史故障数据，这在现实场景中很难实现；也有一些工作基于因果图进行根因定位，他们的因果挖掘算法有极高的计算成本，并且准确率较低。 注：aggregated call在GMTA中提到过，应该就是一段时间（比如1 min）内trace的聚合： 文章提出的MicroDig的核心思想是：调用关系不等于因果关系（在动机中有具体说明），于是在故障诊断前先构造因果图（节点是微服务和调用）。 【从相关工作的分析到方法的提出有点衔接生硬，可能是因为因果方面的分析放到了动机的原因】 动机 调用关系≠异常传播的因果关系 文章举了一个例子来说明这个观点： \\(A \\to B \\to C\\) 的异常次数急剧增加，如果仅仅根据调用关系去分析异常传播，那么根因是\\(C\\)，然而，操作员却没有在\\(C\\)中发现有意义的故障报告。因为 \\(B\\) 已经用尽了文件描述符，无法建立与 \\(C\\) 的新连接，所以\\(B \\to C\\)有大量的异常出现。所以根因是\\(B\\)不是\\(C\\)，这与调用关系的回溯是违背的。文章提到腾讯有35%的性能问题不能仅仅依靠调用关系回溯解决。 所以异常的被调用服务不一定是根因，调用方和被调用方都有可能是根因 异构传播图 由3.1可知，仅仅从调用关系分析异常传播是不够的，所以本文提出了一种异构传播图（heterogeneous propagation graph）来描述故障传播的因果关系： 如上图所示，\\(R(A,B)\\) 代表\\(A \\to B\\)的异常率（anomaly rate），\\(R(A)\\) 代表服务\\(A\\) 本身的异常率。注意，服务本身的异常率，如\\(R(A)\\)，在这个工作中是不可观测的；边的异常率，如\\(R(A,B)\\)，是可以被观测的。 因为3.1中展示了调用方和被调用方均可能贡献异常，所以每个服务都应该有一条指向调用边的因果线（比如\\(R(A) \\to R(A,B)\\)）。 文章添加了一些假设：①服务之间是独立的，比如\\(R(A)\\)和\\(R(B)\\)是独立的【这个假设有点不太符合现实】，②没有交集的两条调用边是独立的，比如\\(R(A,B)\\)和\\(R(C,D)\\) 根据作者的设计，这里应该就能看到\\(R(B,C)\\)是受\\(R(B)\\)和\\(R(C)\\)影响的了，从某种意义上给3.1的问题提供了思路。 MicroDig 架构 可以看到MicroDig分为几个部分： 1. 性能监控 (Monitoring) 2. 相关调用的识别 (Association Call Identification) 3. 异构传播图的构建 (Heterogeneous Propagation Graph Construction) 4. 根因定位 (Root Cause Localization) Association Call Identification 对于大规模微服务系统，如果直接构造调用图，那么图中会包含大量与故障不相关的调用边。所以需要对边进行筛选。 构造port-level 异常子图 文章首先构造 port-level 异常子图，port-level即接口级别，图中的节点都是接口， 具体步骤如下： 构造 port-level 调用图（为什么选用port-level） 在调用图上进行 宽度优先搜索，对于被遍历的边，采用3-sigma 异常检测 对边的异常率或者超时率进行检测，将异常边保留下来，就得到port-level异常子图 为什么先构造port-level异常子图，而不是直接构造service-level异常子图？因为一个service包含太多port，聚合后一些异常port的表现可能被其他正常port掩盖。 构造service-level 异常子图 聚合构造好的port-level异常子图，即把同一个服务的port节点合并为一个service节点，就得到了service-level异常子图。对于服务\\(S\\)和\\(S&#39;\\)，定义\\(F(p,p&#39;)\\)和\\(N(p,p&#39;)\\)分别为其中port-level边\\(p \\to p&#39;\\)的异常调用数和总调用数，那么时间点\\(t\\)的\\(S \\to S&#39;\\)的异常率\\(R_t(S, S&#39;)\\)为： \\[ R_t(S, S&#39;)=\\frac{\\sum_{p\\in S, p&#39; \\in S&#39;}F_t(p,p&#39;)}{\\sum_{p\\in S, p&#39; \\in S&#39;}N_t(p,p&#39;)} \\] 整个过程如图(a) (b)所示 ： 构造 Heterogeneous Propagation Graph 3.1 中提到调用关系≠故障传播的因果关系，所以service-level异常子图也不能直接用于根因定位，需要进一步构建Heterogeneous Propagation Graph （HPG）： 原理很简单： 1. 设置service节点：把service-level异常子图的所有服务加入到 HPG 2. 设置call节点：对于每个服务\\(S\\)，将\\(S\\)的出边和入边作为节点加入HPG 3. call节点和service节点的关系：对于每个call节点（\\(S \\to S&#39;\\)），设置 \\(S \\to (S \\to S&#39;)\\)，\\((S \\to S&#39;) \\to S&#39;\\) 4. call节点和call节点的关系：对于每个服务\\(S\\)，设置：出边 \\(\\to\\) 入边 根因服务定位 异构传播图（HPG）有两种节点（service，call）和两种边（service \\(\\to\\) call, call \\(\\to\\) call）。本文采取针对异构图的随机游走算法来定位根因： 转移权重 随机游走的核心是定义不同边的游走权重： 对于call \\(\\to\\) call：比如\\(C_{23} \\to C_{12}\\)，通过计算这两个调用的异常率数组之间的相关系数来决定游走权重。 对于service \\(\\to\\) call：比如\\(S_1 \\to C_{12}\\) 首先计算service的异常程度，定义\\(\\mathbb{S}_U\\)和\\(\\mathbb{S}_D\\)分别表示服务\\(S\\)的上游服务集合和下游服务集合，服务\\(S\\)的异常程度\\(\\alpha_S\\)可以表示为： \\[ \\alpha_S = \\frac{|\\{S&#39;|S&#39;\\in \\mathbb{S}_U \\cup \\mathbb{S}_D, \\theta(S&#39;)=1 \\}|}{|\\mathbb{S}_U \\cup \\mathbb{S}_D|} \\] 当\\(S&#39;\\)有任意一条port-level的边是异常时，\\(\\theta(S&#39;)=1\\)。 然后计算service \\(\\to\\) call的权重。对于任意一个call节点\\(C=S_{caller} \\to S_{callee}\\)，有两条service \\(\\to\\) call类型的边：\\(S_{caller} \\to C\\) 和 \\(C \\to S_{callee}\\)。这两条边的权重分别为：\\(\\omega_{caller}\\) 和 \\(\\omega_{callee}\\)，分别计算如下： \\[ \\omega_{caller}=max(0, \\Delta \\eta)*[0.5+\\beta sgn(\\Delta \\alpha)] \\] \\[ \\omega_{callee}=max(0, \\Delta \\eta)*[0.5-\\beta sgn(\\Delta \\alpha)] \\] 其中，\\(\\Delta \\alpha = \\alpha(S_{caller})-\\alpha(S_{callee})\\)，\\(\\Delta \\eta\\)即当前服务的所有入边的权重-所有出边的权重。 异构随机游走 与Personal pageRank不同的是，作者没有用个性化向量来跳出陷阱，而是在图上加了以下几种边来防止掉入陷阱： backward edge：如果有节点只有一条有向边连接，那么则加一个与有向边方向相反的backward edge，权重是有向边的\\(\\rho\\)倍。 self-loop edge：给每个节点加上自环边 游走算法如下图所示，与普通的随机游走没有太大差别： 总结 创新点：这篇文章的创新点不是很突出，随机游走感觉已经玩烂了（如果随机游走上能再精进一下，可能会好一些）。。。但是异构图的构建还是让人耳目一新的 动机：动机比较简单，没有实证分析（对腾讯数据的实证分析应该加上的）。 代码复现：公布的代码里应该是没有完整数据的，其实除公司以外的测试数据集应该要公开的。 参考文献 [1] Guo X, Peng X, Wang H, et al. Graph-based trace analysis for microservice architecture understanding and problem diagnosis, ESEC/FSE. 2020: 1387-1397. https://taoxiease.github.io/publications/esecfse20in-trace.pdf","tags":[{"name":"微服务","slug":"微服务","permalink":"https://shuaiyuxie.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"trace","slug":"trace","permalink":"https://shuaiyuxie.github.io/tags/trace/"},{"name":"根因定位","slug":"根因定位","permalink":"https://shuaiyuxie.github.io/tags/%E6%A0%B9%E5%9B%A0%E5%AE%9A%E4%BD%8D/"},{"name":"TSC","slug":"TSC","permalink":"https://shuaiyuxie.github.io/tags/TSC/"},{"name":"2024","slug":"2024","permalink":"https://shuaiyuxie.github.io/tags/2024/"}]},{"title":"[ICSE 2021] MicroHECL: High-Efficient Root Cause Localization in Large-Scale Microservice Systems","date":"2024-05-13T02:35:47.000Z","path":"2024/05/13/MicoHECL/","text":"题目：MicroHECL: High-Efficient Root Cause Localization in Large-Scale Microservice Systems 来源：ICSE 2021 作者：复旦大学CodeWisdom团队，阿里云 摘要 微服务系统的可用性问题直接影响了业务的运行，这些问题通常由各种各样的故障类型以及服务间故障的传播造成。如何设计精准且高效定位故障根因的方法成为了一个重大挑战。然而，现有基于服务调用图的方法在异常检测的准确率和图游走的效率上存在不足。本文提出了一种高效的根因定位方法MicroHECL，通过如下步骤定位故障根因： 动态构建一段时间窗口内的服务调用图 对不同异常类型进行个性化异常检测 对不同异常类型分析异常传播链路，通过剪枝提高效率 背景 工业微服务系统包含大量的微服务（e.g., alibaba有3000个微服务，300个子系统）。一个服务都可能运行在成百上千个容器中，并时常动态创建和销毁。服务之间也存在复杂的调用关系（同步、异步）。 微服务系统可用性问题可能由不同类型的异常引起，每种异常都由一组指标表示。异常可能源自服务并沿服务调用传播，最终导致可用性问题。文章具体关注三种故障类型（就是谷歌提到的几种黄金指标）： 性能异常（Performance Anomaly） 可靠性异常（Reliability Anomaly） 流量异常（Traffic Anomaly） MicroHECL 概述 MicroHECL支持三种故障类型的检测和诊断：性能异常、可靠性异常和流量异常。最终输出候选的故障根因服务排名。 服务调用图构建 当MicroHECL检测到异常后（3-sigma），会启动根因分析流程，首先就是构建过去30min内的服务依赖图（service call graph）。图上的节点就是每一个微服务；图上的边代表服务之间的调用关系，比如\\(S_i \\to S_j\\)代表微服务\\(S_i\\)调用了微服务\\(S_j\\)；节点上具有一些属性：响应时间（RT），错误数量（EC）以及每秒请求量（QPS）。 异常传播链分析 检测到异常的微服务并不一定是故障根因，但是故障根因一般在这个微服务的上游或者下游。异常传播链分析的目的是筛选初始异常服务中可能的异常传播链来识别一组候选根本原因服务。整个过程由以下几步组成： 分析入口服务（即最初汇报异常的微服务，后面会混用） 异常传播链扩展 根因排序 分析入口服务 文章首先根据经验定义了三种故障类型的传播方向： 性能异常和可靠性异常的传播方向很好理解，因为上游服务的响应时间和状态码受下游服务影响。流量异常的传播方向是从上游到下游，原因是【笔者自己的理解】上游服务发生了故障（比如网络拥塞），那么发送到下游的流量会大幅减少，所以下游服务会出现QPS急剧减少的异常。这个结论也可以在ImpactTracer1中找到。 有了故障的传播方向，文章从入口服务开始，向邻居节点不断扩展分析。如图Fig. 2所示，整个过程描述如下： &gt; 1. 将入口服务\\(S_5\\)纳入异常传播链 &gt; 2. 异常检测。检测\\(S_5\\)的邻居节点\\(S_4\\)和\\(S_7\\)的异常类型 &gt; 3. 确认\\(S_4\\)的异常类型为Traffic Anomaly，\\(S_7\\)的异常类型为Performance Anomaly &gt; 4. 检测是否符合传播方向（\\(S_4\\)是否是\\(S_5\\)的上游，\\(S_7\\)是否是\\(S_5\\)的下游） &gt; 5. 符合，将\\(S_4\\)和\\(S_7\\)添加到异常传播链 &gt; 6. 从\\(S_4\\)和\\(S_7\\)出发，对邻居节点重复上述步骤 以上过程其实就是故障的溯源，图中的箭头可以看作故障的传播路径。过程中涉及的异常检测在3.3节会提到。 异常传播链扩展 过程与3.2.1中描述的扩展过程一致。对于每个检测到的上游/下游异常节点，将其添加到异常传播链中。当无法向链中添加更多节点时，异常传播链的扩展结束。比如Fig. 2对于\\(S_4\\)方向的传播分析，以\\(S_1\\)结束；对于\\(S_7\\)方向的传播分析，以\\(S_9\\)和\\(S_{10}\\)结束。 候选根因定位 本文选择异常传播链的末端服务作为候选根因，比如Fig. 2中的候选根因服务为\\(S_1\\)，\\(S_9\\)和\\(S_{10}\\)。那么如何排名呢？ 选取入口服务过去60min的业务指标 \\(X\\) 选取候选根因服务过去60分钟的质量指标（RT, EC or QPS）\\(Y\\) 计算两者之间的皮尔逊相关系数： \\[ P(X, Y)=\\frac{\\sum_{i=1}^n{(X_i-\\overline{X})(Y_i-\\overline{Y})}}{\\sqrt{\\sum_{i=1}^n{(X_i-\\overline{X})^2}\\sum_{i=1}^n{(Y_i-\\overline{Y})^2}}} \\] 皮尔逊相关系数范围为[-1,1]，绝对值越接近1则表明相关性越大。所以，根因则根据皮尔逊相关系数的绝对值来排序。 服务异常检测 这篇文章的重点应该是放在了如何设计精准的异常检测上。不同于以往的方法只使用一种异常检测手段，本文对三种故障类型（Performance Anomaly，Reliability Anomaly，Traffic Anomaly）分别设计了异常检测方法。 这三种方法分别对应三种指标：响应时间（RT），错误数量（EC）以及每秒请求量（QPS），以下是阿里巴巴监控系统中获取的异常案例： 性能异常检测 在RT的异常检测中，需要考虑RT可能存在的周期性（如Fig. 3 (d)）所示，简单的使用3-sigma方法会将这种正常周期波动视为异常。所以不仅需要考虑当前期间的质量指标，还需要考虑前一天和前一周同一天的质量指标。 本文使用OC-SVM训练异常检测模型，OC-SVM是一种常用的无监督机器学习模型，常用于异常检测和分类。文章为RT构建了以下4种特征： 当前检测窗口中RT的值大于给定比较时间窗口内RT的最大值的数量。 当前检测窗口中RT的最大值与给定比较时间窗口内的RT最大值的差值。 当前检测时间窗口中超过给定比较时间窗口中RT滑动平均值最大值的数量。 当前检测窗口中RT的平均值与给定时间窗口内RT的滑动平均值的最大值的比值。 其中，当前检测窗口大小为10min，给定比较时间窗口有3种：①过去一小时、②前一天同一小时、③前一周同一天的同一小时。（如果数据保存没那么完善和严格的话，笔者认为定义一段正常时间为比较窗口应该也是可以接受的）。所以一共有3*4=12种特征。 对于模型训练和验证，文章拿10000样本作为训练集，600个正负比例1:1的样本作为测试集。 可靠性异常检测 这里提到EC大多时候都是0（Fig.2 (b,c)），偶尔会出现少许波动，但很快会恢复（比如断路器打开时EC升高，关闭后EC降低），也没有周期性，如果用性能异常的模型，则会出现大量误报（少许波动都会算进去）。 所以，文章采用随机森林（Random Forest，RF）来分类，文章为EC构建了以下5种特征： 计算最近一小时的EC和前一天同一时间段的EC的增量；使用3-Sigma规则识别当前检测窗口中可能存在的增量异常值；如果存在，则返回异常值的平均值作为特征值，否则返回0。 计算最近一小时内EC值和每一个值的前一分钟EC值的增量；使用3-Sigma规则识别当前检测窗口中可能存在的增量异常值；如果存在，则返回异常值的平均值作为特征值，否则返回0。 检测窗口内的平均RT是否大于设定的阈值（例如，在本文的线上系统中，阈值设置为50ms） 检测窗口内最大错误率（EC/sum(QPS)）。 检测窗口内RT和EC的皮尔逊相关系数 其中，当前检测窗口大小为10min，对于模型训练和验证，文章拿1000样本作为训练集（有标签，正负比1:3），400个正负比例5:3的样本作为测试集。 流量异常检测 QPS大多满足正态分布（Fig.2 (c,f)），所以直接采用3-sigma进行检测。 &gt; 这里笔者有小小的疑问，QPS真的满足正态分布吗？在系统那边的文章，许多流量都是以泊松分布注入的 其中，当前检测窗口大小为10min，选择过去1h计算均值和标准差。3-sigma的均值和方差选择。为了进一步消除误报，还需要检测初始异常服务的QPS和业务指标（就是入口服务被异常检测的指标）的皮尔逊系数，如果大于0.9，则报告流量异常。 剪枝 为了提高MicroHECL的异常回溯效率，需要控制指数增长的异常传播链分支数量，因为不断地进行异常检测也是非常耗时的。 核心思想：异常传播链中的两个连续服务调用的相应质量指标应该具有相似变化趋势 例子：Fig. 2中的 \\(S_1 \\to S_4\\) 和 \\(S_4 \\to S_5\\) 都是Traffic Anomaly 的传播路径，如果\\(S_1 \\to S_4\\) 的 QPS 和 \\(S_4 \\to S_5\\) 的 QPS 没有相似的趋势（即皮尔逊相关系数&lt;0.7），则需要剪掉\\(S_1 \\to S_4\\)，那么\\(S_4\\)就取代\\(S_1\\)变成了候选根因。 这里的检测窗口选取的过去60min。剪枝操作执行在异常节点加入异常调用链之前。 总结 文章思路挺好的，有理有据，方法朴实有效。写作的顺序不是传统的总分形式，首先就把整体流程讲完了，然后拿出异常检测和剪枝单独讲，初看有点不适应。 参考文献 [1] Xie R, Yang J, Li J, et al. ImpactTracer: Root Cause Localization in Microservices Based on Fault Propagation Modeling, (DATE), 2023. https://ieeexplore.ieee.org/abstract/document/10137078/","tags":[{"name":"微服务","slug":"微服务","permalink":"https://shuaiyuxie.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"trace","slug":"trace","permalink":"https://shuaiyuxie.github.io/tags/trace/"},{"name":"根因定位","slug":"根因定位","permalink":"https://shuaiyuxie.github.io/tags/%E6%A0%B9%E5%9B%A0%E5%AE%9A%E4%BD%8D/"},{"name":"ICSE","slug":"ICSE","permalink":"https://shuaiyuxie.github.io/tags/ICSE/"},{"name":"2021","slug":"2021","permalink":"https://shuaiyuxie.github.io/tags/2021/"}]},{"title":"[SoCC 2021] Characterizing Microservice Dependency and Performance: Alibaba Trace Analysis","date":"2024-05-08T03:28:33.000Z","path":"2024/05/08/alibaba-traces-socc-2021/","text":"题目：Characterizing Microservice Dependency and Performance: Alibaba Trace Analysis 来源：SoCC 2021 作者：中国科学院深圳先进技术研究院, 阿里巴巴 摘要 现在有大量针对微服务架构的研究，比如资源管理、弹性伸缩以及故障诊断等。但是目前仍缺乏针对生产环境中微服务特性的实证研究。这篇文章对阿里巴巴公布的trace数据集1进行了详细的实证分析，从以下角度揭示了生产环境下微服务系统的特点： 微服务调用图的特点，与传统作业DAG的不同 无状态微服务之间的依赖关系 微服务系统的运行时性能受哪些因素的影响 此外，现有的微服务benchmark也存在一些问题，如： 规模太小。经典的benchmark（如DeathStarBench2），只包含数个微服务（不超过40）。在这些小规模的微服务benchmark上得到的结论不一定能推广到生产环境中； 静态依赖。这些benchmark的依赖关系都是静态的，无法模拟生产环境中常见的动态性。 所以这篇文章还基于阿里巴巴的trace数据构建了一个仿真的数学模型，模拟大规模动态微服务系统。 背景 微服务架构 这里首先介绍了微服务架构的调用图，以及图中常见的组件： 这里引入了几个关键术语： Entering Microservice：入口微服务，即请求进入微服务系统的入口。通常是前端微服务。 UM, DM：分别指代一条调用链路的上游微服务（upstream microservice）和下游微服务（downstream microservice）。 对于微服务种类，文章基于服务提供的功能将微服务划分为有状态微服务（stateful）和无状态微服务（stateless）。 stateful微服务：通常存储有一些状态数据，常见的有数据库（database）和缓存（memCached），它们大多的接口大多分为两类：reading 和 writing。 stateless微服务：不存储状态数据，所以可以轻松的伸缩，它们通常提供成百上千个不同接口，用于完成不同的业务功能。 对于微服务交互种类，文章基于交互协议划分了三种类别： IP：进程间通信（Inter Process communication），常发生在stateless微服务和stateful微服务之间。 RPC：远程过程调用（Remote Procedure Call），一种双向通信，DM必须返回给UM结果。 MQ：消息队列（Message Queue），一种单向通信，UM发送消息到第三方中间件（消息队列），消息队列储存这个消息，直到DM主动取出这个消息。 一般来说，RPC效率高，MQ更加灵活。 此外，还介绍了两个概念：调用深度（call depth）和响应延迟（RT）。 call depth：调用深度指调用图中最长的路径长度，比如Figure 1中的调用图长度为5。 RT：从UM发出请求到UM收到回复的时长。即使同一种接口的请求也会因为参数、状态的不同产生差距较大的延时。 Alibaba Trace alibaba的trace与常见的trace数据模型不同3，因为它更像一种多模态监控数据，包含了节点信息、指标以及调用链等。具体信息如下： 物理运行环境：阿里巴巴的集群采用K8s进行管理，整个集群运行在裸机云（bare-metal cloud）上，服务与作业通常混合部署在一起以提高资源利用率。Figure 2 (a) 介绍了云上两种常见的运行方式： Online Services：比如微服务，运行在容器中，直接由K8s管理，有持续向外界提供服务的能力。（stateful微服务一般部署在特定集群中，不参与混合部署） Offline Jobs：这些作业一般都需要执行特定的任务，需要K8s事先为它们分配资源，然后调度到特定的机器上执行。 微服务系统指标：这个大概分为三个部分：硬件层（缓存命中率）、操作系统层（CPU利用率）、应用层（JVM垃圾回收），具体内容如Figure 2 （b）。 微服务调用链：如Figure 2 (c)所示，大体上与OpenTracing的数据模型类似，但是摒弃了spanID和parentSpanId，只留下UM和DM的信息，并用rpcId来唯一标识一个trace内的不同调用，Communication Paradigm代表调用类型（又名rpctype，如rpc）。 聚合调用：如Figure 2 (d)所示，本质上是对单个微服务的调用信息进行聚合和统计。 调用图的剖析 这一块内容很多，我只提炼出较为有意义的部分。这里的调用图（call graph）并不是指整个微服务依赖图，应该指的是单个trace的拓扑图。 微服务调用图特征 作者在这里总结了三个特征，对下游任务非常有启发： 调用图的微服务数量呈现长尾分布 现有的benchmark太小了：10%的调用图的微服务数量&gt;40，存在微服务数量&gt;100的调用图。 大量的Memcacheds：大规模的调用图中有一半的微服务都是Memcacheds，可能是为了减少RT。 调用图是一棵树，并且很多图是一条长链路 较短的深度：一半的调用图深度在2~4 （a） 树有点胖：，深度随着微服务数量增加没有明显变化 （b），说明调用图是宽且浅的？很多下游微服务只是简单的查询数据（stateful微服务一般是叶子节点） 较深的图一般都是长链路：深度增加，但是后面的的微服务数量大多为1个，说明这棵树的宽度基本集中在第2层，后面的都是一条长链路 有些下游任务（弹性伸缩）会对调用图进行编码，作者特别提到有些图有很长的深度，会让这些任务产生很大的模型以及过拟合。我觉得这没有直接关系，这些数量远远达不到图网络的极限。而且这个实验也可以反过来说，大部分图深度都是很短的。 许多stateless微服务是热点 存在高入度微服务：有5%的stateless微服务入度&gt;16，这些微服务在90%的调用图存在，处理了95%的请求。这些服务很大概率是瓶颈，可以用来指导弹性伸缩。 微服务调用图大多是动态的 这个动态和其他文章提到的动态不一样，文中的动态性指的是请求同一个服务的接口，如果参数不一样，会产生不同的拓扑链路（Figure 6）；其他文章提到的是微服务系统始终在动态变化。 微服务调用关系特征 不同层之间调用类型差别大 首先考虑微服务是否DM，大致分为以下几类：① black holes（没有DM），②relay（必须有DM），③normal（一定概率有DM） IP(S2D)，IP(S2M)，IP(S2) 表示IP通信的双方分别为：stateless 微服务与database，stateless 微服务与Memcacheds，stateless微服务与stateless微服务 深度增加，black holes比例增加，relay比例减少，normal中对应部分也是如此。 深度增加，IP(S2M) 比例先增后减，IP(S2D)在升高，表明缓存命中率在下降，转而去查询数据库。MQ比例增加，说明业务链路较深时（业务复杂），倾向于使用MQ来减少RT 微服务之间的依赖 这一章节对如何设计和优化微服务架构有启发，不是我研究的范畴，暂时略过 并行依赖 并行依赖很少：数据集中大部分的微服务都很少被并行调用，这个并行给我的感觉就是异步调用 微服务的运行时性能 这个章节很重要，对资源管理有很大的指导作用。首先介绍一个定义：MCR代表微服务调用速率，我的理解是服务承受的负载 MCR对资源的影响 MCR与CPU利用率和Young GC强相关，但与Memory利用率相关性弱 与CPU利用率，Young GC强相关：Young GC指的是对JVM堆内存中的新生代区域进行垃圾回收4，Young GC频繁会造成性能下降或者应用stop，可能是因为内存泄漏等原因。 与内存，Old GC相关性弱：alibaba trace中容器的内存一般都很稳定，Old GC频率可能也是如此（老年代本身垃圾回收就不频繁），所以在实验中不是很明显（受限于数据集特征）。 资源对响应时间的影响 图中的延时选的是P75延时 &gt; 与CPU利用率强相关：随着CPU利用率升高，RT明显升高，但RT对内存反应不是很明显（可能是因为缺乏高内存数据） &gt; 与容器的MCR不太相关：Alibaba trace中即使MCR很高了，CPU利用率可能还低于10%，所以RT变化不大，说明资源浪费很严重 随机图模型 这里简单讲一下，代码实现应该不难： 1. 准备一个存储stateless服务的队列\\(Q\\)，并放入Entering Microservice 2. 执行循环，直到\\(Q\\)为空 1. \\(Q\\)弹出一个服务作为UM 2. 如果UM的类型是Relay或者normal (Relay)，则根据数据集中DM服务类型的分布，生成对应类型的服务数量 3. 为生成的DMs中不同服务类型确定communication paradigm 4. 将生成的DMs中stateless的微服务放入\\(Q\\) 3. 图优化 1. 遍历生成的图的每一层 1. 随机选择两个父项，如果他们共享相同的标签，则合并他们的两个孩子。 2. 合并的节点将连接到两个父节点 暂时还没有看到随机模型被其他论文使用，可能是因为大家都可以自己搭建环境生产数据吧，也可能是因为alibaba trace够用了 参考文献 [1] https://github.com/alibaba/clusterdata [2] Yu Gan. An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud &amp; Edge Systems. ASPLOS, 2019. https://github.com/delimitrou/DeathStarBench [3] OpenTracing, “Opentracing,” https://opentracing.io/specification [4] java 六 Young GC 和 Full GC https://www.cnblogs.com/klvchen/articles/11758324.html","tags":[{"name":"微服务","slug":"微服务","permalink":"https://shuaiyuxie.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"trace","slug":"trace","permalink":"https://shuaiyuxie.github.io/tags/trace/"},{"name":"实证分析","slug":"实证分析","permalink":"https://shuaiyuxie.github.io/tags/%E5%AE%9E%E8%AF%81%E5%88%86%E6%9E%90/"},{"name":"SoCC","slug":"SoCC","permalink":"https://shuaiyuxie.github.io/tags/SoCC/"},{"name":"2021","slug":"2021","permalink":"https://shuaiyuxie.github.io/tags/2021/"}]}]